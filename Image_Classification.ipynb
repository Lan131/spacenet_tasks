{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hZu4HqOEt5aP",
    "outputId": "39d7103b-aeab-4545-d225-21113698f8ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cc4791a230>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from utils_jnb import *\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z1kqZz_v1T2y",
    "outputId": "748f4701-56ea-47f5-f02f-a1e026beceb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uiVw_drJsMwn"
   },
   "outputs": [],
   "source": [
    "directory = \"./src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2sgdxa1sNbM"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "vgg19 = models.vgg19_bn(pretrained=True)\n",
    "\n",
    "# Freeze model parameters\n",
    "# Same for all\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg19.classifier = nn.Sequential(nn.Linear(25088, 4096),\n",
    "nn.LeakyReLU(),\n",
    "nn.Dropout(0.4),\n",
    "nn.Linear(4096, 2048),\n",
    "nn.LeakyReLU(),\n",
    "nn.Dropout(0.2),\n",
    "nn.Linear(2048, 1024),\n",
    "nn.LeakyReLU(),                                 \n",
    "nn.Dropout(0.2),\n",
    "nn.Linear(1024, num_classes),\n",
    "nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2X4kzWXv3b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.01)\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (7): LeakyReLU(negative_slope=0.01)\n",
       "    (8): Dropout(p=0.2, inplace=False)\n",
       "    (9): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (10): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RAdam(vgg19.parameters(), lr=1e-2) #https://arxiv.org/pdf/1908.03265.pdf\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "expLrScheduler = lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)\n",
    "\n",
    "vgg19.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-Sg3anS0PTH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       (1,792)\n",
      "|    └─BatchNorm2d: 2-2                  (128)\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─Conv2d: 2-4                       (36,928)\n",
      "|    └─BatchNorm2d: 2-5                  (128)\n",
      "|    └─ReLU: 2-6                         --\n",
      "|    └─MaxPool2d: 2-7                    --\n",
      "|    └─Conv2d: 2-8                       (73,856)\n",
      "|    └─BatchNorm2d: 2-9                  (256)\n",
      "|    └─ReLU: 2-10                        --\n",
      "|    └─Conv2d: 2-11                      (147,584)\n",
      "|    └─BatchNorm2d: 2-12                 (256)\n",
      "|    └─ReLU: 2-13                        --\n",
      "|    └─MaxPool2d: 2-14                   --\n",
      "|    └─Conv2d: 2-15                      (295,168)\n",
      "|    └─BatchNorm2d: 2-16                 (512)\n",
      "|    └─ReLU: 2-17                        --\n",
      "|    └─Conv2d: 2-18                      (590,080)\n",
      "|    └─BatchNorm2d: 2-19                 (512)\n",
      "|    └─ReLU: 2-20                        --\n",
      "|    └─Conv2d: 2-21                      (590,080)\n",
      "|    └─BatchNorm2d: 2-22                 (512)\n",
      "|    └─ReLU: 2-23                        --\n",
      "|    └─Conv2d: 2-24                      (590,080)\n",
      "|    └─BatchNorm2d: 2-25                 (512)\n",
      "|    └─ReLU: 2-26                        --\n",
      "|    └─MaxPool2d: 2-27                   --\n",
      "|    └─Conv2d: 2-28                      (1,180,160)\n",
      "|    └─BatchNorm2d: 2-29                 (1,024)\n",
      "|    └─ReLU: 2-30                        --\n",
      "|    └─Conv2d: 2-31                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-32                 (1,024)\n",
      "|    └─ReLU: 2-33                        --\n",
      "|    └─Conv2d: 2-34                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-35                 (1,024)\n",
      "|    └─ReLU: 2-36                        --\n",
      "|    └─Conv2d: 2-37                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-38                 (1,024)\n",
      "|    └─ReLU: 2-39                        --\n",
      "|    └─MaxPool2d: 2-40                   --\n",
      "|    └─Conv2d: 2-41                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-42                 (1,024)\n",
      "|    └─ReLU: 2-43                        --\n",
      "|    └─Conv2d: 2-44                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-45                 (1,024)\n",
      "|    └─ReLU: 2-46                        --\n",
      "|    └─Conv2d: 2-47                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-48                 (1,024)\n",
      "|    └─ReLU: 2-49                        --\n",
      "|    └─Conv2d: 2-50                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-51                 (1,024)\n",
      "|    └─ReLU: 2-52                        --\n",
      "|    └─MaxPool2d: 2-53                   --\n",
      "├─AdaptiveAvgPool2d: 1-2                 --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Linear: 2-54                      102,764,544\n",
      "|    └─LeakyReLU: 2-55                   --\n",
      "|    └─Dropout: 2-56                     --\n",
      "|    └─Linear: 2-57                      8,390,656\n",
      "|    └─LeakyReLU: 2-58                   --\n",
      "|    └─Dropout: 2-59                     --\n",
      "|    └─Linear: 2-60                      2,098,176\n",
      "|    └─LeakyReLU: 2-61                   --\n",
      "|    └─Dropout: 2-62                     --\n",
      "|    └─Linear: 2-63                      2,050\n",
      "|    └─LogSoftmax: 2-64                  --\n",
      "=================================================================\n",
      "Total params: 133,290,818\n",
      "Trainable params: 113,255,426\n",
      "Non-trainable params: 20,035,392\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Sequential: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       (1,792)\n",
       "|    └─BatchNorm2d: 2-2                  (128)\n",
       "|    └─ReLU: 2-3                         --\n",
       "|    └─Conv2d: 2-4                       (36,928)\n",
       "|    └─BatchNorm2d: 2-5                  (128)\n",
       "|    └─ReLU: 2-6                         --\n",
       "|    └─MaxPool2d: 2-7                    --\n",
       "|    └─Conv2d: 2-8                       (73,856)\n",
       "|    └─BatchNorm2d: 2-9                  (256)\n",
       "|    └─ReLU: 2-10                        --\n",
       "|    └─Conv2d: 2-11                      (147,584)\n",
       "|    └─BatchNorm2d: 2-12                 (256)\n",
       "|    └─ReLU: 2-13                        --\n",
       "|    └─MaxPool2d: 2-14                   --\n",
       "|    └─Conv2d: 2-15                      (295,168)\n",
       "|    └─BatchNorm2d: 2-16                 (512)\n",
       "|    └─ReLU: 2-17                        --\n",
       "|    └─Conv2d: 2-18                      (590,080)\n",
       "|    └─BatchNorm2d: 2-19                 (512)\n",
       "|    └─ReLU: 2-20                        --\n",
       "|    └─Conv2d: 2-21                      (590,080)\n",
       "|    └─BatchNorm2d: 2-22                 (512)\n",
       "|    └─ReLU: 2-23                        --\n",
       "|    └─Conv2d: 2-24                      (590,080)\n",
       "|    └─BatchNorm2d: 2-25                 (512)\n",
       "|    └─ReLU: 2-26                        --\n",
       "|    └─MaxPool2d: 2-27                   --\n",
       "|    └─Conv2d: 2-28                      (1,180,160)\n",
       "|    └─BatchNorm2d: 2-29                 (1,024)\n",
       "|    └─ReLU: 2-30                        --\n",
       "|    └─Conv2d: 2-31                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-32                 (1,024)\n",
       "|    └─ReLU: 2-33                        --\n",
       "|    └─Conv2d: 2-34                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-35                 (1,024)\n",
       "|    └─ReLU: 2-36                        --\n",
       "|    └─Conv2d: 2-37                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-38                 (1,024)\n",
       "|    └─ReLU: 2-39                        --\n",
       "|    └─MaxPool2d: 2-40                   --\n",
       "|    └─Conv2d: 2-41                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-42                 (1,024)\n",
       "|    └─ReLU: 2-43                        --\n",
       "|    └─Conv2d: 2-44                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-45                 (1,024)\n",
       "|    └─ReLU: 2-46                        --\n",
       "|    └─Conv2d: 2-47                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-48                 (1,024)\n",
       "|    └─ReLU: 2-49                        --\n",
       "|    └─Conv2d: 2-50                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-51                 (1,024)\n",
       "|    └─ReLU: 2-52                        --\n",
       "|    └─MaxPool2d: 2-53                   --\n",
       "├─AdaptiveAvgPool2d: 1-2                 --\n",
       "├─Sequential: 1-3                        --\n",
       "|    └─Linear: 2-54                      102,764,544\n",
       "|    └─LeakyReLU: 2-55                   --\n",
       "|    └─Dropout: 2-56                     --\n",
       "|    └─Linear: 2-57                      8,390,656\n",
       "|    └─LeakyReLU: 2-58                   --\n",
       "|    └─Dropout: 2-59                     --\n",
       "|    └─Linear: 2-60                      2,098,176\n",
       "|    └─LeakyReLU: 2-61                   --\n",
       "|    └─Dropout: 2-62                     --\n",
       "|    └─Linear: 2-63                      2,050\n",
       "|    └─LogSoftmax: 2-64                  --\n",
       "=================================================================\n",
       "Total params: 133,290,818\n",
       "Trainable params: 113,255,426\n",
       "Non-trainable params: 20,035,392\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vgg19, input_size=(3, 224, 224), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npNkQm7W0Pr5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4996, 556]\n",
      "5552\n",
      "Epoch: 1/25\n",
      "Batch number: 000, Training: Loss: 16.4565, Accuracy: 0.8125\n",
      "Batch number: 001, Training: Loss: 1101.1713, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 957.2845, Accuracy: 0.3125\n",
      "Batch number: 003, Training: Loss: 4557.0913, Accuracy: 0.6562\n",
      "Batch number: 004, Training: Loss: 22776.5293, Accuracy: 0.5312\n",
      "Batch number: 005, Training: Loss: 246293.3281, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 110893.2109, Accuracy: 0.7500\n",
      "Batch number: 007, Training: Loss: 351331.0312, Accuracy: 0.5312\n",
      "Batch number: 008, Training: Loss: 200811.0312, Accuracy: 0.6250\n",
      "Batch number: 009, Training: Loss: 250860.0156, Accuracy: 0.6562\n",
      "Batch number: 010, Training: Loss: 155700.7812, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 427349.5312, Accuracy: 0.4688\n",
      "Batch number: 012, Training: Loss: 260458.4062, Accuracy: 0.6250\n",
      "Batch number: 013, Training: Loss: 296933.7812, Accuracy: 0.5625\n",
      "Batch number: 014, Training: Loss: 189688.2500, Accuracy: 0.5938\n",
      "Batch number: 015, Training: Loss: 84244.7109, Accuracy: 0.7188\n",
      "Batch number: 016, Training: Loss: 68855.0625, Accuracy: 0.9062\n",
      "Batch number: 017, Training: Loss: 219020.8906, Accuracy: 0.6562\n",
      "Batch number: 018, Training: Loss: 166381.9062, Accuracy: 0.6875\n",
      "Batch number: 019, Training: Loss: 162444.0938, Accuracy: 0.5625\n",
      "Batch number: 020, Training: Loss: 138520.0938, Accuracy: 0.7500\n",
      "Batch number: 021, Training: Loss: 270221.5312, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 96849.6094, Accuracy: 0.6875\n",
      "Batch number: 023, Training: Loss: 140745.9219, Accuracy: 0.6250\n",
      "Batch number: 024, Training: Loss: 140141.7812, Accuracy: 0.6250\n",
      "Batch number: 025, Training: Loss: 130549.3203, Accuracy: 0.7188\n",
      "Batch number: 026, Training: Loss: 298464.5312, Accuracy: 0.5312\n",
      "Batch number: 027, Training: Loss: 130625.1797, Accuracy: 0.6250\n",
      "Batch number: 028, Training: Loss: 260460.4844, Accuracy: 0.5312\n",
      "Batch number: 029, Training: Loss: 112848.7969, Accuracy: 0.5312\n",
      "Batch number: 030, Training: Loss: 129547.0547, Accuracy: 0.5938\n",
      "Batch number: 031, Training: Loss: 50814.5547, Accuracy: 0.6562\n",
      "Batch number: 032, Training: Loss: 103119.6016, Accuracy: 0.7188\n",
      "Batch number: 033, Training: Loss: 138961.1875, Accuracy: 0.5000\n",
      "Batch number: 034, Training: Loss: 169185.8750, Accuracy: 0.4375\n",
      "Batch number: 035, Training: Loss: 97474.6875, Accuracy: 0.4688\n",
      "Batch number: 036, Training: Loss: 90596.3594, Accuracy: 0.5625\n",
      "Batch number: 037, Training: Loss: 69803.6094, Accuracy: 0.5312\n",
      "Batch number: 038, Training: Loss: 42936.3242, Accuracy: 0.5625\n",
      "Batch number: 039, Training: Loss: 43948.4766, Accuracy: 0.6250\n",
      "Batch number: 040, Training: Loss: 31434.0371, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 20814.3867, Accuracy: 0.7500\n",
      "Batch number: 042, Training: Loss: 40223.3320, Accuracy: 0.6562\n",
      "Batch number: 043, Training: Loss: 27381.2852, Accuracy: 0.7188\n",
      "Batch number: 044, Training: Loss: 21841.4609, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 40128.4492, Accuracy: 0.6250\n",
      "Batch number: 046, Training: Loss: 26033.1953, Accuracy: 0.7500\n",
      "Batch number: 047, Training: Loss: 53006.8789, Accuracy: 0.5312\n",
      "Batch number: 048, Training: Loss: 35473.7852, Accuracy: 0.6875\n",
      "Batch number: 049, Training: Loss: 44133.9023, Accuracy: 0.7188\n",
      "Batch number: 050, Training: Loss: 50546.4375, Accuracy: 0.7188\n",
      "Batch number: 051, Training: Loss: 16070.3193, Accuracy: 0.7500\n",
      "Batch number: 052, Training: Loss: 21317.6855, Accuracy: 0.6875\n",
      "Batch number: 053, Training: Loss: 28102.5898, Accuracy: 0.7812\n",
      "Batch number: 054, Training: Loss: 32050.2461, Accuracy: 0.6562\n",
      "Batch number: 055, Training: Loss: 25023.7930, Accuracy: 0.6875\n",
      "Batch number: 056, Training: Loss: 7244.0278, Accuracy: 0.7500\n",
      "Batch number: 057, Training: Loss: 9771.6309, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 9243.5176, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 15512.1445, Accuracy: 0.7500\n",
      "Batch number: 060, Training: Loss: 1263.0806, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 11234.0869, Accuracy: 0.8125\n",
      "Batch number: 062, Training: Loss: 7236.0845, Accuracy: 0.8125\n",
      "Batch number: 063, Training: Loss: 5857.6123, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 7494.3657, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 6165.3271, Accuracy: 0.8125\n",
      "Batch number: 066, Training: Loss: 1430.6823, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 2224.8367, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 7247.4219, Accuracy: 0.8438\n",
      "Batch number: 069, Training: Loss: 12052.4258, Accuracy: 0.8125\n",
      "Batch number: 070, Training: Loss: 5913.6201, Accuracy: 0.6875\n",
      "Batch number: 071, Training: Loss: 8792.4932, Accuracy: 0.7812\n",
      "Batch number: 072, Training: Loss: 3466.9883, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 3875.1191, Accuracy: 0.8438\n",
      "Batch number: 074, Training: Loss: 331.0694, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 23900.1797, Accuracy: 0.7812\n",
      "Batch number: 076, Training: Loss: 18182.0684, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 2063.2239, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 2960.9238, Accuracy: 0.8438\n",
      "Batch number: 079, Training: Loss: 1177.1597, Accuracy: 0.9062\n",
      "Batch number: 080, Training: Loss: 10539.0732, Accuracy: 0.8125\n",
      "Batch number: 081, Training: Loss: 12091.9951, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 10985.2715, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 4816.9043, Accuracy: 0.7812\n",
      "Batch number: 084, Training: Loss: 5030.4927, Accuracy: 0.8438\n",
      "Batch number: 085, Training: Loss: 164.8072, Accuracy: 0.9688\n",
      "Batch number: 086, Training: Loss: 5214.1929, Accuracy: 0.7812\n",
      "Batch number: 087, Training: Loss: 7161.3267, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 9444.6299, Accuracy: 0.8438\n",
      "Batch number: 089, Training: Loss: 16017.7939, Accuracy: 0.8438\n",
      "Batch number: 090, Training: Loss: 523.6702, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 10012.7393, Accuracy: 0.7500\n",
      "Batch number: 092, Training: Loss: 1711.8431, Accuracy: 0.7812\n",
      "Batch number: 093, Training: Loss: 2212.4590, Accuracy: 0.8438\n",
      "Batch number: 094, Training: Loss: 7250.6470, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 3301.3008, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 727.1001, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 1378.5820, Accuracy: 0.8750\n",
      "Batch number: 098, Training: Loss: 675.4495, Accuracy: 0.8438\n",
      "Batch number: 099, Training: Loss: 27471.2793, Accuracy: 0.8438\n",
      "Batch number: 100, Training: Loss: 540.2064, Accuracy: 0.9062\n",
      "Batch number: 101, Training: Loss: 614.2770, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 4934.5269, Accuracy: 0.8438\n",
      "Batch number: 103, Training: Loss: 167.1802, Accuracy: 0.9062\n",
      "Batch number: 104, Training: Loss: 343.3277, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 227.7493, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 970.0452, Accuracy: 0.8125\n",
      "Batch number: 107, Training: Loss: 7026.4009, Accuracy: 0.7812\n",
      "Batch number: 108, Training: Loss: 8180.2461, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 645.2731, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 3770.1138, Accuracy: 0.8438\n",
      "Batch number: 111, Training: Loss: 2707.5867, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 5221.2646, Accuracy: 0.7500\n",
      "Batch number: 113, Training: Loss: 210.7326, Accuracy: 0.9062\n",
      "Batch number: 114, Training: Loss: 3744.0320, Accuracy: 0.9062\n",
      "Batch number: 115, Training: Loss: 1242.5171, Accuracy: 0.8438\n",
      "Batch number: 116, Training: Loss: 1979.2412, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 2603.9553, Accuracy: 0.7500\n",
      "Batch number: 118, Training: Loss: 307.3855, Accuracy: 0.9062\n",
      "Batch number: 119, Training: Loss: 607.9192, Accuracy: 0.9062\n",
      "Batch number: 120, Training: Loss: 4038.5083, Accuracy: 0.8438\n",
      "Batch number: 121, Training: Loss: 3342.8384, Accuracy: 0.8125\n",
      "Batch number: 122, Training: Loss: 5158.2568, Accuracy: 0.8438\n",
      "Batch number: 123, Training: Loss: 4060.9587, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 6909.0488, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 272.9163, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 300.8135, Accuracy: 0.8125\n",
      "Batch number: 127, Training: Loss: 155.1078, Accuracy: 0.9062\n",
      "Batch number: 128, Training: Loss: 123.0742, Accuracy: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 129, Training: Loss: 1859.8978, Accuracy: 0.6250\n",
      "Batch number: 130, Training: Loss: 15643.6172, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 31740.6250, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 240.1855, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 717.2222, Accuracy: 0.8438\n",
      "Batch number: 134, Training: Loss: 11526.2168, Accuracy: 0.8125\n",
      "Batch number: 135, Training: Loss: 4325.5576, Accuracy: 0.9062\n",
      "Batch number: 136, Training: Loss: 1802.7731, Accuracy: 0.7812\n",
      "Batch number: 137, Training: Loss: 2277.6079, Accuracy: 0.8125\n",
      "Batch number: 138, Training: Loss: 1381.6567, Accuracy: 0.7812\n",
      "Batch number: 139, Training: Loss: 84.2407, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 2334.5151, Accuracy: 0.8438\n",
      "Batch number: 141, Training: Loss: 2673.0640, Accuracy: 0.8438\n",
      "Batch number: 142, Training: Loss: 1811.6631, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 836.8773, Accuracy: 0.7500\n",
      "Batch number: 144, Training: Loss: 16442.5762, Accuracy: 0.7188\n",
      "Batch number: 145, Training: Loss: 292.4521, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 565.5123, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 1724.6587, Accuracy: 0.8125\n",
      "Batch number: 148, Training: Loss: 5670.2168, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 2807.5586, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 518.5698, Accuracy: 0.7812\n",
      "Batch number: 151, Training: Loss: 10764.4854, Accuracy: 0.7188\n",
      "Batch number: 152, Training: Loss: 1445.2191, Accuracy: 0.7812\n",
      "Batch number: 153, Training: Loss: 1517.0618, Accuracy: 0.9062\n",
      "Batch number: 154, Training: Loss: 2580.1733, Accuracy: 0.7188\n",
      "Batch number: 155, Training: Loss: 388.5552, Accuracy: 0.8125\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 724.6183, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 1858.0784, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 1175.0081, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 387.8446, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 24.5436, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 3191.7051, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 1266.0356, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 1374.3086, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 2010.5280, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 1752.8680, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 550.7310, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 732.9755, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 1660.4404, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 2478.3801, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 2183.7480, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 1516.9851, Accuracy: 0.5000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 299.1935, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 254.4783, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 1894.1017, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 1519.4316, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 474.1343, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 757.7107, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 1362.4358, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 389.0700, Accuracy: 0.5000\n",
      "Validation Batch number: 066, Validation: Loss: 2495.2593, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 1889.8387, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 3123.7932, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 524.6856, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 12093.7109, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 38.4425, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 1141.4441, Accuracy: 0.5000\n",
      "Validation Batch number: 085, Validation: Loss: 6621.8945, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 301.7474, Accuracy: 0.7500\n",
      "Validation Batch number: 087, Validation: Loss: 1773.4507, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 088, Validation: Loss: 318.6035, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 175.0572, Accuracy: 0.7500\n",
      "Validation Batch number: 100, Validation: Loss: 248.9666, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 724.6546, Accuracy: 0.5000\n",
      "Validation Batch number: 106, Validation: Loss: 429.4320, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 3770.0806, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 426.7512, Accuracy: 0.7500\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 1411.4463, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 344.1586, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 202.0392, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 186.9194, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 744.7295, Accuracy: 0.5000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 122, Validation: Loss: 261.7329, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 2059.0103, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 1713.1499, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 240.1299, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 197.6197, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 514.2258, Accuracy: 0.7500\n",
      "Epoch : 000, Training: Loss : 44168.5908, Accuracy: 76.2010%\n",
      "Validation : Loss : 531.0239, Accuracy: 88.4892%, Time: 55.0452s\n",
      "model for epoch 0 saved\n",
      "Best accuracy achieved so far : 0.8849 on epoch 0\n",
      "Epoch: 2/25\n",
      "Batch number: 000, Training: Loss: 679.5256, Accuracy: 0.8125\n",
      "Batch number: 001, Training: Loss: 15998.0039, Accuracy: 0.6562\n",
      "Batch number: 002, Training: Loss: 6208.1860, Accuracy: 0.6875\n",
      "Batch number: 003, Training: Loss: 5135.1899, Accuracy: 0.6562\n",
      "Batch number: 004, Training: Loss: 401.2234, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 171.9278, Accuracy: 0.8750\n",
      "Batch number: 006, Training: Loss: 714.4839, Accuracy: 0.9062\n",
      "Batch number: 007, Training: Loss: 591.2522, Accuracy: 0.7812\n",
      "Batch number: 008, Training: Loss: 610.3088, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 5718.3643, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 7677.2676, Accuracy: 0.8125\n",
      "Batch number: 011, Training: Loss: 15545.9902, Accuracy: 0.6250\n",
      "Batch number: 012, Training: Loss: 983.1780, Accuracy: 0.7500\n",
      "Batch number: 013, Training: Loss: 2381.5144, Accuracy: 0.7188\n",
      "Batch number: 014, Training: Loss: 603.7565, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 13827.9404, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 3116.5500, Accuracy: 0.8438\n",
      "Batch number: 018, Training: Loss: 1425.0343, Accuracy: 0.6875\n",
      "Batch number: 019, Training: Loss: 1318.6538, Accuracy: 0.7812\n",
      "Batch number: 020, Training: Loss: 2803.0720, Accuracy: 0.8125\n",
      "Batch number: 021, Training: Loss: 5005.2559, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 1452.5795, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 782.4090, Accuracy: 0.8125\n",
      "Batch number: 024, Training: Loss: 379.2631, Accuracy: 0.8125\n",
      "Batch number: 025, Training: Loss: 1109.9355, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 2963.4375, Accuracy: 0.7188\n",
      "Batch number: 027, Training: Loss: 392.6404, Accuracy: 0.9062\n",
      "Batch number: 028, Training: Loss: 2243.3076, Accuracy: 0.8438\n",
      "Batch number: 029, Training: Loss: 478.6565, Accuracy: 0.9062\n",
      "Batch number: 030, Training: Loss: 1201.4915, Accuracy: 0.8438\n",
      "Batch number: 031, Training: Loss: 710.0101, Accuracy: 0.8438\n",
      "Batch number: 032, Training: Loss: 1476.5076, Accuracy: 0.7812\n",
      "Batch number: 033, Training: Loss: 748.7599, Accuracy: 0.7812\n",
      "Batch number: 034, Training: Loss: 2043.9490, Accuracy: 0.8438\n",
      "Batch number: 035, Training: Loss: 956.6108, Accuracy: 0.9062\n",
      "Batch number: 036, Training: Loss: 850.3469, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 556.8217, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 983.2883, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 416.0798, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 684.0366, Accuracy: 0.8125\n",
      "Batch number: 041, Training: Loss: 1546.1459, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 239.5573, Accuracy: 0.8438\n",
      "Batch number: 043, Training: Loss: 4477.1777, Accuracy: 0.8438\n",
      "Batch number: 044, Training: Loss: 4198.9775, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 721.3256, Accuracy: 0.8438\n",
      "Batch number: 046, Training: Loss: 1468.9790, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 2794.4897, Accuracy: 0.7188\n",
      "Batch number: 048, Training: Loss: 1236.3341, Accuracy: 0.8125\n",
      "Batch number: 049, Training: Loss: 1037.0925, Accuracy: 0.7812\n",
      "Batch number: 050, Training: Loss: 1205.4106, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 1171.4128, Accuracy: 0.6875\n",
      "Batch number: 052, Training: Loss: 491.9568, Accuracy: 0.8125\n",
      "Batch number: 053, Training: Loss: 911.1330, Accuracy: 0.7500\n",
      "Batch number: 054, Training: Loss: 444.2560, Accuracy: 0.9062\n",
      "Batch number: 055, Training: Loss: 1272.5334, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 666.5807, Accuracy: 0.7812\n",
      "Batch number: 057, Training: Loss: 366.7865, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 297.3485, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 1060.7449, Accuracy: 0.8438\n",
      "Batch number: 060, Training: Loss: 512.6796, Accuracy: 0.8125\n",
      "Batch number: 061, Training: Loss: 3026.8894, Accuracy: 0.8125\n",
      "Batch number: 062, Training: Loss: 876.7594, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 227.0845, Accuracy: 0.8438\n",
      "Batch number: 064, Training: Loss: 1300.3237, Accuracy: 0.8438\n",
      "Batch number: 065, Training: Loss: 1463.6998, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 547.8350, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 985.7939, Accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 068, Training: Loss: 515.3684, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 779.8513, Accuracy: 0.8125\n",
      "Batch number: 070, Training: Loss: 325.7753, Accuracy: 0.7812\n",
      "Batch number: 071, Training: Loss: 475.4532, Accuracy: 0.8438\n",
      "Batch number: 072, Training: Loss: 1325.3293, Accuracy: 0.8438\n",
      "Batch number: 073, Training: Loss: 794.3320, Accuracy: 0.7812\n",
      "Batch number: 074, Training: Loss: 522.4557, Accuracy: 0.8438\n",
      "Batch number: 075, Training: Loss: 1553.8423, Accuracy: 0.7188\n",
      "Batch number: 076, Training: Loss: 1370.0671, Accuracy: 0.7500\n",
      "Batch number: 077, Training: Loss: 96.7980, Accuracy: 0.9688\n",
      "Batch number: 078, Training: Loss: 1115.1835, Accuracy: 0.7500\n",
      "Batch number: 079, Training: Loss: 384.4960, Accuracy: 0.8438\n",
      "Batch number: 080, Training: Loss: 1515.9148, Accuracy: 0.7188\n",
      "Batch number: 081, Training: Loss: 546.8597, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 253.5388, Accuracy: 0.8438\n",
      "Batch number: 083, Training: Loss: 700.3607, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 665.9464, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 524.6876, Accuracy: 0.9062\n",
      "Batch number: 086, Training: Loss: 1326.1450, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 1709.4055, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 2566.6477, Accuracy: 0.7812\n",
      "Batch number: 089, Training: Loss: 596.2258, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 164.3300, Accuracy: 0.8438\n",
      "Batch number: 091, Training: Loss: 1485.4346, Accuracy: 0.7500\n",
      "Batch number: 092, Training: Loss: 229.5123, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 1427.8547, Accuracy: 0.7500\n",
      "Batch number: 094, Training: Loss: 659.4852, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 394.2427, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 1172.2236, Accuracy: 0.7812\n",
      "Batch number: 097, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 387.4820, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 442.2750, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 635.0854, Accuracy: 0.7812\n",
      "Batch number: 101, Training: Loss: 404.7798, Accuracy: 0.8438\n",
      "Batch number: 102, Training: Loss: 472.2631, Accuracy: 0.8438\n",
      "Batch number: 103, Training: Loss: 424.3047, Accuracy: 0.8438\n",
      "Batch number: 104, Training: Loss: 1096.7861, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 589.8951, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 468.6588, Accuracy: 0.8438\n",
      "Batch number: 107, Training: Loss: 983.8513, Accuracy: 0.7500\n",
      "Batch number: 108, Training: Loss: 884.7852, Accuracy: 0.8125\n",
      "Batch number: 109, Training: Loss: 147.4346, Accuracy: 0.9062\n",
      "Batch number: 110, Training: Loss: 228.9919, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 394.2441, Accuracy: 0.8438\n",
      "Batch number: 112, Training: Loss: 1117.0033, Accuracy: 0.9062\n",
      "Batch number: 113, Training: Loss: 1043.9500, Accuracy: 0.8438\n",
      "Batch number: 114, Training: Loss: 317.0855, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 577.7336, Accuracy: 0.8125\n",
      "Batch number: 116, Training: Loss: 320.4158, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 245.4463, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 1140.2878, Accuracy: 0.8125\n",
      "Batch number: 119, Training: Loss: 668.3561, Accuracy: 0.8438\n",
      "Batch number: 120, Training: Loss: 40.1328, Accuracy: 0.9062\n",
      "Batch number: 121, Training: Loss: 105.6414, Accuracy: 0.9375\n",
      "Batch number: 122, Training: Loss: 217.0002, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 575.6677, Accuracy: 0.8438\n",
      "Batch number: 124, Training: Loss: 888.9557, Accuracy: 0.7188\n",
      "Batch number: 125, Training: Loss: 86.7708, Accuracy: 0.9062\n",
      "Batch number: 126, Training: Loss: 392.0471, Accuracy: 0.7812\n",
      "Batch number: 127, Training: Loss: 1000.5482, Accuracy: 0.7812\n",
      "Batch number: 128, Training: Loss: 120.0409, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 709.5478, Accuracy: 0.7500\n",
      "Batch number: 130, Training: Loss: 217.8893, Accuracy: 0.9062\n",
      "Batch number: 131, Training: Loss: 2131.3428, Accuracy: 0.7188\n",
      "Batch number: 132, Training: Loss: 93.4899, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 804.9036, Accuracy: 0.8125\n",
      "Batch number: 134, Training: Loss: 531.2884, Accuracy: 0.8750\n",
      "Batch number: 135, Training: Loss: 469.4601, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 810.1351, Accuracy: 0.8125\n",
      "Batch number: 137, Training: Loss: 486.0758, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 1202.0741, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 457.5108, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 329.0690, Accuracy: 0.8438\n",
      "Batch number: 141, Training: Loss: 833.0768, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 393.5574, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 808.6682, Accuracy: 0.7188\n",
      "Batch number: 144, Training: Loss: 1119.6554, Accuracy: 0.8438\n",
      "Batch number: 145, Training: Loss: 536.6918, Accuracy: 0.9062\n",
      "Batch number: 146, Training: Loss: 226.9554, Accuracy: 0.8438\n",
      "Batch number: 147, Training: Loss: 866.3124, Accuracy: 0.7500\n",
      "Batch number: 148, Training: Loss: 1088.2024, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 552.2620, Accuracy: 0.8438\n",
      "Batch number: 150, Training: Loss: 242.3378, Accuracy: 0.7812\n",
      "Batch number: 151, Training: Loss: 427.8255, Accuracy: 0.8438\n",
      "Batch number: 152, Training: Loss: 741.9359, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 842.5828, Accuracy: 0.8125\n",
      "Batch number: 154, Training: Loss: 1194.0531, Accuracy: 0.8438\n",
      "Batch number: 155, Training: Loss: 591.6276, Accuracy: 0.7812\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 221.2151, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 1215.8640, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 816.3269, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 554.0621, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 1826.3779, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 1106.7590, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 1992.5754, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 1170.9175, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 305.9920, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 1004.9471, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 774.1935, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 33.4888, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 1185.1013, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 916.2383, Accuracy: 0.5000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 299.6958, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 2.6612, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 1435.1592, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 868.3585, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 190.2162, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 310.4543, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 739.7001, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 32.6414, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 1626.0811, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 1310.0731, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 343.9149, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 229.5810, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 1502.9608, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 306.7891, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 1069.5503, Accuracy: 0.5000\n",
      "Validation Batch number: 085, Validation: Loss: 3284.5989, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 80.0624, Accuracy: 0.7500\n",
      "Validation Batch number: 087, Validation: Loss: 1616.7400, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 700.7902, Accuracy: 0.5000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 524.1246, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 604.5723, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 695.3182, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 2144.9097, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 1891.0261, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 330.0542, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 1027.0808, Accuracy: 0.5000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 1175.1892, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 1172.1589, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 1128.4077, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 338.3354, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 560.7618, Accuracy: 0.7500\n",
      "Epoch : 001, Training: Loss : 1320.8669, Accuracy: 83.2266%\n",
      "Validation : Loss : 292.5613, Accuracy: 90.4676%, Time: 54.0769s\n",
      "model for epoch 1 saved\n",
      "Best accuracy achieved so far : 0.9047 on epoch 1\n",
      "Epoch: 3/25\n",
      "Batch number: 000, Training: Loss: 141.7817, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 2436.2646, Accuracy: 0.7500\n",
      "Batch number: 002, Training: Loss: 881.4438, Accuracy: 0.7500\n",
      "Batch number: 003, Training: Loss: 1139.9395, Accuracy: 0.8125\n",
      "Batch number: 004, Training: Loss: 670.7890, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 301.4442, Accuracy: 0.8750\n",
      "Batch number: 006, Training: Loss: 413.2484, Accuracy: 0.8438\n",
      "Batch number: 007, Training: Loss: 799.7654, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 377.3221, Accuracy: 0.8438\n",
      "Batch number: 009, Training: Loss: 477.3477, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 461.9175, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 011, Training: Loss: 775.6402, Accuracy: 0.7812\n",
      "Batch number: 012, Training: Loss: 493.8561, Accuracy: 0.9062\n",
      "Batch number: 013, Training: Loss: 442.2932, Accuracy: 0.7812\n",
      "Batch number: 014, Training: Loss: 164.9214, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 227.9613, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 1858.3508, Accuracy: 0.6250\n",
      "Batch number: 017, Training: Loss: 738.3302, Accuracy: 0.8438\n",
      "Batch number: 018, Training: Loss: 557.7275, Accuracy: 0.8438\n",
      "Batch number: 019, Training: Loss: 506.2981, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 375.1873, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 842.5221, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 235.2711, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 378.2838, Accuracy: 0.7500\n",
      "Batch number: 024, Training: Loss: 696.5094, Accuracy: 0.7500\n",
      "Batch number: 025, Training: Loss: 866.7950, Accuracy: 0.7500\n",
      "Batch number: 026, Training: Loss: 1254.3357, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 453.0158, Accuracy: 0.8438\n",
      "Batch number: 028, Training: Loss: 434.8577, Accuracy: 0.8438\n",
      "Batch number: 029, Training: Loss: 190.6166, Accuracy: 0.8750\n",
      "Batch number: 030, Training: Loss: 672.1147, Accuracy: 0.9062\n",
      "Batch number: 031, Training: Loss: 1389.5732, Accuracy: 0.7188\n",
      "Batch number: 032, Training: Loss: 860.6808, Accuracy: 0.7188\n",
      "Batch number: 033, Training: Loss: 588.5795, Accuracy: 0.7188\n",
      "Batch number: 034, Training: Loss: 218.1211, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 202.7487, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 442.1399, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 476.1181, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 1383.8896, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 355.7281, Accuracy: 0.8438\n",
      "Batch number: 040, Training: Loss: 98.2242, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 130.8171, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 484.5828, Accuracy: 0.7500\n",
      "Batch number: 043, Training: Loss: 301.3145, Accuracy: 0.8125\n",
      "Batch number: 044, Training: Loss: 557.3797, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 535.2072, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 841.1494, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 533.0711, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 548.3442, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 99.2215, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 945.5958, Accuracy: 0.8125\n",
      "Batch number: 051, Training: Loss: 681.9702, Accuracy: 0.7812\n",
      "Batch number: 052, Training: Loss: 174.7980, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 812.9172, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 675.7588, Accuracy: 0.8438\n",
      "Batch number: 055, Training: Loss: 363.2777, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 429.1958, Accuracy: 0.8438\n",
      "Batch number: 057, Training: Loss: 99.9111, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 405.7164, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 527.8127, Accuracy: 0.7500\n",
      "Batch number: 060, Training: Loss: 276.6841, Accuracy: 0.8438\n",
      "Batch number: 061, Training: Loss: 2173.8086, Accuracy: 0.8125\n",
      "Batch number: 062, Training: Loss: 377.9236, Accuracy: 0.8438\n",
      "Batch number: 063, Training: Loss: 194.1895, Accuracy: 0.8438\n",
      "Batch number: 064, Training: Loss: 223.4939, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 567.5452, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 295.9315, Accuracy: 0.8438\n",
      "Batch number: 067, Training: Loss: 657.8834, Accuracy: 0.9062\n",
      "Batch number: 068, Training: Loss: 55.5830, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 221.5414, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 451.3596, Accuracy: 0.9062\n",
      "Batch number: 071, Training: Loss: 690.3688, Accuracy: 0.7500\n",
      "Batch number: 072, Training: Loss: 547.7480, Accuracy: 0.9062\n",
      "Batch number: 073, Training: Loss: 476.4429, Accuracy: 0.8438\n",
      "Batch number: 074, Training: Loss: 205.4966, Accuracy: 0.9062\n",
      "Batch number: 075, Training: Loss: 616.3939, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 668.7990, Accuracy: 0.7188\n",
      "Batch number: 077, Training: Loss: 344.6800, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 579.9099, Accuracy: 0.7500\n",
      "Batch number: 079, Training: Loss: 152.5003, Accuracy: 0.8750\n",
      "Batch number: 080, Training: Loss: 223.8698, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 512.1882, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 562.7265, Accuracy: 0.8438\n",
      "Batch number: 083, Training: Loss: 268.4657, Accuracy: 0.8125\n",
      "Batch number: 084, Training: Loss: 310.0670, Accuracy: 0.8438\n",
      "Batch number: 085, Training: Loss: 169.6734, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 476.2969, Accuracy: 0.8125\n",
      "Batch number: 087, Training: Loss: 447.2796, Accuracy: 0.8438\n",
      "Batch number: 088, Training: Loss: 1025.3300, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 320.4483, Accuracy: 0.9062\n",
      "Batch number: 090, Training: Loss: 157.9473, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 493.8922, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 355.1372, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 251.0650, Accuracy: 0.9688\n",
      "Batch number: 094, Training: Loss: 141.2671, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 74.0016, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 746.1064, Accuracy: 0.9062\n",
      "Batch number: 097, Training: Loss: 373.4915, Accuracy: 0.8750\n",
      "Batch number: 098, Training: Loss: 195.0697, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 569.0845, Accuracy: 0.7500\n",
      "Batch number: 100, Training: Loss: 328.3701, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 440.8877, Accuracy: 0.7188\n",
      "Batch number: 102, Training: Loss: 421.7305, Accuracy: 0.7500\n",
      "Batch number: 103, Training: Loss: 83.0619, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 382.2220, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 635.2325, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 372.0176, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 298.0146, Accuracy: 0.8438\n",
      "Batch number: 108, Training: Loss: 422.1821, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 32.5458, Accuracy: 0.9688\n",
      "Batch number: 110, Training: Loss: 663.7626, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 540.0311, Accuracy: 0.8438\n",
      "Batch number: 112, Training: Loss: 398.8540, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 120.8811, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 0.4244, Accuracy: 0.9688\n",
      "Batch number: 115, Training: Loss: 536.8907, Accuracy: 0.8438\n",
      "Batch number: 116, Training: Loss: 455.1804, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 340.1621, Accuracy: 0.8438\n",
      "Batch number: 118, Training: Loss: 380.7003, Accuracy: 0.8438\n",
      "Batch number: 119, Training: Loss: 810.3370, Accuracy: 0.7500\n",
      "Batch number: 120, Training: Loss: 324.4709, Accuracy: 0.8438\n",
      "Batch number: 121, Training: Loss: 751.5378, Accuracy: 0.7188\n",
      "Batch number: 122, Training: Loss: 300.1125, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 129.7415, Accuracy: 0.9062\n",
      "Batch number: 124, Training: Loss: 759.5427, Accuracy: 0.7812\n",
      "Batch number: 125, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 561.4196, Accuracy: 0.7812\n",
      "Batch number: 127, Training: Loss: 285.9431, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 278.9361, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 512.8259, Accuracy: 0.7812\n",
      "Batch number: 130, Training: Loss: 319.1237, Accuracy: 0.8125\n",
      "Batch number: 131, Training: Loss: 331.8024, Accuracy: 0.8125\n",
      "Batch number: 132, Training: Loss: 384.1400, Accuracy: 0.8438\n",
      "Batch number: 133, Training: Loss: 852.5844, Accuracy: 0.9062\n",
      "Batch number: 134, Training: Loss: 63.6007, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 537.7298, Accuracy: 0.8438\n",
      "Batch number: 136, Training: Loss: 411.4024, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 1512.3225, Accuracy: 0.7812\n",
      "Batch number: 138, Training: Loss: 514.3315, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 17.7396, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 88.8195, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 721.0372, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 29.0509, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 603.3844, Accuracy: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 144, Training: Loss: 577.9210, Accuracy: 0.8438\n",
      "Batch number: 145, Training: Loss: 476.4628, Accuracy: 0.7500\n",
      "Batch number: 146, Training: Loss: 118.4287, Accuracy: 0.9688\n",
      "Batch number: 147, Training: Loss: 696.8038, Accuracy: 0.6250\n",
      "Batch number: 148, Training: Loss: 1014.3241, Accuracy: 0.7500\n",
      "Batch number: 149, Training: Loss: 230.1702, Accuracy: 0.8438\n",
      "Batch number: 150, Training: Loss: 122.5285, Accuracy: 0.9062\n",
      "Batch number: 151, Training: Loss: 320.1218, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 395.0524, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 154, Training: Loss: 1060.8606, Accuracy: 0.8125\n",
      "Batch number: 155, Training: Loss: 715.4443, Accuracy: 0.8438\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 906.7290, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 345.4418, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 1003.4788, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 1550.6244, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 1898.6752, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 1535.2968, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 1119.4260, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 293.8437, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 675.9515, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 208.4274, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 392.5620, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 629.9690, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 1018.4775, Accuracy: 0.5000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 292.2898, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 1352.0636, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 1236.7771, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 440.2077, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 594.2406, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 1194.8307, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 1113.1777, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 1143.8706, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 376.9352, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 1544.2515, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 619.0314, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 721.0722, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 2741.6604, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 1304.7227, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 217.2623, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 10.9213, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 344.3675, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 271.3312, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 2071.3296, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 37.5545, Accuracy: 0.7500\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 1282.8560, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 220.8432, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 280.8200, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 1562.4922, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 1110.3291, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 1248.8893, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 832.8537, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 200.0169, Accuracy: 0.7500\n",
      "Epoch : 002, Training: Loss : 490.4076, Accuracy: 84.6477%\n",
      "Validation : Loss : 258.6036, Accuracy: 91.3669%, Time: 53.1123s\n",
      "model for epoch 2 saved\n",
      "Best accuracy achieved so far : 0.9137 on epoch 2\n",
      "Epoch: 4/25\n",
      "Batch number: 000, Training: Loss: 511.6378, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 2558.3005, Accuracy: 0.7188\n",
      "Batch number: 002, Training: Loss: 584.6884, Accuracy: 0.8125\n",
      "Batch number: 003, Training: Loss: 248.3580, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 94.7071, Accuracy: 0.9062\n",
      "Batch number: 005, Training: Loss: 396.4005, Accuracy: 0.8438\n",
      "Batch number: 006, Training: Loss: 215.5018, Accuracy: 0.9062\n",
      "Batch number: 007, Training: Loss: 837.2966, Accuracy: 0.6250\n",
      "Batch number: 008, Training: Loss: 270.0414, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 655.0791, Accuracy: 0.8438\n",
      "Batch number: 010, Training: Loss: 467.5332, Accuracy: 0.7812\n",
      "Batch number: 011, Training: Loss: 1157.3376, Accuracy: 0.7812\n",
      "Batch number: 012, Training: Loss: 174.3689, Accuracy: 0.8438\n",
      "Batch number: 013, Training: Loss: 340.0059, Accuracy: 0.8125\n",
      "Batch number: 014, Training: Loss: 33.9673, Accuracy: 0.9688\n",
      "Batch number: 015, Training: Loss: 577.8649, Accuracy: 0.8125\n",
      "Batch number: 016, Training: Loss: 996.8898, Accuracy: 0.7500\n",
      "Batch number: 017, Training: Loss: 186.8261, Accuracy: 0.9688\n",
      "Batch number: 018, Training: Loss: 108.6747, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 342.1691, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 624.8729, Accuracy: 0.7500\n",
      "Batch number: 021, Training: Loss: 331.5511, Accuracy: 0.8438\n",
      "Batch number: 022, Training: Loss: 132.6044, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 339.4555, Accuracy: 0.8125\n",
      "Batch number: 024, Training: Loss: 252.3760, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 548.3286, Accuracy: 0.7812\n",
      "Batch number: 026, Training: Loss: 1481.7820, Accuracy: 0.7812\n",
      "Batch number: 027, Training: Loss: 369.5388, Accuracy: 0.7812\n",
      "Batch number: 028, Training: Loss: 910.4231, Accuracy: 0.7500\n",
      "Batch number: 029, Training: Loss: 187.4818, Accuracy: 0.9688\n",
      "Batch number: 030, Training: Loss: 144.7924, Accuracy: 0.9062\n",
      "Batch number: 031, Training: Loss: 877.6504, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 183.2070, Accuracy: 0.9062\n",
      "Batch number: 033, Training: Loss: 259.6026, Accuracy: 0.8438\n",
      "Batch number: 034, Training: Loss: 18.3644, Accuracy: 0.9688\n",
      "Batch number: 035, Training: Loss: 335.5909, Accuracy: 0.8438\n",
      "Batch number: 036, Training: Loss: 290.9945, Accuracy: 0.8438\n",
      "Batch number: 037, Training: Loss: 420.7809, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 155.3231, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 285.5993, Accuracy: 0.8438\n",
      "Batch number: 040, Training: Loss: 280.7663, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 450.1850, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 405.8823, Accuracy: 0.8125\n",
      "Batch number: 043, Training: Loss: 440.8367, Accuracy: 0.8125\n",
      "Batch number: 044, Training: Loss: 307.8595, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 297.6993, Accuracy: 0.8125\n",
      "Batch number: 046, Training: Loss: 387.3792, Accuracy: 0.8438\n",
      "Batch number: 047, Training: Loss: 669.4662, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 23.6215, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 298.2382, Accuracy: 0.8438\n",
      "Batch number: 050, Training: Loss: 1575.1620, Accuracy: 0.7812\n",
      "Batch number: 051, Training: Loss: 292.7584, Accuracy: 0.8438\n",
      "Batch number: 052, Training: Loss: 80.1716, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 443.5259, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 394.4384, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 202.2392, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 775.1424, Accuracy: 0.7188\n",
      "Batch number: 057, Training: Loss: 221.5614, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 504.2829, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 389.3342, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 134.7488, Accuracy: 0.8438\n",
      "Batch number: 061, Training: Loss: 1376.6611, Accuracy: 0.8125\n",
      "Batch number: 062, Training: Loss: 358.7331, Accuracy: 0.7812\n",
      "Batch number: 063, Training: Loss: 255.2114, Accuracy: 0.8438\n",
      "Batch number: 064, Training: Loss: 339.5611, Accuracy: 0.9062\n",
      "Batch number: 065, Training: Loss: 504.3311, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 479.1735, Accuracy: 0.8750\n",
      "Batch number: 067, Training: Loss: 1122.8062, Accuracy: 0.8438\n",
      "Batch number: 068, Training: Loss: 318.4570, Accuracy: 0.8125\n",
      "Batch number: 069, Training: Loss: 218.1037, Accuracy: 0.9062\n",
      "Batch number: 070, Training: Loss: 140.5288, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 383.5439, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 325.1947, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 171.4766, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 207.7334, Accuracy: 0.9062\n",
      "Batch number: 075, Training: Loss: 581.5138, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 930.2300, Accuracy: 0.6875\n",
      "Batch number: 077, Training: Loss: 185.5374, Accuracy: 0.8438\n",
      "Batch number: 078, Training: Loss: 275.4983, Accuracy: 0.9062\n",
      "Batch number: 079, Training: Loss: 147.6480, Accuracy: 0.8125\n",
      "Batch number: 080, Training: Loss: 312.9393, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 964.9791, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 356.3376, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 406.4576, Accuracy: 0.9062\n",
      "Batch number: 084, Training: Loss: 349.5074, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 63.0656, Accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 086, Training: Loss: 228.7256, Accuracy: 0.8438\n",
      "Batch number: 087, Training: Loss: 527.7550, Accuracy: 0.7812\n",
      "Batch number: 088, Training: Loss: 238.8153, Accuracy: 0.8125\n",
      "Batch number: 089, Training: Loss: 11.8312, Accuracy: 0.9688\n",
      "Batch number: 090, Training: Loss: 239.3406, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 319.0813, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 174.6805, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 603.9626, Accuracy: 0.7500\n",
      "Batch number: 094, Training: Loss: 292.0737, Accuracy: 0.9062\n",
      "Batch number: 095, Training: Loss: 260.6985, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 307.3218, Accuracy: 0.9062\n",
      "Batch number: 097, Training: Loss: 273.4055, Accuracy: 0.8438\n",
      "Batch number: 098, Training: Loss: 405.7808, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 357.8456, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 336.1550, Accuracy: 0.8438\n",
      "Batch number: 101, Training: Loss: 238.8470, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 122.4494, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 126.9154, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 324.7185, Accuracy: 0.7812\n",
      "Batch number: 105, Training: Loss: 504.8818, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 47.8257, Accuracy: 0.9062\n",
      "Batch number: 107, Training: Loss: 322.2422, Accuracy: 0.8125\n",
      "Batch number: 108, Training: Loss: 373.8194, Accuracy: 0.7500\n",
      "Batch number: 109, Training: Loss: 198.5840, Accuracy: 0.8750\n",
      "Batch number: 110, Training: Loss: 123.1710, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 266.1361, Accuracy: 0.8438\n",
      "Batch number: 112, Training: Loss: 442.2357, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 123.6926, Accuracy: 0.9062\n",
      "Batch number: 114, Training: Loss: 0.2579, Accuracy: 0.9688\n",
      "Batch number: 115, Training: Loss: 484.9973, Accuracy: 0.7812\n",
      "Batch number: 116, Training: Loss: 138.3662, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 282.6452, Accuracy: 0.8125\n",
      "Batch number: 118, Training: Loss: 318.7186, Accuracy: 0.9062\n",
      "Batch number: 119, Training: Loss: 282.6476, Accuracy: 0.9062\n",
      "Batch number: 120, Training: Loss: 281.5650, Accuracy: 0.8438\n",
      "Batch number: 121, Training: Loss: 425.0111, Accuracy: 0.7188\n",
      "Batch number: 122, Training: Loss: 165.4595, Accuracy: 0.9062\n",
      "Batch number: 123, Training: Loss: 180.1147, Accuracy: 0.9062\n",
      "Batch number: 124, Training: Loss: 423.8001, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 31.5346, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 75.1320, Accuracy: 0.9062\n",
      "Batch number: 127, Training: Loss: 353.9532, Accuracy: 0.8125\n",
      "Batch number: 128, Training: Loss: 209.0831, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 318.2579, Accuracy: 0.8438\n",
      "Batch number: 130, Training: Loss: 177.7008, Accuracy: 0.8438\n",
      "Batch number: 131, Training: Loss: 746.4380, Accuracy: 0.8125\n",
      "Batch number: 132, Training: Loss: 415.9931, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 387.2755, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 443.0660, Accuracy: 0.9062\n",
      "Batch number: 135, Training: Loss: 135.2593, Accuracy: 0.9062\n",
      "Batch number: 136, Training: Loss: 657.6213, Accuracy: 0.8438\n",
      "Batch number: 137, Training: Loss: 433.3218, Accuracy: 0.8125\n",
      "Batch number: 138, Training: Loss: 505.1470, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 157.1694, Accuracy: 0.9062\n",
      "Batch number: 140, Training: Loss: 492.0884, Accuracy: 0.8438\n",
      "Batch number: 141, Training: Loss: 205.7697, Accuracy: 0.8438\n",
      "Batch number: 142, Training: Loss: 378.7330, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 302.1710, Accuracy: 0.7812\n",
      "Batch number: 144, Training: Loss: 379.7977, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 135.8624, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 133.6893, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 574.2954, Accuracy: 0.8125\n",
      "Batch number: 148, Training: Loss: 865.7838, Accuracy: 0.9062\n",
      "Batch number: 149, Training: Loss: 297.9880, Accuracy: 0.8438\n",
      "Batch number: 150, Training: Loss: 56.1915, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 424.6215, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 365.4536, Accuracy: 0.9062\n",
      "Batch number: 153, Training: Loss: 240.4721, Accuracy: 0.9062\n",
      "Batch number: 154, Training: Loss: 728.8549, Accuracy: 0.9062\n",
      "Batch number: 155, Training: Loss: 364.4054, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 697.8822, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 883.0643, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 1713.6682, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 377.4729, Accuracy: 0.7500\n",
      "Validation Batch number: 027, Validation: Loss: 617.0547, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 96.8213, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 379.5293, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 559.2509, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 180.0172, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 140.0031, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 176.0024, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 941.5919, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 116.6191, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 400.6074, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 107.1092, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 847.6157, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 403.2865, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 782.3608, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 949.4488, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 610.2773, Accuracy: 0.5000\n",
      "Validation Batch number: 085, Validation: Loss: 2170.1411, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 792.0392, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 525.8688, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 1292.8265, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 600.5220, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 143.7220, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 918.9880, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 674.5912, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 508.1655, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 404.9489, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 250.1901, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 124.4626, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 25.0182, Accuracy: 0.7500\n",
      "Epoch : 003, Training: Loss : 384.4881, Accuracy: 85.3883%\n",
      "Validation : Loss : 139.6487, Accuracy: 93.1655%, Time: 52.1788s\n",
      "model for epoch 3 saved\n",
      "Best accuracy achieved so far : 0.9317 on epoch 3\n",
      "Epoch: 5/25\n",
      "Batch number: 000, Training: Loss: 642.9921, Accuracy: 0.8125\n",
      "Batch number: 001, Training: Loss: 1083.2482, Accuracy: 0.7188\n",
      "Batch number: 002, Training: Loss: 523.3582, Accuracy: 0.7812\n",
      "Batch number: 003, Training: Loss: 553.9728, Accuracy: 0.8125\n",
      "Batch number: 004, Training: Loss: 501.0015, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 19.9336, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 242.0217, Accuracy: 0.9062\n",
      "Batch number: 007, Training: Loss: 339.8996, Accuracy: 0.7812\n",
      "Batch number: 008, Training: Loss: 348.8515, Accuracy: 0.8125\n",
      "Batch number: 009, Training: Loss: 160.8128, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 324.0321, Accuracy: 0.8125\n",
      "Batch number: 011, Training: Loss: 275.1075, Accuracy: 0.9062\n",
      "Batch number: 012, Training: Loss: 251.8356, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 426.5649, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 201.0033, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 173.3705, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 618.4645, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 485.0399, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 60.7111, Accuracy: 0.9062\n",
      "Batch number: 019, Training: Loss: 577.0867, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 844.4899, Accuracy: 0.8438\n",
      "Batch number: 021, Training: Loss: 747.3432, Accuracy: 0.7188\n",
      "Batch number: 022, Training: Loss: 47.5541, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 71.2092, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 199.9986, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 197.8501, Accuracy: 0.8125\n",
      "Batch number: 026, Training: Loss: 499.7313, Accuracy: 0.8438\n",
      "Batch number: 027, Training: Loss: 153.0777, Accuracy: 0.9062\n",
      "Batch number: 028, Training: Loss: 93.8002, Accuracy: 0.8750\n",
      "Batch number: 029, Training: Loss: 157.7070, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 286.7388, Accuracy: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 031, Training: Loss: 266.0273, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 405.7299, Accuracy: 0.8125\n",
      "Batch number: 033, Training: Loss: 208.7097, Accuracy: 0.7500\n",
      "Batch number: 034, Training: Loss: 367.2122, Accuracy: 0.8125\n",
      "Batch number: 035, Training: Loss: 522.9421, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 244.9585, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 120.4871, Accuracy: 0.8438\n",
      "Batch number: 038, Training: Loss: 215.4442, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 346.9498, Accuracy: 0.8438\n",
      "Batch number: 040, Training: Loss: 266.4713, Accuracy: 0.8438\n",
      "Batch number: 041, Training: Loss: 3.7470, Accuracy: 0.9688\n",
      "Batch number: 042, Training: Loss: 162.4068, Accuracy: 0.8438\n",
      "Batch number: 043, Training: Loss: 79.2163, Accuracy: 0.9062\n",
      "Batch number: 044, Training: Loss: 244.2271, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 237.7764, Accuracy: 0.8438\n",
      "Batch number: 046, Training: Loss: 125.8720, Accuracy: 0.8438\n",
      "Batch number: 047, Training: Loss: 223.8723, Accuracy: 0.8438\n",
      "Batch number: 048, Training: Loss: 172.4900, Accuracy: 0.9062\n",
      "Batch number: 049, Training: Loss: 341.3122, Accuracy: 0.8438\n",
      "Batch number: 050, Training: Loss: 512.9645, Accuracy: 0.7812\n",
      "Batch number: 051, Training: Loss: 487.9153, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 57.1918, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 241.7772, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 284.1526, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 491.0729, Accuracy: 0.8438\n",
      "Batch number: 056, Training: Loss: 153.8381, Accuracy: 0.7812\n",
      "Batch number: 057, Training: Loss: 156.3644, Accuracy: 0.8438\n",
      "Batch number: 058, Training: Loss: 399.5075, Accuracy: 0.8125\n",
      "Batch number: 059, Training: Loss: 243.2255, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 308.6250, Accuracy: 0.8125\n",
      "Batch number: 061, Training: Loss: 380.1212, Accuracy: 0.8438\n",
      "Batch number: 062, Training: Loss: 446.8629, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 437.9072, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 614.5969, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 272.6658, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 43.0470, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 209.9540, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 158.0793, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 361.4754, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 194.7179, Accuracy: 0.8125\n",
      "Batch number: 071, Training: Loss: 228.3895, Accuracy: 0.8438\n",
      "Batch number: 072, Training: Loss: 216.5941, Accuracy: 0.9062\n",
      "Batch number: 073, Training: Loss: 132.2803, Accuracy: 0.9062\n",
      "Batch number: 074, Training: Loss: 149.4493, Accuracy: 0.9062\n",
      "Batch number: 075, Training: Loss: 454.0092, Accuracy: 0.8438\n",
      "Batch number: 076, Training: Loss: 279.1354, Accuracy: 0.7500\n",
      "Batch number: 077, Training: Loss: 328.6507, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 331.1727, Accuracy: 0.8125\n",
      "Batch number: 079, Training: Loss: 48.1181, Accuracy: 0.9062\n",
      "Batch number: 080, Training: Loss: 213.2485, Accuracy: 0.8438\n",
      "Batch number: 081, Training: Loss: 297.1027, Accuracy: 0.8438\n",
      "Batch number: 082, Training: Loss: 299.2575, Accuracy: 0.8125\n",
      "Batch number: 083, Training: Loss: 548.6115, Accuracy: 0.7500\n",
      "Batch number: 084, Training: Loss: 222.9556, Accuracy: 0.7812\n",
      "Batch number: 085, Training: Loss: 144.9153, Accuracy: 0.9062\n",
      "Batch number: 086, Training: Loss: 303.4554, Accuracy: 0.8125\n",
      "Batch number: 087, Training: Loss: 259.8251, Accuracy: 0.7812\n",
      "Batch number: 088, Training: Loss: 494.6185, Accuracy: 0.8438\n",
      "Batch number: 089, Training: Loss: 56.1778, Accuracy: 0.9062\n",
      "Batch number: 090, Training: Loss: 29.1351, Accuracy: 0.9062\n",
      "Batch number: 091, Training: Loss: 378.2462, Accuracy: 0.7812\n",
      "Batch number: 092, Training: Loss: 217.0710, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 173.7104, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 58.9259, Accuracy: 0.9688\n",
      "Batch number: 095, Training: Loss: 168.0723, Accuracy: 0.9062\n",
      "Batch number: 096, Training: Loss: 131.9083, Accuracy: 0.9062\n",
      "Batch number: 097, Training: Loss: 68.8082, Accuracy: 0.8438\n",
      "Batch number: 098, Training: Loss: 454.2336, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 355.6259, Accuracy: 0.8125\n",
      "Batch number: 100, Training: Loss: 131.9507, Accuracy: 0.9062\n",
      "Batch number: 101, Training: Loss: 171.0084, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 491.8233, Accuracy: 0.7812\n",
      "Batch number: 103, Training: Loss: 50.2919, Accuracy: 0.9688\n",
      "Batch number: 104, Training: Loss: 526.2393, Accuracy: 0.7812\n",
      "Batch number: 105, Training: Loss: 172.5697, Accuracy: 0.7812\n",
      "Batch number: 106, Training: Loss: 89.9664, Accuracy: 0.9062\n",
      "Batch number: 107, Training: Loss: 173.3051, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 186.7697, Accuracy: 0.9062\n",
      "Batch number: 109, Training: Loss: 9.5101, Accuracy: 0.9688\n",
      "Batch number: 110, Training: Loss: 218.9845, Accuracy: 0.9062\n",
      "Batch number: 111, Training: Loss: 651.4736, Accuracy: 0.7500\n",
      "Batch number: 112, Training: Loss: 170.6601, Accuracy: 0.9062\n",
      "Batch number: 113, Training: Loss: 72.0582, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 64.7314, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 443.7815, Accuracy: 0.8438\n",
      "Batch number: 116, Training: Loss: 165.7484, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 453.8898, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 204.4246, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 814.1360, Accuracy: 0.6562\n",
      "Batch number: 120, Training: Loss: 266.9250, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 257.8582, Accuracy: 0.8125\n",
      "Batch number: 122, Training: Loss: 459.0577, Accuracy: 0.8125\n",
      "Batch number: 123, Training: Loss: 46.6440, Accuracy: 0.9688\n",
      "Batch number: 124, Training: Loss: 529.0792, Accuracy: 0.7812\n",
      "Batch number: 125, Training: Loss: 81.5246, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 287.3045, Accuracy: 0.8125\n",
      "Batch number: 127, Training: Loss: 196.6313, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 117.7577, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 230.8056, Accuracy: 0.8125\n",
      "Batch number: 130, Training: Loss: 263.5087, Accuracy: 0.8125\n",
      "Batch number: 131, Training: Loss: 448.5647, Accuracy: 0.7812\n",
      "Batch number: 132, Training: Loss: 145.1915, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 339.5161, Accuracy: 0.8438\n",
      "Batch number: 134, Training: Loss: 17.4104, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 168.3734, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 9.7684, Accuracy: 0.9688\n",
      "Batch number: 137, Training: Loss: 297.1135, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 100.4080, Accuracy: 0.8438\n",
      "Batch number: 139, Training: Loss: 55.7286, Accuracy: 0.9062\n",
      "Batch number: 140, Training: Loss: 159.9330, Accuracy: 0.8125\n",
      "Batch number: 141, Training: Loss: 224.6012, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 264.1666, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 164.9516, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 112.1685, Accuracy: 0.9688\n",
      "Batch number: 145, Training: Loss: 46.2447, Accuracy: 0.9062\n",
      "Batch number: 146, Training: Loss: 236.7818, Accuracy: 0.9062\n",
      "Batch number: 147, Training: Loss: 492.3909, Accuracy: 0.7500\n",
      "Batch number: 148, Training: Loss: 625.9772, Accuracy: 0.7812\n",
      "Batch number: 149, Training: Loss: 102.7936, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 218.1570, Accuracy: 0.8438\n",
      "Batch number: 151, Training: Loss: 344.1703, Accuracy: 0.9062\n",
      "Batch number: 152, Training: Loss: 273.2105, Accuracy: 0.8125\n",
      "Batch number: 153, Training: Loss: 189.3162, Accuracy: 0.9062\n",
      "Batch number: 154, Training: Loss: 358.9835, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 117.4509, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 85.5699, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 29.2019, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 524.0316, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 892.6174, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 508.0934, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 252.0104, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 83.1292, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 208.3454, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 44.0357, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 248.7422, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 300.9836, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 100.6829, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 578.5675, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 274.6708, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 34.9463, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 855.5310, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 615.8080, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 106.7224, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 264.0564, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 832.3541, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 1251.5941, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 220.3110, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 1484.9255, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 421.8375, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 522.4200, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 175.6473, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 443.3994, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 16.8338, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 1018.5284, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 568.1462, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 81.1023, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 316.3296, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 439.5230, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 106.1425, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 72.5874, Accuracy: 0.7500\n",
      "Epoch : 004, Training: Loss : 273.3593, Accuracy: 85.6886%\n",
      "Validation : Loss : 100.5714, Accuracy: 92.8058%, Time: 54.5511s\n",
      "Best accuracy achieved so far : 0.9317 on epoch 3\n",
      "Epoch: 6/25\n",
      "Batch number: 000, Training: Loss: 105.9216, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 316.6205, Accuracy: 0.8125\n",
      "Batch number: 002, Training: Loss: 17.9337, Accuracy: 0.9688\n",
      "Batch number: 003, Training: Loss: 216.1468, Accuracy: 0.8125\n",
      "Batch number: 004, Training: Loss: 89.4906, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 27.0079, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 132.9919, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 59.5387, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 75.9652, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 317.2082, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 76.6718, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 157.7568, Accuracy: 0.8125\n",
      "Batch number: 012, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 331.5363, Accuracy: 0.7812\n",
      "Batch number: 014, Training: Loss: 55.0308, Accuracy: 0.9062\n",
      "Batch number: 015, Training: Loss: 202.4701, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 486.0550, Accuracy: 0.7812\n",
      "Batch number: 017, Training: Loss: 244.0116, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 55.2383, Accuracy: 0.9688\n",
      "Batch number: 019, Training: Loss: 88.9327, Accuracy: 0.9062\n",
      "Batch number: 020, Training: Loss: 18.7287, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 334.4379, Accuracy: 0.7812\n",
      "Batch number: 022, Training: Loss: 113.7580, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 49.6601, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 266.0476, Accuracy: 0.8125\n",
      "Batch number: 025, Training: Loss: 55.7725, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 911.2679, Accuracy: 0.7188\n",
      "Batch number: 027, Training: Loss: 173.9109, Accuracy: 0.8750\n",
      "Batch number: 028, Training: Loss: 118.3445, Accuracy: 0.9062\n",
      "Batch number: 029, Training: Loss: 123.6020, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 274.9793, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 323.6309, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 309.4218, Accuracy: 0.7812\n",
      "Batch number: 033, Training: Loss: 222.5839, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 222.4700, Accuracy: 0.8438\n",
      "Batch number: 035, Training: Loss: 158.9767, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 307.9365, Accuracy: 0.8438\n",
      "Batch number: 037, Training: Loss: 242.5345, Accuracy: 0.7812\n",
      "Batch number: 038, Training: Loss: 492.4517, Accuracy: 0.8125\n",
      "Batch number: 039, Training: Loss: 235.8605, Accuracy: 0.8438\n",
      "Batch number: 040, Training: Loss: 203.1503, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 47.0021, Accuracy: 0.9062\n",
      "Batch number: 042, Training: Loss: 125.3828, Accuracy: 0.9062\n",
      "Batch number: 043, Training: Loss: 151.1821, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 194.8992, Accuracy: 0.9062\n",
      "Batch number: 045, Training: Loss: 203.0912, Accuracy: 0.8125\n",
      "Batch number: 046, Training: Loss: 273.3214, Accuracy: 0.7500\n",
      "Batch number: 047, Training: Loss: 370.1049, Accuracy: 0.8438\n",
      "Batch number: 048, Training: Loss: 339.6300, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 186.7255, Accuracy: 0.7500\n",
      "Batch number: 050, Training: Loss: 304.6003, Accuracy: 0.7812\n",
      "Batch number: 051, Training: Loss: 222.1767, Accuracy: 0.9062\n",
      "Batch number: 052, Training: Loss: 212.0037, Accuracy: 0.8438\n",
      "Batch number: 053, Training: Loss: 135.3652, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 131.3769, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 276.2645, Accuracy: 0.7500\n",
      "Batch number: 056, Training: Loss: 217.0816, Accuracy: 0.7500\n",
      "Batch number: 057, Training: Loss: 64.1685, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 214.5156, Accuracy: 0.7812\n",
      "Batch number: 059, Training: Loss: 200.4219, Accuracy: 0.7500\n",
      "Batch number: 060, Training: Loss: 186.8390, Accuracy: 0.9062\n",
      "Batch number: 061, Training: Loss: 723.9414, Accuracy: 0.7188\n",
      "Batch number: 062, Training: Loss: 197.5265, Accuracy: 0.7812\n",
      "Batch number: 063, Training: Loss: 24.3543, Accuracy: 0.9688\n",
      "Batch number: 064, Training: Loss: 227.6321, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 308.7387, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 171.4505, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 425.4756, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 283.1439, Accuracy: 0.9062\n",
      "Batch number: 069, Training: Loss: 108.7282, Accuracy: 0.8438\n",
      "Batch number: 070, Training: Loss: 170.6160, Accuracy: 0.9062\n",
      "Batch number: 071, Training: Loss: 369.0324, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 374.5645, Accuracy: 0.8125\n",
      "Batch number: 073, Training: Loss: 127.5703, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 31.6648, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 907.3541, Accuracy: 0.7188\n",
      "Batch number: 076, Training: Loss: 246.2563, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 63.4818, Accuracy: 0.9062\n",
      "Batch number: 078, Training: Loss: 355.3052, Accuracy: 0.8438\n",
      "Batch number: 079, Training: Loss: 91.6503, Accuracy: 0.9062\n",
      "Batch number: 080, Training: Loss: 78.8131, Accuracy: 0.9062\n",
      "Batch number: 081, Training: Loss: 387.9876, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 477.4762, Accuracy: 0.7188\n",
      "Batch number: 083, Training: Loss: 52.8883, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 317.7990, Accuracy: 0.8438\n",
      "Batch number: 085, Training: Loss: 18.9002, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 367.2606, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 32.1152, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 382.8784, Accuracy: 0.8125\n",
      "Batch number: 089, Training: Loss: 73.4553, Accuracy: 0.9062\n",
      "Batch number: 090, Training: Loss: 75.4476, Accuracy: 0.8438\n",
      "Batch number: 091, Training: Loss: 358.4703, Accuracy: 0.9062\n",
      "Batch number: 092, Training: Loss: 78.1950, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 119.6920, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 45.8549, Accuracy: 0.9062\n",
      "Batch number: 095, Training: Loss: 252.6338, Accuracy: 0.8125\n",
      "Batch number: 096, Training: Loss: 157.0465, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 44.5964, Accuracy: 0.9062\n",
      "Batch number: 098, Training: Loss: 56.4699, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 91.7188, Accuracy: 0.9062\n",
      "Batch number: 100, Training: Loss: 129.8572, Accuracy: 0.8438\n",
      "Batch number: 101, Training: Loss: 4.1977, Accuracy: 0.9688\n",
      "Batch number: 102, Training: Loss: 222.6049, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 72.8172, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 199.4373, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 335.0755, Accuracy: 0.7812\n",
      "Batch number: 106, Training: Loss: 40.9854, Accuracy: 0.9375\n",
      "Batch number: 107, Training: Loss: 182.0753, Accuracy: 0.9062\n",
      "Batch number: 108, Training: Loss: 551.1915, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 335.9547, Accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 111, Training: Loss: 232.1964, Accuracy: 0.8438\n",
      "Batch number: 112, Training: Loss: 270.1215, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 112.2649, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 97.1852, Accuracy: 0.9062\n",
      "Batch number: 115, Training: Loss: 179.6609, Accuracy: 0.9062\n",
      "Batch number: 116, Training: Loss: 128.3109, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 189.9405, Accuracy: 0.8438\n",
      "Batch number: 118, Training: Loss: 298.5681, Accuracy: 0.8125\n",
      "Batch number: 119, Training: Loss: 194.6493, Accuracy: 0.8438\n",
      "Batch number: 120, Training: Loss: 64.5231, Accuracy: 0.9062\n",
      "Batch number: 121, Training: Loss: 147.3401, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 70.5169, Accuracy: 0.9062\n",
      "Batch number: 123, Training: Loss: 218.3793, Accuracy: 0.8438\n",
      "Batch number: 124, Training: Loss: 536.1905, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 27.9348, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 238.5088, Accuracy: 0.8438\n",
      "Batch number: 127, Training: Loss: 200.2637, Accuracy: 0.8125\n",
      "Batch number: 128, Training: Loss: 151.5127, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 243.9939, Accuracy: 0.8125\n",
      "Batch number: 130, Training: Loss: 24.0766, Accuracy: 0.9062\n",
      "Batch number: 131, Training: Loss: 141.1387, Accuracy: 0.8438\n",
      "Batch number: 132, Training: Loss: 166.3455, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 300.3411, Accuracy: 0.7812\n",
      "Batch number: 134, Training: Loss: 13.3556, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 82.4673, Accuracy: 0.8438\n",
      "Batch number: 136, Training: Loss: 274.3939, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 151.8023, Accuracy: 0.8438\n",
      "Batch number: 138, Training: Loss: 213.8372, Accuracy: 0.9062\n",
      "Batch number: 139, Training: Loss: 50.4802, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 307.7389, Accuracy: 0.7812\n",
      "Batch number: 141, Training: Loss: 38.9098, Accuracy: 0.9062\n",
      "Batch number: 142, Training: Loss: 57.3282, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 202.1484, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 168.5354, Accuracy: 0.8125\n",
      "Batch number: 145, Training: Loss: 256.4421, Accuracy: 0.8438\n",
      "Batch number: 146, Training: Loss: 148.8359, Accuracy: 0.8438\n",
      "Batch number: 147, Training: Loss: 348.9239, Accuracy: 0.7500\n",
      "Batch number: 148, Training: Loss: 111.2367, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 31.4101, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 421.3820, Accuracy: 0.8125\n",
      "Batch number: 152, Training: Loss: 109.3957, Accuracy: 0.9062\n",
      "Batch number: 153, Training: Loss: 5.6342, Accuracy: 0.9688\n",
      "Batch number: 154, Training: Loss: 309.1493, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 198.4168, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 566.1105, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 102.2762, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 228.9930, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 45.3127, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 506.2328, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 467.4160, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 149.8575, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 763.3746, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 402.3453, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 106.6948, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 383.2032, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 366.5430, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 158.6219, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 402.5624, Accuracy: 0.5000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 117.0445, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 678.9725, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 266.9980, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 23.6214, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 251.2710, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 96.8876, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 643.6670, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 396.5764, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 569.8135, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 458.1739, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 341.4735, Accuracy: 0.5000\n",
      "Validation Batch number: 085, Validation: Loss: 1420.7866, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 442.5856, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 1120.0403, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 626.6142, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 87.9623, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 227.6854, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 790.6731, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 371.0139, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 424.4414, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 172.4569, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 43.5372, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 3.5902, Accuracy: 0.7500\n",
      "Epoch : 005, Training: Loss : 197.0240, Accuracy: 86.7894%\n",
      "Validation : Loss : 102.3412, Accuracy: 92.0863%, Time: 54.0401s\n",
      "Best accuracy achieved so far : 0.9317 on epoch 3\n",
      "Epoch: 7/25\n",
      "Batch number: 000, Training: Loss: 149.7216, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 368.3707, Accuracy: 0.7812\n",
      "Batch number: 002, Training: Loss: 40.9132, Accuracy: 0.9062\n",
      "Batch number: 003, Training: Loss: 213.5117, Accuracy: 0.8438\n",
      "Batch number: 004, Training: Loss: 195.2401, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 124.1838, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 92.0037, Accuracy: 0.8125\n",
      "Batch number: 007, Training: Loss: 92.9144, Accuracy: 0.9062\n",
      "Batch number: 008, Training: Loss: 127.2052, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 106.4443, Accuracy: 0.9062\n",
      "Batch number: 010, Training: Loss: 78.0546, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 144.2063, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 363.1160, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 125.8760, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 32.3261, Accuracy: 0.9062\n",
      "Batch number: 015, Training: Loss: 10.0418, Accuracy: 0.9688\n",
      "Batch number: 016, Training: Loss: 21.9209, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 68.4758, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 116.3257, Accuracy: 0.8438\n",
      "Batch number: 019, Training: Loss: 120.0198, Accuracy: 0.7812\n",
      "Batch number: 020, Training: Loss: 88.1436, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 786.1371, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 52.3207, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 45.1535, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 105.1585, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 344.8481, Accuracy: 0.7812\n",
      "Batch number: 026, Training: Loss: 193.4065, Accuracy: 0.8438\n",
      "Batch number: 027, Training: Loss: 86.1136, Accuracy: 0.9062\n",
      "Batch number: 028, Training: Loss: 79.0468, Accuracy: 0.8438\n",
      "Batch number: 029, Training: Loss: 7.4689, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 274.3813, Accuracy: 0.9062\n",
      "Batch number: 031, Training: Loss: 93.8157, Accuracy: 0.9062\n",
      "Batch number: 032, Training: Loss: 253.0371, Accuracy: 0.8438\n",
      "Batch number: 033, Training: Loss: 134.2906, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 67.0017, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 244.6377, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 207.8830, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 66.6112, Accuracy: 0.9062\n",
      "Batch number: 038, Training: Loss: 205.1001, Accuracy: 0.7812\n",
      "Batch number: 039, Training: Loss: 67.7111, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 166.0010, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 63.8948, Accuracy: 0.9688\n",
      "Batch number: 042, Training: Loss: 87.7171, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 91.6573, Accuracy: 0.8438\n",
      "Batch number: 044, Training: Loss: 457.8865, Accuracy: 0.7188\n",
      "Batch number: 045, Training: Loss: 108.1721, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 131.3111, Accuracy: 0.8438\n",
      "Batch number: 047, Training: Loss: 268.7741, Accuracy: 0.7812\n",
      "Batch number: 048, Training: Loss: 89.3792, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 199.3315, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 318.6696, Accuracy: 0.8125\n",
      "Batch number: 051, Training: Loss: 113.3874, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 32.1738, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 169.7263, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 37.9564, Accuracy: 0.9062\n",
      "Batch number: 055, Training: Loss: 171.4814, Accuracy: 0.8438\n",
      "Batch number: 056, Training: Loss: 54.0424, Accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 057, Training: Loss: 68.6879, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 256.0984, Accuracy: 0.7812\n",
      "Batch number: 059, Training: Loss: 217.3177, Accuracy: 0.7812\n",
      "Batch number: 060, Training: Loss: 55.2347, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 124.4228, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 445.1705, Accuracy: 0.7188\n",
      "Batch number: 063, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 68.8608, Accuracy: 0.9062\n",
      "Batch number: 065, Training: Loss: 155.2984, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 88.2580, Accuracy: 0.9688\n",
      "Batch number: 067, Training: Loss: 90.4735, Accuracy: 0.9062\n",
      "Batch number: 068, Training: Loss: 36.0847, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 107.6009, Accuracy: 0.9062\n",
      "Batch number: 070, Training: Loss: 9.0939, Accuracy: 0.9688\n",
      "Batch number: 071, Training: Loss: 149.6470, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 40.1164, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 141.4158, Accuracy: 0.8438\n",
      "Batch number: 074, Training: Loss: 19.3825, Accuracy: 0.9062\n",
      "Batch number: 075, Training: Loss: 315.8037, Accuracy: 0.7812\n",
      "Batch number: 076, Training: Loss: 256.6285, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 80.7933, Accuracy: 0.9062\n",
      "Batch number: 078, Training: Loss: 134.9994, Accuracy: 0.8125\n",
      "Batch number: 079, Training: Loss: 36.2470, Accuracy: 0.9688\n",
      "Batch number: 080, Training: Loss: 11.3745, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 447.5036, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 199.9081, Accuracy: 0.9062\n",
      "Batch number: 083, Training: Loss: 50.3508, Accuracy: 0.9062\n",
      "Batch number: 084, Training: Loss: 456.3880, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 219.7689, Accuracy: 0.8438\n",
      "Batch number: 086, Training: Loss: 160.2544, Accuracy: 0.7812\n",
      "Batch number: 087, Training: Loss: 151.1941, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 356.6062, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 493.8067, Accuracy: 0.6875\n",
      "Batch number: 090, Training: Loss: 34.6808, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 226.3448, Accuracy: 0.9062\n",
      "Batch number: 092, Training: Loss: 38.2653, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 105.5303, Accuracy: 0.8438\n",
      "Batch number: 094, Training: Loss: 223.5970, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 91.7233, Accuracy: 0.9062\n",
      "Batch number: 096, Training: Loss: 164.5316, Accuracy: 0.9062\n",
      "Batch number: 097, Training: Loss: 107.5565, Accuracy: 0.9062\n",
      "Batch number: 098, Training: Loss: 116.2277, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 118.2358, Accuracy: 0.9062\n",
      "Batch number: 100, Training: Loss: 201.0909, Accuracy: 0.8438\n",
      "Batch number: 101, Training: Loss: 8.0321, Accuracy: 0.9688\n",
      "Batch number: 102, Training: Loss: 40.2790, Accuracy: 0.9062\n",
      "Batch number: 103, Training: Loss: 95.1520, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 115.9530, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 0.7763, Accuracy: 0.9688\n",
      "Batch number: 106, Training: Loss: 118.1849, Accuracy: 0.8438\n",
      "Batch number: 107, Training: Loss: 25.8866, Accuracy: 0.9062\n",
      "Batch number: 108, Training: Loss: 268.7430, Accuracy: 0.8125\n",
      "Batch number: 109, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 309.8806, Accuracy: 0.8438\n",
      "Batch number: 111, Training: Loss: 159.4943, Accuracy: 0.8438\n",
      "Batch number: 112, Training: Loss: 150.6618, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 157.5586, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 89.2262, Accuracy: 0.9688\n",
      "Batch number: 115, Training: Loss: 183.7252, Accuracy: 0.8438\n",
      "Batch number: 116, Training: Loss: 113.3326, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 55.7273, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 57.7948, Accuracy: 0.9062\n",
      "Batch number: 119, Training: Loss: 162.0503, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 200.6793, Accuracy: 0.8438\n",
      "Batch number: 121, Training: Loss: 222.3304, Accuracy: 0.7812\n",
      "Batch number: 122, Training: Loss: 33.7370, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 33.2586, Accuracy: 0.9062\n",
      "Batch number: 124, Training: Loss: 77.5136, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 53.1645, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 141.7539, Accuracy: 0.9062\n",
      "Batch number: 127, Training: Loss: 53.7575, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 17.7559, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 47.2897, Accuracy: 0.9062\n",
      "Batch number: 130, Training: Loss: 107.9749, Accuracy: 0.9062\n",
      "Batch number: 131, Training: Loss: 174.8709, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 40.2399, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 107.6170, Accuracy: 0.9062\n",
      "Batch number: 134, Training: Loss: 55.3331, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 255.6403, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 39.9196, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 59.2843, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 51.0845, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 23.2260, Accuracy: 0.8750\n",
      "Batch number: 140, Training: Loss: 17.3926, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 119.7140, Accuracy: 0.9062\n",
      "Batch number: 142, Training: Loss: 32.7593, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 147.9653, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 160.6875, Accuracy: 0.9062\n",
      "Batch number: 145, Training: Loss: 93.4959, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 13.6435, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 178.9835, Accuracy: 0.8438\n",
      "Batch number: 148, Training: Loss: 481.7891, Accuracy: 0.7812\n",
      "Batch number: 149, Training: Loss: 9.8950, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 45.2725, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 237.2118, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 127.1450, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 197.1754, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 346.5273, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 227.5737, Accuracy: 0.8125\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 584.5101, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 53.6262, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 146.1590, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 480.0871, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 421.4243, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 188.9172, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 650.2672, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 355.8014, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 61.4054, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 352.8348, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 97.1417, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 217.2294, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 0.0027, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 304.0353, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 131.6939, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 168.9775, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 769.1123, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 256.5380, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 37.2047, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 407.5197, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 69.9934, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 326.6908, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 526.5192, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 523.4618, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 241.5994, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 43.5227, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 491.7441, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 1101.0215, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 250.4985, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 1036.7886, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 268.3259, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 40.7832, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 512.5908, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 4.3072, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 1045.6876, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 541.6772, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 63.7641, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 623.9524, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 233.5226, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 46.3747, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 178.4044, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 30.8081, Accuracy: 0.7500\n",
      "Epoch : 006, Training: Loss : 139.4613, Accuracy: 88.1705%\n",
      "Validation : Loss : 99.9031, Accuracy: 92.0863%, Time: 52.5637s\n",
      "Best accuracy achieved so far : 0.9317 on epoch 3\n",
      "Epoch: 8/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 000, Training: Loss: 139.9381, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 491.5753, Accuracy: 0.7500\n",
      "Batch number: 002, Training: Loss: 183.5211, Accuracy: 0.8438\n",
      "Batch number: 003, Training: Loss: 692.0259, Accuracy: 0.7188\n",
      "Batch number: 004, Training: Loss: 49.5766, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 46.5560, Accuracy: 0.9062\n",
      "Batch number: 006, Training: Loss: 169.6901, Accuracy: 0.9062\n",
      "Batch number: 007, Training: Loss: 69.5239, Accuracy: 0.9062\n",
      "Batch number: 008, Training: Loss: 113.2192, Accuracy: 0.8438\n",
      "Batch number: 009, Training: Loss: 56.1042, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 33.6021, Accuracy: 0.9688\n",
      "Batch number: 011, Training: Loss: 153.3525, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 126.0576, Accuracy: 0.9062\n",
      "Batch number: 013, Training: Loss: 194.6405, Accuracy: 0.7188\n",
      "Batch number: 014, Training: Loss: 47.0741, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 201.6703, Accuracy: 0.8125\n",
      "Batch number: 016, Training: Loss: 225.7201, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 124.0479, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 402.7746, Accuracy: 0.7812\n",
      "Batch number: 019, Training: Loss: 236.2083, Accuracy: 0.6562\n",
      "Batch number: 020, Training: Loss: 172.7247, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 180.9215, Accuracy: 0.7812\n",
      "Batch number: 022, Training: Loss: 109.9759, Accuracy: 0.8125\n",
      "Batch number: 023, Training: Loss: 127.1692, Accuracy: 0.7812\n",
      "Batch number: 024, Training: Loss: 232.6362, Accuracy: 0.7812\n",
      "Batch number: 025, Training: Loss: 123.3540, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 188.6811, Accuracy: 0.8125\n",
      "Batch number: 027, Training: Loss: 140.5648, Accuracy: 0.9062\n",
      "Batch number: 028, Training: Loss: 66.9926, Accuracy: 0.9062\n",
      "Batch number: 029, Training: Loss: 109.6085, Accuracy: 0.9062\n",
      "Batch number: 030, Training: Loss: 211.1225, Accuracy: 0.9062\n",
      "Batch number: 031, Training: Loss: 78.5458, Accuracy: 0.9062\n",
      "Batch number: 032, Training: Loss: 146.9854, Accuracy: 0.8438\n",
      "Batch number: 033, Training: Loss: 313.5577, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 474.1469, Accuracy: 0.7812\n",
      "Batch number: 035, Training: Loss: 172.9766, Accuracy: 0.9062\n",
      "Batch number: 036, Training: Loss: 102.0318, Accuracy: 0.8438\n",
      "Batch number: 037, Training: Loss: 105.9441, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 156.0190, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 40.8132, Accuracy: 0.9062\n",
      "Batch number: 040, Training: Loss: 378.1708, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 352.3708, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 243.1162, Accuracy: 0.8438\n",
      "Batch number: 043, Training: Loss: 65.2565, Accuracy: 0.9688\n",
      "Batch number: 044, Training: Loss: 96.4289, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 43.2329, Accuracy: 0.9062\n",
      "Batch number: 046, Training: Loss: 281.9468, Accuracy: 0.7812\n",
      "Batch number: 047, Training: Loss: 249.2906, Accuracy: 0.8438\n",
      "Batch number: 048, Training: Loss: 87.1366, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 92.0249, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 113.7321, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 201.1028, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 52.7275, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 179.2500, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 174.3663, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 56.6807, Accuracy: 0.9062\n",
      "Batch number: 056, Training: Loss: 129.5733, Accuracy: 0.8438\n",
      "Batch number: 057, Training: Loss: 55.0888, Accuracy: 0.8750\n",
      "Batch number: 058, Training: Loss: 106.6547, Accuracy: 0.8438\n",
      "Batch number: 059, Training: Loss: 108.0823, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 99.8943, Accuracy: 0.8438\n",
      "Batch number: 061, Training: Loss: 530.4255, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 116.6554, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 129.1232, Accuracy: 0.8438\n",
      "Batch number: 064, Training: Loss: 24.7490, Accuracy: 0.9062\n",
      "Batch number: 065, Training: Loss: 75.0111, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 48.7279, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 157.4023, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 38.8326, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 92.0087, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 116.2106, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 199.9051, Accuracy: 0.8438\n",
      "Batch number: 072, Training: Loss: 113.8206, Accuracy: 0.9688\n",
      "Batch number: 073, Training: Loss: 69.3599, Accuracy: 0.8438\n",
      "Batch number: 074, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 248.0056, Accuracy: 0.8438\n",
      "Batch number: 076, Training: Loss: 294.8704, Accuracy: 0.7500\n",
      "Batch number: 077, Training: Loss: 107.5785, Accuracy: 0.7812\n",
      "Batch number: 078, Training: Loss: 69.2975, Accuracy: 0.9062\n",
      "Batch number: 079, Training: Loss: 58.6082, Accuracy: 0.8750\n",
      "Batch number: 080, Training: Loss: 69.2939, Accuracy: 0.8438\n",
      "Batch number: 081, Training: Loss: 144.5179, Accuracy: 0.8438\n",
      "Batch number: 082, Training: Loss: 128.9171, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 39.5168, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 51.2834, Accuracy: 0.9062\n",
      "Batch number: 085, Training: Loss: 131.5873, Accuracy: 0.7812\n",
      "Batch number: 086, Training: Loss: 18.1130, Accuracy: 0.9688\n",
      "Batch number: 087, Training: Loss: 98.3817, Accuracy: 0.9688\n",
      "Batch number: 088, Training: Loss: 147.0557, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 7.1078, Accuracy: 0.9688\n",
      "Batch number: 090, Training: Loss: 69.1167, Accuracy: 0.9062\n",
      "Batch number: 091, Training: Loss: 59.4100, Accuracy: 0.9062\n",
      "Batch number: 092, Training: Loss: 152.2075, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 126.6047, Accuracy: 0.8125\n",
      "Batch number: 094, Training: Loss: 61.1920, Accuracy: 0.8438\n",
      "Batch number: 095, Training: Loss: 47.6125, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 0.8301, Accuracy: 0.9688\n",
      "Batch number: 097, Training: Loss: 12.3459, Accuracy: 0.9688\n",
      "Batch number: 098, Training: Loss: 82.5480, Accuracy: 0.9062\n",
      "Batch number: 099, Training: Loss: 3475.8311, Accuracy: 0.8438\n",
      "Batch number: 100, Training: Loss: 0.4665, Accuracy: 0.9688\n",
      "Batch number: 101, Training: Loss: 82.9883, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 180.3262, Accuracy: 0.7500\n",
      "Batch number: 103, Training: Loss: 78.7900, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 797.0491, Accuracy: 0.7188\n",
      "Batch number: 105, Training: Loss: 52.6778, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 35.3426, Accuracy: 0.9062\n",
      "Batch number: 107, Training: Loss: 26.9136, Accuracy: 0.9688\n",
      "Batch number: 108, Training: Loss: 17.8368, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 52.8479, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 113.5725, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 252.1064, Accuracy: 0.7500\n",
      "Batch number: 112, Training: Loss: 166.5620, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 34.8053, Accuracy: 0.9062\n",
      "Batch number: 114, Training: Loss: 122.6987, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 136.3335, Accuracy: 0.8438\n",
      "Batch number: 116, Training: Loss: 68.0491, Accuracy: 0.8438\n",
      "Batch number: 117, Training: Loss: 69.4263, Accuracy: 0.8438\n",
      "Batch number: 118, Training: Loss: 49.1304, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 164.4926, Accuracy: 0.8438\n",
      "Batch number: 120, Training: Loss: 125.4338, Accuracy: 0.7812\n",
      "Batch number: 121, Training: Loss: 70.7446, Accuracy: 0.9062\n",
      "Batch number: 122, Training: Loss: 235.8322, Accuracy: 0.7500\n",
      "Batch number: 123, Training: Loss: 233.8178, Accuracy: 0.7812\n",
      "Batch number: 124, Training: Loss: 109.6596, Accuracy: 0.9062\n",
      "Batch number: 125, Training: Loss: 5.9856, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 111.9691, Accuracy: 0.8438\n",
      "Batch number: 127, Training: Loss: 161.1234, Accuracy: 0.8125\n",
      "Batch number: 128, Training: Loss: 36.4528, Accuracy: 0.9062\n",
      "Batch number: 129, Training: Loss: 149.9232, Accuracy: 0.8750\n",
      "Batch number: 130, Training: Loss: 38.5719, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 237.5152, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 85.4055, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 480.6888, Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 134, Training: Loss: 19.4671, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 142.4965, Accuracy: 0.9062\n",
      "Batch number: 136, Training: Loss: 74.5867, Accuracy: 0.9062\n",
      "Batch number: 137, Training: Loss: 220.8537, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 250.9527, Accuracy: 0.8438\n",
      "Batch number: 139, Training: Loss: 9.2764, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 177.8227, Accuracy: 0.7812\n",
      "Batch number: 141, Training: Loss: 131.4057, Accuracy: 0.9062\n",
      "Batch number: 142, Training: Loss: 97.4083, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 224.7027, Accuracy: 0.7812\n",
      "Batch number: 144, Training: Loss: 157.7136, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 111.6889, Accuracy: 0.9062\n",
      "Batch number: 146, Training: Loss: 150.4928, Accuracy: 0.8438\n",
      "Batch number: 147, Training: Loss: 181.7574, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 532.8618, Accuracy: 0.7812\n",
      "Batch number: 149, Training: Loss: 30.3477, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 135.1340, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 112.5292, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 101.0509, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 32.0201, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 235.9280, Accuracy: 0.8438\n",
      "Batch number: 155, Training: Loss: 95.3034, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 232.1238, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 192.7544, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 62.4898, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 25.6974, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 71.8942, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 395.1949, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 509.4473, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 374.0703, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 58.7505, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 70.4566, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 220.7690, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 214.9265, Accuracy: 0.7500\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 65.3821, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0168, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 423.1274, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 323.9590, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 12.8551, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 398.6865, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 301.3179, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 327.8737, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 57.4090, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 240.1165, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 676.9397, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 085, Validation: Loss: 347.2293, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 132.7426, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 094, Validation: Loss: 137.6333, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 438.6045, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 134.8655, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 222.5894, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 70.2301, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 120.3022, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 810.5686, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 181.3167, Accuracy: 0.7500\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 206.2562, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 2.6972, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 413.4776, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 137.9416, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch : 007, Training: Loss : 162.7105, Accuracy: 87.0697%\n",
      "Validation : Loss : 61.9620, Accuracy: 92.4460%, Time: 52.2002s\n",
      "Best accuracy achieved so far : 0.9317 on epoch 3\n",
      "Epoch: 9/25\n",
      "Batch number: 000, Training: Loss: 259.8675, Accuracy: 0.7812\n",
      "Batch number: 001, Training: Loss: 105.9469, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 123.2758, Accuracy: 0.9375\n",
      "Batch number: 003, Training: Loss: 50.1891, Accuracy: 0.9062\n",
      "Batch number: 004, Training: Loss: 141.8222, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 46.8780, Accuracy: 0.9688\n",
      "Batch number: 006, Training: Loss: 83.1063, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 38.1119, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 40.8279, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 46.0960, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 144.2654, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 78.1809, Accuracy: 0.8438\n",
      "Batch number: 012, Training: Loss: 70.7855, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 90.5655, Accuracy: 0.9062\n",
      "Batch number: 014, Training: Loss: 0.8489, Accuracy: 0.9688\n",
      "Batch number: 015, Training: Loss: 26.4570, Accuracy: 0.9688\n",
      "Batch number: 016, Training: Loss: 31.2707, Accuracy: 0.9062\n",
      "Batch number: 017, Training: Loss: 51.9773, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 142.7493, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 68.9963, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 36.7648, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 177.7678, Accuracy: 0.9062\n",
      "Batch number: 022, Training: Loss: 99.5078, Accuracy: 0.8438\n",
      "Batch number: 023, Training: Loss: 62.3066, Accuracy: 0.8125\n",
      "Batch number: 024, Training: Loss: 29.5136, Accuracy: 0.9062\n",
      "Batch number: 025, Training: Loss: 130.0670, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 165.9138, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 134.8017, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 38.6489, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 4.5224, Accuracy: 0.9688\n",
      "Batch number: 030, Training: Loss: 44.4041, Accuracy: 0.9062\n",
      "Batch number: 031, Training: Loss: 48.3480, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 71.6319, Accuracy: 0.7812\n",
      "Batch number: 033, Training: Loss: 90.6203, Accuracy: 0.7812\n",
      "Batch number: 034, Training: Loss: 170.4999, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 32.3623, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 169.6313, Accuracy: 0.8438\n",
      "Batch number: 037, Training: Loss: 36.0168, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 49.3388, Accuracy: 0.9062\n",
      "Batch number: 039, Training: Loss: 103.5777, Accuracy: 0.9062\n",
      "Batch number: 040, Training: Loss: 86.0269, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 97.8256, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 17.1067, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 207.8298, Accuracy: 0.8438\n",
      "Batch number: 044, Training: Loss: 109.0481, Accuracy: 0.9062\n",
      "Batch number: 045, Training: Loss: 99.2534, Accuracy: 0.9062\n",
      "Batch number: 046, Training: Loss: 78.4792, Accuracy: 0.9062\n",
      "Batch number: 047, Training: Loss: 53.9762, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 160.9676, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 050, Training: Loss: 386.7439, Accuracy: 0.7812\n",
      "Batch number: 051, Training: Loss: 66.3778, Accuracy: 0.9062\n",
      "Batch number: 052, Training: Loss: 207.3567, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 230.0873, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 71.5501, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 105.4044, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 78.6010, Accuracy: 0.8438\n",
      "Batch number: 057, Training: Loss: 5.6460, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 85.8863, Accuracy: 0.8438\n",
      "Batch number: 059, Training: Loss: 41.9844, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 72.7231, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 23.8178, Accuracy: 0.9688\n",
      "Batch number: 062, Training: Loss: 204.5011, Accuracy: 0.7812\n",
      "Batch number: 063, Training: Loss: 5.1337, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 46.8533, Accuracy: 0.9062\n",
      "Batch number: 065, Training: Loss: 76.4810, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 132.2524, Accuracy: 0.8125\n",
      "Batch number: 067, Training: Loss: 193.5423, Accuracy: 0.8125\n",
      "Batch number: 068, Training: Loss: 36.3116, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 26.0221, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 193.5997, Accuracy: 0.8125\n",
      "Batch number: 071, Training: Loss: 156.5310, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 73.8325, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 28.5396, Accuracy: 0.9062\n",
      "Batch number: 074, Training: Loss: 19.7647, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 146.9378, Accuracy: 0.9062\n",
      "Batch number: 076, Training: Loss: 247.4496, Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 077, Training: Loss: 85.9730, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 65.4621, Accuracy: 0.9062\n",
      "Batch number: 079, Training: Loss: 91.9173, Accuracy: 0.9062\n",
      "Batch number: 080, Training: Loss: 4.9745, Accuracy: 0.9688\n",
      "Batch number: 081, Training: Loss: 88.6693, Accuracy: 0.9062\n",
      "Batch number: 082, Training: Loss: 86.7480, Accuracy: 0.8438\n",
      "Batch number: 083, Training: Loss: 40.4700, Accuracy: 0.9688\n",
      "Batch number: 084, Training: Loss: 1.3382, Accuracy: 0.9688\n",
      "Batch number: 085, Training: Loss: 80.3834, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 102.7929, Accuracy: 0.8750\n",
      "Batch number: 087, Training: Loss: 78.2211, Accuracy: 0.8438\n",
      "Batch number: 088, Training: Loss: 154.0429, Accuracy: 0.9062\n",
      "Batch number: 089, Training: Loss: 98.1423, Accuracy: 0.9062\n",
      "Batch number: 090, Training: Loss: 46.8716, Accuracy: 0.9062\n",
      "Batch number: 091, Training: Loss: 119.5512, Accuracy: 0.8438\n",
      "Batch number: 092, Training: Loss: 50.1724, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 34.0884, Accuracy: 0.9688\n",
      "Batch number: 094, Training: Loss: 90.5753, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 72.7792, Accuracy: 0.9688\n",
      "Batch number: 096, Training: Loss: 54.5783, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 12.5936, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 54.5027, Accuracy: 0.9688\n",
      "Batch number: 099, Training: Loss: 19.3975, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 53.6967, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 57.5755, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 111.5435, Accuracy: 0.8125\n",
      "Batch number: 103, Training: Loss: 87.1835, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 121.2705, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 69.4569, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 52.6439, Accuracy: 0.9062\n",
      "Batch number: 107, Training: Loss: 99.8107, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 79.3458, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 3.2033, Accuracy: 0.9688\n",
      "Batch number: 110, Training: Loss: 21.0977, Accuracy: 0.9688\n",
      "Batch number: 111, Training: Loss: 234.8291, Accuracy: 0.8438\n",
      "Batch number: 112, Training: Loss: 95.6548, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 56.7387, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 69.3836, Accuracy: 0.9062\n",
      "Batch number: 115, Training: Loss: 117.1674, Accuracy: 0.9062\n",
      "Batch number: 116, Training: Loss: 51.8454, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 7.2218, Accuracy: 0.9688\n",
      "Batch number: 118, Training: Loss: 80.3962, Accuracy: 0.9062\n",
      "Batch number: 119, Training: Loss: 33.9845, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 122.2774, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 59.2141, Accuracy: 0.9062\n",
      "Batch number: 122, Training: Loss: 58.0653, Accuracy: 0.8438\n",
      "Batch number: 123, Training: Loss: 8.8402, Accuracy: 0.9688\n",
      "Batch number: 124, Training: Loss: 635.3534, Accuracy: 0.7812\n",
      "Batch number: 125, Training: Loss: 40.4761, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 39.6498, Accuracy: 0.9062\n",
      "Batch number: 127, Training: Loss: 119.0285, Accuracy: 0.8438\n",
      "Batch number: 128, Training: Loss: 18.1536, Accuracy: 0.9062\n",
      "Batch number: 129, Training: Loss: 201.9742, Accuracy: 0.7812\n",
      "Batch number: 130, Training: Loss: 102.1218, Accuracy: 0.8438\n",
      "Batch number: 131, Training: Loss: 159.8609, Accuracy: 0.9062\n",
      "Batch number: 132, Training: Loss: 181.9745, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 137.6251, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 10.5138, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 135.1185, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 118.6639, Accuracy: 0.9062\n",
      "Batch number: 137, Training: Loss: 140.4791, Accuracy: 0.8438\n",
      "Batch number: 138, Training: Loss: 119.0760, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 59.1794, Accuracy: 0.9062\n",
      "Batch number: 140, Training: Loss: 149.5966, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 49.5717, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 49.5562, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 72.9937, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 55.9982, Accuracy: 0.9062\n",
      "Batch number: 145, Training: Loss: 92.1795, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 202.9523, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 76.7123, Accuracy: 0.7188\n",
      "Batch number: 148, Training: Loss: 107.2943, Accuracy: 0.7812\n",
      "Batch number: 149, Training: Loss: 120.8665, Accuracy: 0.8125\n",
      "Batch number: 150, Training: Loss: 45.9149, Accuracy: 0.9688\n",
      "Batch number: 151, Training: Loss: 65.1145, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 58.8654, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 37.8017, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 232.7869, Accuracy: 0.8125\n",
      "Batch number: 155, Training: Loss: 58.6801, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 310.6691, Accuracy: 0.7500\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 108.7385, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 314.3880, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 45.8106, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 148.2924, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0025, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 192.8760, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 69.8413, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 11.1417, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 166.9376, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 68.3753, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 61.4660, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 296.2254, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 105.7057, Accuracy: 0.7500\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 122.8121, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 57.5910, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 379.6599, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 385.4949, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 50.0324, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 100.6562, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 233.2902, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 58.0797, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 234.6884, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 917.9581, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 70.4684, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 610.4689, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 205.0776, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 74.9173, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 181.9634, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 423.5606, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 46.7520, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 28.1638, Accuracy: 0.7500\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 474.1883, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 249.3982, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 13.0957, Accuracy: 0.7500\n",
      "Epoch : 008, Training: Loss : 90.9254, Accuracy: 89.0512%\n",
      "Validation : Loss : 49.0560, Accuracy: 93.3453%, Time: 52.5636s\n",
      "model for epoch 8 saved\n",
      "Best accuracy achieved so far : 0.9335 on epoch 8\n",
      "Epoch: 10/25\n",
      "Batch number: 000, Training: Loss: 80.4922, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 130.4770, Accuracy: 0.9062\n",
      "Batch number: 002, Training: Loss: 109.9362, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 259.5132, Accuracy: 0.8438\n",
      "Batch number: 004, Training: Loss: 186.4750, Accuracy: 0.7500\n",
      "Batch number: 005, Training: Loss: 77.4263, Accuracy: 0.8438\n",
      "Batch number: 006, Training: Loss: 45.5203, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 187.6496, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 113.3361, Accuracy: 0.8438\n",
      "Batch number: 009, Training: Loss: 104.8810, Accuracy: 0.9062\n",
      "Batch number: 010, Training: Loss: 40.4269, Accuracy: 0.8438\n",
      "Batch number: 011, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 012, Training: Loss: 115.9767, Accuracy: 0.9062\n",
      "Batch number: 013, Training: Loss: 51.2252, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 22.2309, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 88.4785, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 26.6940, Accuracy: 0.9375\n",
      "Batch number: 017, Training: Loss: 114.4610, Accuracy: 0.9062\n",
      "Batch number: 018, Training: Loss: 28.3932, Accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 019, Training: Loss: 55.0510, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 80.6468, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 150.9116, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 75.0977, Accuracy: 0.8438\n",
      "Batch number: 023, Training: Loss: 26.6963, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 31.1092, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 90.7144, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 452.6780, Accuracy: 0.6875\n",
      "Batch number: 027, Training: Loss: 131.2636, Accuracy: 0.8125\n",
      "Batch number: 028, Training: Loss: 185.2428, Accuracy: 0.8438\n",
      "Batch number: 029, Training: Loss: 122.7400, Accuracy: 0.9062\n",
      "Batch number: 030, Training: Loss: 89.4512, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 69.0652, Accuracy: 0.8750\n",
      "Batch number: 032, Training: Loss: 193.9576, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 40.3970, Accuracy: 0.9062\n",
      "Batch number: 034, Training: Loss: 120.1889, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 40.1356, Accuracy: 0.9688\n",
      "Batch number: 036, Training: Loss: 162.5537, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 15.3134, Accuracy: 0.9688\n",
      "Batch number: 038, Training: Loss: 38.5009, Accuracy: 0.9688\n",
      "Batch number: 039, Training: Loss: 39.4662, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 3.1602, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 35.6817, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 39.7493, Accuracy: 0.9688\n",
      "Batch number: 043, Training: Loss: 8.5616, Accuracy: 0.9688\n",
      "Batch number: 044, Training: Loss: 107.0173, Accuracy: 0.8438\n",
      "Batch number: 045, Training: Loss: 61.6387, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 136.9327, Accuracy: 0.8438\n",
      "Batch number: 047, Training: Loss: 239.1210, Accuracy: 0.7500\n",
      "Batch number: 048, Training: Loss: 51.0255, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 79.4568, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 53.3157, Accuracy: 0.9062\n",
      "Batch number: 051, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 37.3027, Accuracy: 0.9688\n",
      "Batch number: 054, Training: Loss: 27.8700, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 98.3551, Accuracy: 0.9062\n",
      "Batch number: 056, Training: Loss: 64.0115, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 40.2601, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 95.8153, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 79.4415, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 100.2659, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 94.9314, Accuracy: 0.9062\n",
      "Batch number: 062, Training: Loss: 239.8450, Accuracy: 0.7188\n",
      "Batch number: 063, Training: Loss: 10.6341, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 49.9214, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 62.9645, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 75.0994, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 27.3278, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 2.8830, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 58.0764, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 32.7308, Accuracy: 0.9062\n",
      "Batch number: 071, Training: Loss: 133.7805, Accuracy: 0.7812\n",
      "Batch number: 072, Training: Loss: 131.1853, Accuracy: 0.8438\n",
      "Batch number: 073, Training: Loss: 54.8248, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 22.6653, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 133.0341, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 23.0657, Accuracy: 0.9062\n",
      "Batch number: 077, Training: Loss: 51.3340, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 264.7750, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 17.2983, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 8.2499, Accuracy: 0.9062\n",
      "Batch number: 081, Training: Loss: 13.5663, Accuracy: 0.9375\n",
      "Batch number: 082, Training: Loss: 62.1833, Accuracy: 0.9062\n",
      "Batch number: 083, Training: Loss: 100.2980, Accuracy: 0.8438\n",
      "Batch number: 084, Training: Loss: 142.8634, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 9.0787, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 42.3666, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 47.6150, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 119.7919, Accuracy: 0.8438\n",
      "Batch number: 089, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 11.5218, Accuracy: 0.9688\n",
      "Batch number: 091, Training: Loss: 166.7947, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 117.2342, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 42.7136, Accuracy: 0.8438\n",
      "Batch number: 094, Training: Loss: 49.7780, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 29.0580, Accuracy: 0.9688\n",
      "Batch number: 096, Training: Loss: 54.6992, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 45.4033, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 65.5578, Accuracy: 0.9062\n",
      "Batch number: 099, Training: Loss: 41.2194, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 22.1332, Accuracy: 0.9688\n",
      "Batch number: 101, Training: Loss: 160.8874, Accuracy: 0.9062\n",
      "Batch number: 102, Training: Loss: 52.5678, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 50.0382, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 22.3259, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 55.6421, Accuracy: 0.8438\n",
      "Batch number: 106, Training: Loss: 63.7050, Accuracy: 0.9062\n",
      "Batch number: 107, Training: Loss: 116.6075, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 14.0045, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 11.1718, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 12.6298, Accuracy: 0.9688\n",
      "Batch number: 111, Training: Loss: 84.4234, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 121.4039, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 85.9074, Accuracy: 0.8125\n",
      "Batch number: 114, Training: Loss: 48.6347, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 95.5302, Accuracy: 0.8125\n",
      "Batch number: 116, Training: Loss: 1.1683, Accuracy: 0.9688\n",
      "Batch number: 117, Training: Loss: 14.0159, Accuracy: 0.9062\n",
      "Batch number: 118, Training: Loss: 28.4005, Accuracy: 0.8438\n",
      "Batch number: 119, Training: Loss: 37.2964, Accuracy: 0.9688\n",
      "Batch number: 120, Training: Loss: 137.1086, Accuracy: 0.8438\n",
      "Batch number: 121, Training: Loss: 137.5037, Accuracy: 0.8438\n",
      "Batch number: 122, Training: Loss: 27.5320, Accuracy: 0.8438\n",
      "Batch number: 123, Training: Loss: 124.9445, Accuracy: 0.8438\n",
      "Batch number: 124, Training: Loss: 168.7873, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 55.2750, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 26.2078, Accuracy: 0.9062\n",
      "Batch number: 127, Training: Loss: 35.3545, Accuracy: 0.9062\n",
      "Batch number: 128, Training: Loss: 49.9440, Accuracy: 0.9062\n",
      "Batch number: 129, Training: Loss: 153.3455, Accuracy: 0.7812\n",
      "Batch number: 130, Training: Loss: 33.7397, Accuracy: 0.8438\n",
      "Batch number: 131, Training: Loss: 46.5199, Accuracy: 0.8438\n",
      "Batch number: 132, Training: Loss: 37.2384, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 12.8489, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 35.2748, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 75.3489, Accuracy: 0.9062\n",
      "Batch number: 136, Training: Loss: 42.5897, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 79.6406, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 29.0605, Accuracy: 0.9688\n",
      "Batch number: 139, Training: Loss: 3.9479, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 13.7196, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 20.5042, Accuracy: 0.9688\n",
      "Batch number: 142, Training: Loss: 73.0718, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 47.2915, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 108.7148, Accuracy: 0.9062\n",
      "Batch number: 145, Training: Loss: 86.3037, Accuracy: 0.8438\n",
      "Batch number: 146, Training: Loss: 27.2253, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 8.3284, Accuracy: 0.9375\n",
      "Batch number: 148, Training: Loss: 14.1780, Accuracy: 0.9688\n",
      "Batch number: 149, Training: Loss: 2.0000, Accuracy: 0.9688\n",
      "Batch number: 150, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 151, Training: Loss: 64.8042, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 64.8587, Accuracy: 0.9062\n",
      "Batch number: 153, Training: Loss: 67.5640, Accuracy: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 154, Training: Loss: 166.2644, Accuracy: 0.8438\n",
      "Batch number: 155, Training: Loss: 63.1244, Accuracy: 0.8750\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 479.3509, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 65.2039, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 26.7075, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 661.4738, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 660.8929, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 766.2200, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 174.4409, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 103.3152, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 188.0379, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 94.6499, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 511.1890, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 79.6372, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 82.3515, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 508.7732, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 475.6436, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 110.8627, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 5.3940, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 91.1906, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 10.1729, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 240.9409, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 907.2376, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 383.5808, Accuracy: 0.5000\n",
      "Validation Batch number: 085, Validation: Loss: 792.3224, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 407.4656, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 105.8305, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 134.0391, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 1022.9555, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 114, Validation: Loss: 538.7808, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 52.8827, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 745.6576, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 525.2402, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 56.8858, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 357.4368, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 134.0199, Accuracy: 0.7500\n",
      "Epoch : 009, Training: Loss : 72.0499, Accuracy: 89.8919%\n",
      "Validation : Loss : 82.7395, Accuracy: 92.9856%, Time: 56.1573s\n",
      "Best accuracy achieved so far : 0.9335 on epoch 8\n",
      "Epoch: 11/25\n",
      "Batch number: 000, Training: Loss: 54.2747, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 110.6871, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 100.4889, Accuracy: 0.9062\n",
      "Batch number: 003, Training: Loss: 22.3284, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 17.5691, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 006, Training: Loss: 8.1703, Accuracy: 0.9688\n",
      "Batch number: 007, Training: Loss: 51.7850, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 19.3436, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 13.3629, Accuracy: 0.9062\n",
      "Batch number: 010, Training: Loss: 42.0995, Accuracy: 0.9688\n",
      "Batch number: 011, Training: Loss: 28.2215, Accuracy: 0.9062\n",
      "Batch number: 012, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 106.4170, Accuracy: 0.8125\n",
      "Batch number: 014, Training: Loss: 28.0033, Accuracy: 0.9688\n",
      "Batch number: 015, Training: Loss: 50.4138, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 344.8951, Accuracy: 0.7812\n",
      "Batch number: 017, Training: Loss: 44.6462, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 59.4629, Accuracy: 0.9062\n",
      "Batch number: 019, Training: Loss: 62.9069, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 97.2674, Accuracy: 0.8438\n",
      "Batch number: 021, Training: Loss: 47.0744, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 43.8629, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 52.2162, Accuracy: 0.8438\n",
      "Batch number: 024, Training: Loss: 29.1691, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 184.2101, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 98.0217, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 52.5882, Accuracy: 0.9062\n",
      "Batch number: 028, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 2.6322, Accuracy: 0.9688\n",
      "Batch number: 031, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 11.7626, Accuracy: 0.9688\n",
      "Batch number: 033, Training: Loss: 50.2582, Accuracy: 0.8438\n",
      "Batch number: 034, Training: Loss: 68.6336, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 22.6236, Accuracy: 0.9062\n",
      "Batch number: 036, Training: Loss: 26.8338, Accuracy: 0.9375\n",
      "Batch number: 037, Training: Loss: 21.4803, Accuracy: 0.9688\n",
      "Batch number: 038, Training: Loss: 7.1121, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 20.0225, Accuracy: 0.9688\n",
      "Batch number: 040, Training: Loss: 117.6312, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 37.8221, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 16.6049, Accuracy: 0.9062\n",
      "Batch number: 043, Training: Loss: 47.8208, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 167.2785, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 134.0033, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 24.9719, Accuracy: 0.9062\n",
      "Batch number: 047, Training: Loss: 186.7603, Accuracy: 0.8438\n",
      "Batch number: 048, Training: Loss: 6.0604, Accuracy: 0.9688\n",
      "Batch number: 049, Training: Loss: 51.9353, Accuracy: 0.9688\n",
      "Batch number: 050, Training: Loss: 78.2374, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 91.9515, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 21.0647, Accuracy: 0.9688\n",
      "Batch number: 053, Training: Loss: 89.5793, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 33.4335, Accuracy: 0.9062\n",
      "Batch number: 055, Training: Loss: 7.7920, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 5.2086, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 52.4510, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 55.5153, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 32.0474, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 061, Training: Loss: 286.5890, Accuracy: 0.9062\n",
      "Batch number: 062, Training: Loss: 117.9008, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 11.3758, Accuracy: 0.9062\n",
      "Batch number: 064, Training: Loss: 1.7837, Accuracy: 0.9688\n",
      "Batch number: 065, Training: Loss: 53.8393, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 18.2502, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 149.4673, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 15.9188, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 49.6250, Accuracy: 0.9062\n",
      "Batch number: 070, Training: Loss: 16.2408, Accuracy: 0.9062\n",
      "Batch number: 071, Training: Loss: 35.6857, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 16.9346, Accuracy: 0.9688\n",
      "Batch number: 073, Training: Loss: 39.9139, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 18.5709, Accuracy: 0.9688\n",
      "Batch number: 075, Training: Loss: 298.8024, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 29.7065, Accuracy: 0.8438\n",
      "Batch number: 077, Training: Loss: 41.5865, Accuracy: 0.9062\n",
      "Batch number: 078, Training: Loss: 28.0055, Accuracy: 0.9062\n",
      "Batch number: 079, Training: Loss: 25.4440, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 2.5368, Accuracy: 0.9688\n",
      "Batch number: 081, Training: Loss: 32.6056, Accuracy: 0.9688\n",
      "Batch number: 082, Training: Loss: 58.3955, Accuracy: 0.9062\n",
      "Batch number: 083, Training: Loss: 92.7932, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 085, Training: Loss: 4.9862, Accuracy: 0.9688\n",
      "Batch number: 086, Training: Loss: 54.1174, Accuracy: 0.8438\n",
      "Batch number: 087, Training: Loss: 14.8377, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 15.8008, Accuracy: 0.9062\n",
      "Batch number: 089, Training: Loss: 19.1468, Accuracy: 0.9062\n",
      "Batch number: 090, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 091, Training: Loss: 11.2448, Accuracy: 0.9688\n",
      "Batch number: 092, Training: Loss: 29.8093, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 50.9682, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 7.1560, Accuracy: 0.9688\n",
      "Batch number: 095, Training: Loss: 2.2505, Accuracy: 0.9688\n",
      "Batch number: 096, Training: Loss: 26.1305, Accuracy: 0.9062\n",
      "Batch number: 097, Training: Loss: 85.4800, Accuracy: 0.8438\n",
      "Batch number: 098, Training: Loss: 8.4202, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 1.1161, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 11.6477, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 102, Training: Loss: 40.0130, Accuracy: 0.8438\n",
      "Batch number: 103, Training: Loss: 26.6469, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 105, Training: Loss: 27.3385, Accuracy: 0.9062\n",
      "Batch number: 106, Training: Loss: 1818.9534, Accuracy: 0.9062\n",
      "Batch number: 107, Training: Loss: 19.7401, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 295.9355, Accuracy: 0.8125\n",
      "Batch number: 109, Training: Loss: 127.2553, Accuracy: 0.8750\n",
      "Batch number: 110, Training: Loss: 122.2187, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 321.7210, Accuracy: 0.6875\n",
      "Batch number: 112, Training: Loss: 186.8013, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 34.4212, Accuracy: 0.9062\n",
      "Batch number: 114, Training: Loss: 53.2399, Accuracy: 0.9062\n",
      "Batch number: 115, Training: Loss: 102.8048, Accuracy: 0.9062\n",
      "Batch number: 116, Training: Loss: 77.3380, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 126.8145, Accuracy: 0.9062\n",
      "Batch number: 118, Training: Loss: 278.8822, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 106.9225, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 220.2721, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 50.5227, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 54.2367, Accuracy: 0.9062\n",
      "Batch number: 124, Training: Loss: 157.9259, Accuracy: 0.8438\n",
      "Batch number: 125, Training: Loss: 134.2466, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 50.8308, Accuracy: 0.9062\n",
      "Batch number: 127, Training: Loss: 185.7703, Accuracy: 0.7812\n",
      "Batch number: 128, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 129, Training: Loss: 191.3481, Accuracy: 0.8438\n",
      "Batch number: 130, Training: Loss: 25.8049, Accuracy: 0.9062\n",
      "Batch number: 131, Training: Loss: 27.6531, Accuracy: 0.9688\n",
      "Batch number: 132, Training: Loss: 140.7506, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 513.4962, Accuracy: 0.8438\n",
      "Batch number: 134, Training: Loss: 548.6873, Accuracy: 0.8438\n",
      "Batch number: 135, Training: Loss: 93.2697, Accuracy: 0.9062\n",
      "Batch number: 136, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 224.4560, Accuracy: 0.7812\n",
      "Batch number: 138, Training: Loss: 1660.8632, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 71.7890, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 265.6911, Accuracy: 0.8125\n",
      "Batch number: 141, Training: Loss: 247.8849, Accuracy: 0.8438\n",
      "Batch number: 142, Training: Loss: 157.8908, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 377.4187, Accuracy: 0.7500\n",
      "Batch number: 144, Training: Loss: 559.7426, Accuracy: 0.7500\n",
      "Batch number: 145, Training: Loss: 109.9289, Accuracy: 0.9062\n",
      "Batch number: 146, Training: Loss: 101.9883, Accuracy: 0.9062\n",
      "Batch number: 147, Training: Loss: 194.5938, Accuracy: 0.7188\n",
      "Batch number: 148, Training: Loss: 179.2136, Accuracy: 0.8438\n",
      "Batch number: 149, Training: Loss: 13.0369, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 40.3353, Accuracy: 0.9062\n",
      "Batch number: 151, Training: Loss: 88.9787, Accuracy: 0.9062\n",
      "Batch number: 152, Training: Loss: 172.5139, Accuracy: 0.9062\n",
      "Batch number: 153, Training: Loss: 7.6094, Accuracy: 0.9688\n",
      "Batch number: 154, Training: Loss: 96.7912, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 114.9727, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 232.1204, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 502.2461, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 911.9775, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 646.3057, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 131.9936, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 652.5471, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 103.1236, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 95.5979, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 136.7596, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 844.0028, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 30.1532, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 273.6559, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 26.9068, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 356.7134, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 186.2740, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 558.1084, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 257.0787, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 428.7128, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 580.8804, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 412.3201, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 1103.7379, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 470.0951, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 85.0961, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 223.0943, Accuracy: 0.7500\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 457.7231, Accuracy: 0.7500\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 556.3488, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 1002.3314, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 567.2351, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 221.7065, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 1202.4800, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 847.3399, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 216.6743, Accuracy: 0.7500\n",
      "Epoch : 010, Training: Loss : 101.7880, Accuracy: 90.4323%\n",
      "Validation : Loss : 103.0312, Accuracy: 93.7050%, Time: 56.8428s\n",
      "model for epoch 10 saved\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 12/25\n",
      "Batch number: 000, Training: Loss: 135.6109, Accuracy: 0.8438\n",
      "Batch number: 001, Training: Loss: 355.7593, Accuracy: 0.8125\n",
      "Batch number: 002, Training: Loss: 375.3700, Accuracy: 0.8125\n",
      "Batch number: 003, Training: Loss: 77.0940, Accuracy: 0.9688\n",
      "Batch number: 004, Training: Loss: 103.7695, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 36.0591, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 64.7307, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 80.9933, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 137.6613, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 27.4436, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 53.3707, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 54.4329, Accuracy: 0.9062\n",
      "Batch number: 012, Training: Loss: 93.3049, Accuracy: 0.9062\n",
      "Batch number: 013, Training: Loss: 192.3061, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 6.8641, Accuracy: 0.9062\n",
      "Batch number: 015, Training: Loss: 79.8783, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 8.5667, Accuracy: 0.9688\n",
      "Batch number: 017, Training: Loss: 1182.1892, Accuracy: 0.6875\n",
      "Batch number: 018, Training: Loss: 65.1656, Accuracy: 0.9062\n",
      "Batch number: 019, Training: Loss: 72.4523, Accuracy: 0.9062\n",
      "Batch number: 020, Training: Loss: 260.7872, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 288.3341, Accuracy: 0.8438\n",
      "Batch number: 022, Training: Loss: 145.6637, Accuracy: 0.8125\n",
      "Batch number: 023, Training: Loss: 79.8233, Accuracy: 0.8438\n",
      "Batch number: 024, Training: Loss: 120.0646, Accuracy: 0.8125\n",
      "Batch number: 025, Training: Loss: 49.9761, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 532.5568, Accuracy: 0.7812\n",
      "Batch number: 027, Training: Loss: 21.8418, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 180.3416, Accuracy: 0.9062\n",
      "Batch number: 029, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 249.8962, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 83.7011, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 209.3580, Accuracy: 0.8438\n",
      "Batch number: 033, Training: Loss: 170.7231, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 47.2486, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 34.6667, Accuracy: 0.9688\n",
      "Batch number: 036, Training: Loss: 127.3043, Accuracy: 0.9062\n",
      "Batch number: 037, Training: Loss: 64.1416, Accuracy: 0.8750\n",
      "Batch number: 038, Training: Loss: 117.6585, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 71.1563, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 92.5566, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.0012, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 93.5343, Accuracy: 0.9062\n",
      "Batch number: 043, Training: Loss: 198.5666, Accuracy: 0.8125\n",
      "Batch number: 044, Training: Loss: 345.2127, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 127.3296, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 246.6308, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 76.2758, Accuracy: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 048, Training: Loss: 84.3019, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 131.3782, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 44.8943, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 159.6910, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 053, Training: Loss: 1.6756, Accuracy: 0.9688\n",
      "Batch number: 054, Training: Loss: 94.1174, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 170.5704, Accuracy: 0.7812\n",
      "Batch number: 056, Training: Loss: 133.2355, Accuracy: 0.8125\n",
      "Batch number: 057, Training: Loss: 120.3065, Accuracy: 0.8438\n",
      "Batch number: 058, Training: Loss: 22.6023, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 67.2745, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 58.2183, Accuracy: 0.8438\n",
      "Batch number: 061, Training: Loss: 460.3975, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 57.2447, Accuracy: 0.9062\n",
      "Batch number: 063, Training: Loss: 23.4783, Accuracy: 0.9688\n",
      "Batch number: 064, Training: Loss: 61.3683, Accuracy: 0.9062\n",
      "Batch number: 065, Training: Loss: 207.9283, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 170.4344, Accuracy: 0.9062\n",
      "Batch number: 067, Training: Loss: 48.5092, Accuracy: 0.9062\n",
      "Batch number: 068, Training: Loss: 132.9944, Accuracy: 0.9062\n",
      "Batch number: 069, Training: Loss: 58.7649, Accuracy: 0.9062\n",
      "Batch number: 070, Training: Loss: 72.7627, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 158.2077, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 65.2254, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 270.6870, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 87.9107, Accuracy: 0.9062\n",
      "Batch number: 075, Training: Loss: 213.6005, Accuracy: 0.7812\n",
      "Batch number: 076, Training: Loss: 174.2301, Accuracy: 0.7812\n",
      "Batch number: 077, Training: Loss: 46.5091, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 566.2302, Accuracy: 0.8438\n",
      "Batch number: 079, Training: Loss: 216.3164, Accuracy: 0.8438\n",
      "Batch number: 080, Training: Loss: 280.2497, Accuracy: 0.8750\n",
      "Batch number: 081, Training: Loss: 379.9127, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 276.6376, Accuracy: 0.8750\n",
      "Batch number: 083, Training: Loss: 235.7238, Accuracy: 0.7812\n",
      "Batch number: 084, Training: Loss: 193.3930, Accuracy: 0.8438\n",
      "Batch number: 085, Training: Loss: 91.6439, Accuracy: 0.8438\n",
      "Batch number: 086, Training: Loss: 95.6511, Accuracy: 0.8438\n",
      "Batch number: 087, Training: Loss: 91.4187, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 378.5011, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 100.9297, Accuracy: 0.9688\n",
      "Batch number: 090, Training: Loss: 195.3452, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 254.3251, Accuracy: 0.9062\n",
      "Batch number: 092, Training: Loss: 107.9313, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 185.7205, Accuracy: 0.7500\n",
      "Batch number: 094, Training: Loss: 120.0382, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 167.0968, Accuracy: 0.9688\n",
      "Batch number: 096, Training: Loss: 450.1830, Accuracy: 0.8438\n",
      "Batch number: 097, Training: Loss: 163.7932, Accuracy: 0.9062\n",
      "Batch number: 098, Training: Loss: 33.8558, Accuracy: 0.9688\n",
      "Batch number: 099, Training: Loss: 52.2902, Accuracy: 0.9062\n",
      "Batch number: 100, Training: Loss: 234.3528, Accuracy: 0.7812\n",
      "Batch number: 101, Training: Loss: 211.5550, Accuracy: 0.7188\n",
      "Batch number: 102, Training: Loss: 167.1667, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 97.4959, Accuracy: 0.9062\n",
      "Batch number: 104, Training: Loss: 156.7704, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 47.3622, Accuracy: 0.9062\n",
      "Batch number: 106, Training: Loss: 25.5335, Accuracy: 0.9688\n",
      "Batch number: 107, Training: Loss: 134.4659, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 78.0677, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 109.7653, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 380.4291, Accuracy: 0.7812\n",
      "Batch number: 112, Training: Loss: 171.2183, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 134.4599, Accuracy: 0.8438\n",
      "Batch number: 114, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 70.4142, Accuracy: 0.9062\n",
      "Batch number: 116, Training: Loss: 53.5807, Accuracy: 0.8438\n",
      "Batch number: 117, Training: Loss: 143.2130, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 41.8745, Accuracy: 0.9062\n",
      "Batch number: 119, Training: Loss: 289.4756, Accuracy: 0.7188\n",
      "Batch number: 120, Training: Loss: 114.8409, Accuracy: 0.8438\n",
      "Batch number: 121, Training: Loss: 154.2791, Accuracy: 0.9062\n",
      "Batch number: 122, Training: Loss: 48.3858, Accuracy: 0.8438\n",
      "Batch number: 123, Training: Loss: 15.2880, Accuracy: 0.9688\n",
      "Batch number: 124, Training: Loss: 159.3957, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 27.9607, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 157.4498, Accuracy: 0.8438\n",
      "Batch number: 127, Training: Loss: 97.5967, Accuracy: 0.9062\n",
      "Batch number: 128, Training: Loss: 27.2331, Accuracy: 0.9062\n",
      "Batch number: 129, Training: Loss: 461.4242, Accuracy: 0.6250\n",
      "Batch number: 130, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 131, Training: Loss: 173.2608, Accuracy: 0.9062\n",
      "Batch number: 132, Training: Loss: 72.1286, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 166.3580, Accuracy: 0.8438\n",
      "Batch number: 134, Training: Loss: 1.7083, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 117.5196, Accuracy: 0.8438\n",
      "Batch number: 136, Training: Loss: 114.9184, Accuracy: 0.9062\n",
      "Batch number: 137, Training: Loss: 99.8773, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 9.2070, Accuracy: 0.9688\n",
      "Batch number: 139, Training: Loss: 12.3171, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 172.2757, Accuracy: 0.8438\n",
      "Batch number: 141, Training: Loss: 39.9398, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 39.5907, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 153.4434, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 129.2582, Accuracy: 0.9688\n",
      "Batch number: 145, Training: Loss: 78.2132, Accuracy: 0.9062\n",
      "Batch number: 146, Training: Loss: 38.8520, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 69.8233, Accuracy: 0.9062\n",
      "Batch number: 148, Training: Loss: 113.4171, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 16.3648, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 13.1722, Accuracy: 0.9062\n",
      "Batch number: 151, Training: Loss: 45.7912, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 23.6071, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 62.5351, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 145.5195, Accuracy: 0.8125\n",
      "Batch number: 155, Training: Loss: 67.4176, Accuracy: 0.8750\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 344.9295, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 117.2338, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 183.8104, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 222.6216, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 121.6558, Accuracy: 0.7500\n",
      "Validation Batch number: 026, Validation: Loss: 441.1595, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 250.3295, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 71.1626, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 1.8751, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 91.1809, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 35.4880, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 38.5560, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 608.6921, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 281.8243, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 210.1549, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 533.7409, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 067, Validation: Loss: 24.8569, Accuracy: 0.7500\n",
      "Validation Batch number: 068, Validation: Loss: 182.6607, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 276.0680, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 325.4603, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 644.8549, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 69.2005, Accuracy: 0.5000\n",
      "Validation Batch number: 084, Validation: Loss: 226.6444, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 562.7021, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 517.0864, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 34.9439, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 56.3809, Accuracy: 0.7500\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 435.4323, Accuracy: 0.7500\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 191.0477, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 62.5975, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 67.1628, Accuracy: 0.7500\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 544.4395, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 215.8385, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 128.2441, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 456.7380, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 706.7942, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 114.8451, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch : 011, Training: Loss : 134.9025, Accuracy: 88.3106%\n",
      "Validation : Loss : 67.6145, Accuracy: 92.6259%, Time: 52.7587s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 13/25\n",
      "Batch number: 000, Training: Loss: 42.6190, Accuracy: 0.9062\n",
      "Batch number: 001, Training: Loss: 57.9499, Accuracy: 0.9062\n",
      "Batch number: 002, Training: Loss: 144.0788, Accuracy: 0.8438\n",
      "Batch number: 003, Training: Loss: 260.1315, Accuracy: 0.9062\n",
      "Batch number: 004, Training: Loss: 82.2075, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 10.2472, Accuracy: 0.9688\n",
      "Batch number: 006, Training: Loss: 63.3083, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 102.6022, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 30.5052, Accuracy: 0.9688\n",
      "Batch number: 009, Training: Loss: 59.2785, Accuracy: 0.9062\n",
      "Batch number: 010, Training: Loss: 17.9032, Accuracy: 0.9688\n",
      "Batch number: 011, Training: Loss: 54.1398, Accuracy: 0.9062\n",
      "Batch number: 012, Training: Loss: 1.9775, Accuracy: 0.9688\n",
      "Batch number: 013, Training: Loss: 109.5902, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 3.3013, Accuracy: 0.9688\n",
      "Batch number: 015, Training: Loss: 2351.6191, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 130.5708, Accuracy: 0.8438\n",
      "Batch number: 017, Training: Loss: 358.5182, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 427.7109, Accuracy: 0.8438\n",
      "Batch number: 019, Training: Loss: 226.6408, Accuracy: 0.8438\n",
      "Batch number: 020, Training: Loss: 117.8845, Accuracy: 0.9062\n",
      "Batch number: 021, Training: Loss: 314.1129, Accuracy: 0.8438\n",
      "Batch number: 022, Training: Loss: 135.0117, Accuracy: 0.7812\n",
      "Batch number: 023, Training: Loss: 279.4877, Accuracy: 0.7500\n",
      "Batch number: 024, Training: Loss: 152.4659, Accuracy: 0.8438\n",
      "Batch number: 025, Training: Loss: 129.9464, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 168.3521, Accuracy: 0.9062\n",
      "Batch number: 027, Training: Loss: 174.0224, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 122.6752, Accuracy: 0.9062\n",
      "Batch number: 029, Training: Loss: 77.0242, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 91.2165, Accuracy: 0.8750\n",
      "Batch number: 031, Training: Loss: 93.8652, Accuracy: 0.9062\n",
      "Batch number: 032, Training: Loss: 161.3679, Accuracy: 0.8438\n",
      "Batch number: 033, Training: Loss: 294.8098, Accuracy: 0.7188\n",
      "Batch number: 034, Training: Loss: 129.1524, Accuracy: 0.7812\n",
      "Batch number: 035, Training: Loss: 109.4810, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 320.0791, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 290.6743, Accuracy: 0.9062\n",
      "Batch number: 038, Training: Loss: 140.8155, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 202.1066, Accuracy: 0.9062\n",
      "Batch number: 040, Training: Loss: 42.9013, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 119.3135, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 211.0946, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 75.9991, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 155.3448, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 200.4678, Accuracy: 0.9062\n",
      "Batch number: 046, Training: Loss: 194.5170, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 111.3847, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 4.1583, Accuracy: 0.9688\n",
      "Batch number: 049, Training: Loss: 126.2700, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 198.0874, Accuracy: 0.8438\n",
      "Batch number: 051, Training: Loss: 193.0329, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 4.8265, Accuracy: 0.9688\n",
      "Batch number: 053, Training: Loss: 73.7559, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 46.7670, Accuracy: 0.8438\n",
      "Batch number: 055, Training: Loss: 455.2315, Accuracy: 0.7188\n",
      "Batch number: 056, Training: Loss: 69.3201, Accuracy: 0.8438\n",
      "Batch number: 057, Training: Loss: 72.2326, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 37.1186, Accuracy: 0.9688\n",
      "Batch number: 059, Training: Loss: 131.3374, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 117.4678, Accuracy: 0.9062\n",
      "Batch number: 061, Training: Loss: 387.3896, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 229.3412, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 28.8258, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 141.4000, Accuracy: 0.9062\n",
      "Batch number: 065, Training: Loss: 204.8760, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 107.6113, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 2.3823, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 80.7854, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 137.4542, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 123.1476, Accuracy: 0.9062\n",
      "Batch number: 071, Training: Loss: 281.4490, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 2345.8530, Accuracy: 0.7812\n",
      "Batch number: 073, Training: Loss: 236.5297, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 132.2891, Accuracy: 0.8438\n",
      "Batch number: 075, Training: Loss: 243.8654, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 17.2509, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 216.1515, Accuracy: 0.9062\n",
      "Batch number: 078, Training: Loss: 190.0741, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 1.8985, Accuracy: 0.9688\n",
      "Batch number: 080, Training: Loss: 30.1177, Accuracy: 0.9062\n",
      "Batch number: 081, Training: Loss: 163.4144, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 104.7791, Accuracy: 0.8438\n",
      "Batch number: 083, Training: Loss: 252.6866, Accuracy: 0.8438\n",
      "Batch number: 084, Training: Loss: 427.5920, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 74.6355, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 184.0235, Accuracy: 0.8438\n",
      "Batch number: 087, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 088, Training: Loss: 7.3397, Accuracy: 0.9688\n",
      "Batch number: 089, Training: Loss: 285.6288, Accuracy: 0.9062\n",
      "Batch number: 090, Training: Loss: 137.3771, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 282.4897, Accuracy: 0.8438\n",
      "Batch number: 092, Training: Loss: 224.5247, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 553.8043, Accuracy: 0.7812\n",
      "Batch number: 094, Training: Loss: 73.9932, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 442.2212, Accuracy: 0.8750\n",
      "Batch number: 096, Training: Loss: 396.3857, Accuracy: 0.9062\n",
      "Batch number: 097, Training: Loss: 156.4552, Accuracy: 0.9062\n",
      "Batch number: 098, Training: Loss: 473.7627, Accuracy: 0.9062\n",
      "Batch number: 099, Training: Loss: 202.5826, Accuracy: 0.9062\n",
      "Batch number: 100, Training: Loss: 236.0249, Accuracy: 0.8438\n",
      "Batch number: 101, Training: Loss: 395.7141, Accuracy: 0.7500\n",
      "Batch number: 102, Training: Loss: 188.3637, Accuracy: 0.8125\n",
      "Batch number: 103, Training: Loss: 143.9066, Accuracy: 0.8750\n",
      "Batch number: 104, Training: Loss: 121.3821, Accuracy: 0.9688\n",
      "Batch number: 105, Training: Loss: 205.8838, Accuracy: 0.9062\n",
      "Batch number: 106, Training: Loss: 42.1528, Accuracy: 0.9688\n",
      "Batch number: 107, Training: Loss: 195.2637, Accuracy: 0.8438\n",
      "Batch number: 108, Training: Loss: 153.3556, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 1.0570, Accuracy: 0.9688\n",
      "Batch number: 110, Training: Loss: 117.6649, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 554.0179, Accuracy: 0.8438\n",
      "Batch number: 112, Training: Loss: 131.8055, Accuracy: 0.9062\n",
      "Batch number: 113, Training: Loss: 250.5061, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 77.7144, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 349.5008, Accuracy: 0.8438\n",
      "Batch number: 116, Training: Loss: 126.1917, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 82.8906, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 299.4314, Accuracy: 0.8438\n",
      "Batch number: 119, Training: Loss: 506.6728, Accuracy: 0.7812\n",
      "Batch number: 120, Training: Loss: 98.3324, Accuracy: 0.8438\n",
      "Batch number: 121, Training: Loss: 186.7939, Accuracy: 0.9062\n",
      "Batch number: 122, Training: Loss: 170.3505, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 283.2965, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 289.8819, Accuracy: 0.8438\n",
      "Batch number: 125, Training: Loss: 52.7383, Accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 126, Training: Loss: 315.4867, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 555.3497, Accuracy: 0.7500\n",
      "Batch number: 128, Training: Loss: 186.1701, Accuracy: 0.8438\n",
      "Batch number: 129, Training: Loss: 54.0371, Accuracy: 0.9062\n",
      "Batch number: 130, Training: Loss: 143.8642, Accuracy: 0.9062\n",
      "Batch number: 131, Training: Loss: 60.8695, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 146.4012, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 333.7331, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 47.4682, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 248.7679, Accuracy: 0.8438\n",
      "Batch number: 137, Training: Loss: 42.5478, Accuracy: 0.9062\n",
      "Batch number: 138, Training: Loss: 403.3120, Accuracy: 0.7812\n",
      "Batch number: 139, Training: Loss: 21.3304, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 67.3761, Accuracy: 0.9062\n",
      "Batch number: 141, Training: Loss: 85.9678, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 293.5760, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 456.9679, Accuracy: 0.7500\n",
      "Batch number: 144, Training: Loss: 446.7414, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 136.0886, Accuracy: 0.9062\n",
      "Batch number: 146, Training: Loss: 47.6058, Accuracy: 0.9688\n",
      "Batch number: 147, Training: Loss: 365.5917, Accuracy: 0.8125\n",
      "Batch number: 148, Training: Loss: 223.4960, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 80.8694, Accuracy: 0.9062\n",
      "Batch number: 150, Training: Loss: 216.4955, Accuracy: 0.8125\n",
      "Batch number: 151, Training: Loss: 114.7182, Accuracy: 0.9062\n",
      "Batch number: 152, Training: Loss: 252.0082, Accuracy: 0.8438\n",
      "Batch number: 153, Training: Loss: 113.6748, Accuracy: 0.8438\n",
      "Batch number: 154, Training: Loss: 236.6662, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 169.9421, Accuracy: 0.8750\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 111.7494, Accuracy: 0.7500\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 517.1277, Accuracy: 0.5000\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 390.0215, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 266.7279, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 533.1492, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 50.6856, Accuracy: 0.7500\n",
      "Validation Batch number: 017, Validation: Loss: 317.3201, Accuracy: 0.7500\n",
      "Validation Batch number: 018, Validation: Loss: 157.0222, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 431.1096, Accuracy: 0.5000\n",
      "Validation Batch number: 021, Validation: Loss: 3896.9248, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 378.4006, Accuracy: 0.7500\n",
      "Validation Batch number: 026, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 86.6218, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 198.0155, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 43.0420, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 73.2793, Accuracy: 0.7500\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 137.7332, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 319.1922, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 86.7172, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 463.5027, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 137.2807, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 646.1796, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 925.6500, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 144.9233, Accuracy: 0.7500\n",
      "Validation Batch number: 064, Validation: Loss: 2.0700, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 480.1617, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 85.5601, Accuracy: 0.7500\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 76.7482, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 546.0772, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 1530.0710, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 228.8837, Accuracy: 0.5000\n",
      "Validation Batch number: 084, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 085, Validation: Loss: 1615.8264, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 73.8859, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 498.2491, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 180.1546, Accuracy: 0.7500\n",
      "Validation Batch number: 099, Validation: Loss: 260.5850, Accuracy: 0.7500\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 47.5184, Accuracy: 0.7500\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 417.8644, Accuracy: 0.7500\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 90.8676, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 115, Validation: Loss: 42.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 108.6663, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 2361.4014, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 254.4924, Accuracy: 0.5000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 251.3658, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 1355.8198, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 14.6402, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 1840.7490, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 256.1638, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 223.4926, Accuracy: 0.5000\n",
      "Validation Batch number: 137, Validation: Loss: 43.0207, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch : 012, Training: Loss : 201.0600, Accuracy: 88.4107%\n",
      "Validation : Loss : 166.9017, Accuracy: 89.3885%, Time: 52.8396s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 14/25\n",
      "Batch number: 000, Training: Loss: 184.0204, Accuracy: 0.9688\n",
      "Batch number: 001, Training: Loss: 7512.0596, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 110319.3516, Accuracy: 0.7188\n",
      "Batch number: 003, Training: Loss: 10156.6289, Accuracy: 0.8438\n",
      "Batch number: 004, Training: Loss: 1749.2754, Accuracy: 0.6250\n",
      "Batch number: 005, Training: Loss: 1939.1495, Accuracy: 0.7188\n",
      "Batch number: 006, Training: Loss: 707.1744, Accuracy: 0.8438\n",
      "Batch number: 007, Training: Loss: 428.6691, Accuracy: 0.7812\n",
      "Batch number: 008, Training: Loss: 1256.5773, Accuracy: 0.7812\n",
      "Batch number: 009, Training: Loss: 2276.3640, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 1163.6750, Accuracy: 0.8438\n",
      "Batch number: 011, Training: Loss: 7653.3301, Accuracy: 0.6250\n",
      "Batch number: 012, Training: Loss: 1105.3645, Accuracy: 0.9062\n",
      "Batch number: 013, Training: Loss: 1024.3247, Accuracy: 0.8125\n",
      "Batch number: 014, Training: Loss: 1200.0613, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 723.3083, Accuracy: 0.8438\n",
      "Batch number: 016, Training: Loss: 416.8148, Accuracy: 0.9062\n",
      "Batch number: 017, Training: Loss: 880.7694, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 2147.0645, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 1966.3423, Accuracy: 0.6562\n",
      "Batch number: 020, Training: Loss: 10677.7598, Accuracy: 0.6875\n",
      "Batch number: 021, Training: Loss: 3937.4907, Accuracy: 0.7500\n",
      "Batch number: 022, Training: Loss: 4618.1743, Accuracy: 0.7188\n",
      "Batch number: 023, Training: Loss: 536.0789, Accuracy: 0.9375\n",
      "Batch number: 024, Training: Loss: 1052.5586, Accuracy: 0.8438\n",
      "Batch number: 025, Training: Loss: 2658.9644, Accuracy: 0.7812\n",
      "Batch number: 026, Training: Loss: 951.2950, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 2724.6309, Accuracy: 0.7812\n",
      "Batch number: 028, Training: Loss: 1484.7126, Accuracy: 0.8438\n",
      "Batch number: 029, Training: Loss: 1669.7500, Accuracy: 0.8438\n",
      "Batch number: 030, Training: Loss: 1133.9812, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 718.9702, Accuracy: 0.9062\n",
      "Batch number: 032, Training: Loss: 11724.3164, Accuracy: 0.8125\n",
      "Batch number: 033, Training: Loss: 1926.0714, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 1542.6980, Accuracy: 0.8438\n",
      "Batch number: 035, Training: Loss: 2772.4487, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 2496.7737, Accuracy: 0.7812\n",
      "Batch number: 037, Training: Loss: 6033.2178, Accuracy: 0.6875\n",
      "Batch number: 038, Training: Loss: 3666.1577, Accuracy: 0.7500\n",
      "Batch number: 039, Training: Loss: 810.2912, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 685.0529, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 4743.1294, Accuracy: 0.6562\n",
      "Batch number: 042, Training: Loss: 3603.8804, Accuracy: 0.7500\n",
      "Batch number: 043, Training: Loss: 2198.7739, Accuracy: 0.8125\n",
      "Batch number: 044, Training: Loss: 924.2213, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 1421.9619, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 2769.5195, Accuracy: 0.6562\n",
      "Batch number: 047, Training: Loss: 2143.3394, Accuracy: 0.7500\n",
      "Batch number: 048, Training: Loss: 2672.5186, Accuracy: 0.7812\n",
      "Batch number: 049, Training: Loss: 312.3144, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 1956.1888, Accuracy: 0.8125\n",
      "Batch number: 051, Training: Loss: 3054.1895, Accuracy: 0.7812\n",
      "Batch number: 052, Training: Loss: 1218.2832, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 846.5389, Accuracy: 0.8438\n",
      "Batch number: 054, Training: Loss: 1711.8574, Accuracy: 0.7188\n",
      "Batch number: 055, Training: Loss: 6359.5576, Accuracy: 0.7188\n",
      "Batch number: 056, Training: Loss: 2400.3845, Accuracy: 0.7812\n",
      "Batch number: 057, Training: Loss: 878.4099, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 1964.1946, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 3449.1379, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 1005.2252, Accuracy: 0.8438\n",
      "Batch number: 061, Training: Loss: 2127.3291, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 1990.3586, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 662.4351, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 711.8567, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 1946.5464, Accuracy: 0.8438\n",
      "Batch number: 066, Training: Loss: 1566.2927, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 2643.6646, Accuracy: 0.8125\n",
      "Batch number: 068, Training: Loss: 113.6279, Accuracy: 0.9062\n",
      "Batch number: 069, Training: Loss: 1089.6663, Accuracy: 0.7812\n",
      "Batch number: 070, Training: Loss: 795.6230, Accuracy: 0.9375\n",
      "Batch number: 071, Training: Loss: 1954.6562, Accuracy: 0.6875\n",
      "Batch number: 072, Training: Loss: 1429.3320, Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 073, Training: Loss: 1198.6696, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 448.2766, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 3108.6570, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 594.2518, Accuracy: 0.8125\n",
      "Batch number: 077, Training: Loss: 1936.1735, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 1710.8774, Accuracy: 0.9062\n",
      "Batch number: 079, Training: Loss: 918.8033, Accuracy: 0.8125\n",
      "Batch number: 080, Training: Loss: 332.6700, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 2305.1077, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 1917.1918, Accuracy: 0.9062\n",
      "Batch number: 083, Training: Loss: 1829.0900, Accuracy: 0.7188\n",
      "Batch number: 084, Training: Loss: 406.9439, Accuracy: 0.9062\n",
      "Batch number: 085, Training: Loss: 1046.1182, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 322.4152, Accuracy: 0.9688\n",
      "Batch number: 087, Training: Loss: 514.2113, Accuracy: 0.9062\n",
      "Batch number: 088, Training: Loss: 2219.2361, Accuracy: 0.8438\n",
      "Batch number: 089, Training: Loss: 567.9531, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 547.0841, Accuracy: 0.9062\n",
      "Batch number: 091, Training: Loss: 1531.3348, Accuracy: 0.7500\n",
      "Batch number: 092, Training: Loss: 1407.7244, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 1762.9028, Accuracy: 0.7500\n",
      "Batch number: 094, Training: Loss: 151.9356, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 1070.5651, Accuracy: 0.8438\n",
      "Batch number: 096, Training: Loss: 1343.3981, Accuracy: 0.8125\n",
      "Batch number: 097, Training: Loss: 729.0497, Accuracy: 0.9062\n",
      "Batch number: 098, Training: Loss: 560.0743, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 369.8090, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 667.5609, Accuracy: 0.8125\n",
      "Batch number: 101, Training: Loss: 649.2516, Accuracy: 0.7812\n",
      "Batch number: 102, Training: Loss: 1689.4532, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 242.5073, Accuracy: 0.9688\n",
      "Batch number: 104, Training: Loss: 771.3953, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 1451.5446, Accuracy: 0.7188\n",
      "Batch number: 106, Training: Loss: 634.7994, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 430.0837, Accuracy: 0.8438\n",
      "Batch number: 108, Training: Loss: 1284.1331, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 282.2137, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 1474.0594, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 720.4296, Accuracy: 0.9062\n",
      "Batch number: 112, Training: Loss: 1635.2064, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 682.3596, Accuracy: 0.8438\n",
      "Batch number: 114, Training: Loss: 436.1972, Accuracy: 0.9062\n",
      "Batch number: 115, Training: Loss: 321.8289, Accuracy: 0.9062\n",
      "Batch number: 116, Training: Loss: 450.1259, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 800.2152, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 171.4834, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 332.8138, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 723.5926, Accuracy: 0.8125\n",
      "Batch number: 121, Training: Loss: 1855.3698, Accuracy: 0.8438\n",
      "Batch number: 122, Training: Loss: 255.8213, Accuracy: 0.9062\n",
      "Batch number: 123, Training: Loss: 899.0326, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 974.1328, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 779.7629, Accuracy: 0.9062\n",
      "Batch number: 126, Training: Loss: 934.8671, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 1750.5117, Accuracy: 0.7812\n",
      "Batch number: 128, Training: Loss: 192.1575, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 130, Training: Loss: 108.6688, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 990.0276, Accuracy: 0.8125\n",
      "Batch number: 132, Training: Loss: 1635.8083, Accuracy: 0.8438\n",
      "Batch number: 133, Training: Loss: 1434.3202, Accuracy: 0.8438\n",
      "Batch number: 134, Training: Loss: 371.6218, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 893.2381, Accuracy: 0.8125\n",
      "Batch number: 136, Training: Loss: 995.8373, Accuracy: 0.8438\n",
      "Batch number: 137, Training: Loss: 842.6261, Accuracy: 0.8125\n",
      "Batch number: 138, Training: Loss: 704.3353, Accuracy: 0.8438\n",
      "Batch number: 139, Training: Loss: 982.8649, Accuracy: 0.9062\n",
      "Batch number: 140, Training: Loss: 655.1797, Accuracy: 0.9062\n",
      "Batch number: 141, Training: Loss: 1605.8502, Accuracy: 0.8438\n",
      "Batch number: 142, Training: Loss: 679.2170, Accuracy: 0.8125\n",
      "Batch number: 143, Training: Loss: 310.5118, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 1978.4446, Accuracy: 0.7812\n",
      "Batch number: 145, Training: Loss: 1309.0358, Accuracy: 0.7500\n",
      "Batch number: 146, Training: Loss: 786.9504, Accuracy: 0.9062\n",
      "Batch number: 147, Training: Loss: 1312.6425, Accuracy: 0.7188\n",
      "Batch number: 148, Training: Loss: 3938.2495, Accuracy: 0.7188\n",
      "Batch number: 149, Training: Loss: 871.9584, Accuracy: 0.8125\n",
      "Batch number: 150, Training: Loss: 889.5663, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 141.5288, Accuracy: 0.9688\n",
      "Batch number: 152, Training: Loss: 941.8599, Accuracy: 0.7812\n",
      "Batch number: 153, Training: Loss: 1591.1285, Accuracy: 0.8438\n",
      "Batch number: 154, Training: Loss: 2141.3438, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 1470.7737, Accuracy: 0.8750\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 725.3739, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 1822.9352, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 227.3115, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 728.2274, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 101.0342, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 2369.0210, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 2039.2937, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 9.7117, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 1836.7744, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 1004.9216, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 618.7494, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 1406.0281, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 804.4216, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 637.0244, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 20.1531, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 640.4176, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 211.7935, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 1724.6106, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 1023.0085, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 2718.4102, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 838.4050, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 1427.6788, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 788.2378, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 2117.0728, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 410.3027, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 3142.8301, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 1375.6241, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 1315.5117, Accuracy: 0.5000\n",
      "Validation Batch number: 085, Validation: Loss: 5789.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 1759.9321, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 846.6172, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 780.9302, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 3070.3186, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 1254.3389, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 543.0833, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 2393.7681, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 1110.2246, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 846.4534, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 200.0072, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 2214.7644, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 75.5738, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 668.6938, Accuracy: 0.7500\n",
      "Epoch : 013, Training: Loss : 2339.3035, Accuracy: 83.9271%\n",
      "Validation : Loss : 385.8941, Accuracy: 91.1871%, Time: 54.1842s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 15/25\n",
      "Batch number: 000, Training: Loss: 2085.8875, Accuracy: 0.9062\n",
      "Batch number: 001, Training: Loss: 1664.3665, Accuracy: 0.7812\n",
      "Batch number: 002, Training: Loss: 720.0416, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 1029.2496, Accuracy: 0.8125\n",
      "Batch number: 004, Training: Loss: 1130.8434, Accuracy: 0.7188\n",
      "Batch number: 005, Training: Loss: 318.1399, Accuracy: 0.8750\n",
      "Batch number: 006, Training: Loss: 158.0642, Accuracy: 0.9062\n",
      "Batch number: 007, Training: Loss: 239.2942, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 283.6542, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 514.5870, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 161.6415, Accuracy: 0.9688\n",
      "Batch number: 011, Training: Loss: 161.5636, Accuracy: 0.9062\n",
      "Batch number: 012, Training: Loss: 1094.1263, Accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 013, Training: Loss: 469.6006, Accuracy: 0.8125\n",
      "Batch number: 014, Training: Loss: 99.6123, Accuracy: 0.9688\n",
      "Batch number: 015, Training: Loss: 891.6996, Accuracy: 0.8438\n",
      "Batch number: 016, Training: Loss: 2075.5198, Accuracy: 0.7188\n",
      "Batch number: 017, Training: Loss: 216.0271, Accuracy: 0.9062\n",
      "Batch number: 018, Training: Loss: 114.2884, Accuracy: 0.9062\n",
      "Batch number: 019, Training: Loss: 905.8359, Accuracy: 0.7500\n",
      "Batch number: 020, Training: Loss: 1150.6077, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 1706.2097, Accuracy: 0.7188\n",
      "Batch number: 022, Training: Loss: 303.8827, Accuracy: 0.9062\n",
      "Batch number: 023, Training: Loss: 297.6282, Accuracy: 0.9688\n",
      "Batch number: 024, Training: Loss: 1046.5460, Accuracy: 0.7812\n",
      "Batch number: 025, Training: Loss: 1316.5024, Accuracy: 0.8125\n",
      "Batch number: 026, Training: Loss: 1016.2349, Accuracy: 0.9062\n",
      "Batch number: 027, Training: Loss: 396.0113, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 1459.1881, Accuracy: 0.9062\n",
      "Batch number: 029, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 49.8826, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 648.2291, Accuracy: 0.8750\n",
      "Batch number: 032, Training: Loss: 470.8821, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 97.6853, Accuracy: 0.9688\n",
      "Batch number: 034, Training: Loss: 854.8921, Accuracy: 0.9062\n",
      "Batch number: 035, Training: Loss: 277.9554, Accuracy: 0.9062\n",
      "Batch number: 036, Training: Loss: 650.7650, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 25.5394, Accuracy: 0.9062\n",
      "Batch number: 038, Training: Loss: 787.7539, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 345.5588, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 1131.5923, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 504.0186, Accuracy: 0.9062\n",
      "Batch number: 042, Training: Loss: 437.1852, Accuracy: 0.8438\n",
      "Batch number: 043, Training: Loss: 450.0212, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 287.1210, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 610.3066, Accuracy: 0.8438\n",
      "Batch number: 046, Training: Loss: 742.0167, Accuracy: 0.7812\n",
      "Batch number: 047, Training: Loss: 457.4406, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 702.6260, Accuracy: 0.9062\n",
      "Batch number: 049, Training: Loss: 1391.2334, Accuracy: 0.7188\n",
      "Batch number: 050, Training: Loss: 605.4290, Accuracy: 0.8438\n",
      "Batch number: 051, Training: Loss: 739.9167, Accuracy: 0.9062\n",
      "Batch number: 052, Training: Loss: 582.7006, Accuracy: 0.8125\n",
      "Batch number: 053, Training: Loss: 132.0675, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 802.6194, Accuracy: 0.8438\n",
      "Batch number: 055, Training: Loss: 516.2303, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 901.8301, Accuracy: 0.7812\n",
      "Batch number: 057, Training: Loss: 503.4860, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 1253.1885, Accuracy: 0.7812\n",
      "Batch number: 059, Training: Loss: 256.0608, Accuracy: 0.8438\n",
      "Batch number: 060, Training: Loss: 108.4215, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 49.4106, Accuracy: 0.9062\n",
      "Batch number: 062, Training: Loss: 484.7631, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 134.2663, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 483.3568, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 546.6051, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 417.7666, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 617.5096, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 130.9731, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 402.7308, Accuracy: 0.9062\n",
      "Batch number: 070, Training: Loss: 601.8557, Accuracy: 0.8438\n",
      "Batch number: 071, Training: Loss: 291.3969, Accuracy: 0.8438\n",
      "Batch number: 072, Training: Loss: 389.7019, Accuracy: 0.9062\n",
      "Batch number: 073, Training: Loss: 136.7437, Accuracy: 0.9062\n",
      "Batch number: 074, Training: Loss: 73.6818, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 1033.8612, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 761.0713, Accuracy: 0.8125\n",
      "Batch number: 077, Training: Loss: 242.6316, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 371.1417, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 293.6455, Accuracy: 0.9062\n",
      "Batch number: 080, Training: Loss: 70.2659, Accuracy: 0.9688\n",
      "Batch number: 081, Training: Loss: 1461.4873, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 1015.0634, Accuracy: 0.8125\n",
      "Batch number: 083, Training: Loss: 520.9853, Accuracy: 0.8125\n",
      "Batch number: 084, Training: Loss: 942.3504, Accuracy: 0.8750\n",
      "Batch number: 085, Training: Loss: 5.7277, Accuracy: 0.9688\n",
      "Batch number: 086, Training: Loss: 972.1010, Accuracy: 0.8125\n",
      "Batch number: 087, Training: Loss: 343.3015, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 447.5856, Accuracy: 0.9062\n",
      "Batch number: 089, Training: Loss: 316.6118, Accuracy: 0.9062\n",
      "Batch number: 090, Training: Loss: 117.2163, Accuracy: 0.9688\n",
      "Batch number: 091, Training: Loss: 1146.3770, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 171.6237, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 755.2961, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 110.2258, Accuracy: 0.9062\n",
      "Batch number: 095, Training: Loss: 301.1554, Accuracy: 0.9062\n",
      "Batch number: 096, Training: Loss: 3.1515, Accuracy: 0.9688\n",
      "Batch number: 097, Training: Loss: 63.8452, Accuracy: 0.9688\n",
      "Batch number: 098, Training: Loss: 469.0428, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 503.4937, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 25772.2441, Accuracy: 0.8438\n",
      "Batch number: 101, Training: Loss: 882.6871, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 3637.6353, Accuracy: 0.7812\n",
      "Batch number: 103, Training: Loss: 3865.8459, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 3505.4331, Accuracy: 0.7812\n",
      "Batch number: 105, Training: Loss: 3526.3555, Accuracy: 0.6250\n",
      "Batch number: 106, Training: Loss: 54.7943, Accuracy: 0.9688\n",
      "Batch number: 107, Training: Loss: 1073.3401, Accuracy: 0.8125\n",
      "Batch number: 108, Training: Loss: 4022.6606, Accuracy: 0.5938\n",
      "Batch number: 109, Training: Loss: 597.9761, Accuracy: 0.7812\n",
      "Batch number: 110, Training: Loss: 449.7368, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 1686.5535, Accuracy: 0.7500\n",
      "Batch number: 112, Training: Loss: 15.4705, Accuracy: 0.9688\n",
      "Batch number: 113, Training: Loss: 104.7744, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 1934.0654, Accuracy: 0.8125\n",
      "Batch number: 115, Training: Loss: 128.5520, Accuracy: 0.9688\n",
      "Batch number: 116, Training: Loss: 668.2554, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 717.6976, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 2164.2090, Accuracy: 0.7812\n",
      "Batch number: 119, Training: Loss: 543.5831, Accuracy: 0.8125\n",
      "Batch number: 120, Training: Loss: 396.7626, Accuracy: 0.9062\n",
      "Batch number: 121, Training: Loss: 1070.0121, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 961.0378, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 413.3895, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 247.3217, Accuracy: 0.8438\n",
      "Batch number: 125, Training: Loss: 214.8272, Accuracy: 0.9062\n",
      "Batch number: 126, Training: Loss: 987.3506, Accuracy: 0.8438\n",
      "Batch number: 127, Training: Loss: 644.8590, Accuracy: 0.8438\n",
      "Batch number: 128, Training: Loss: 134.6616, Accuracy: 0.9062\n",
      "Batch number: 129, Training: Loss: 1187.6150, Accuracy: 0.8438\n",
      "Batch number: 130, Training: Loss: 237.8123, Accuracy: 0.9062\n",
      "Batch number: 131, Training: Loss: 839.0177, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 276.6215, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 3557.6663, Accuracy: 0.7500\n",
      "Batch number: 134, Training: Loss: 346.6180, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 555.2581, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 550.5608, Accuracy: 0.8438\n",
      "Batch number: 137, Training: Loss: 912.2185, Accuracy: 0.7812\n",
      "Batch number: 138, Training: Loss: 1179.1935, Accuracy: 0.8438\n",
      "Batch number: 139, Training: Loss: 54.3866, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 1260.4485, Accuracy: 0.7500\n",
      "Batch number: 141, Training: Loss: 93.4994, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 487.2031, Accuracy: 0.8438\n",
      "Batch number: 143, Training: Loss: 1067.8781, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 1753.0708, Accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 145, Training: Loss: 204.5267, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 834.9140, Accuracy: 0.8750\n",
      "Batch number: 147, Training: Loss: 430.1346, Accuracy: 0.8438\n",
      "Batch number: 148, Training: Loss: 1211.7051, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 17.6955, Accuracy: 0.9688\n",
      "Batch number: 150, Training: Loss: 744.9026, Accuracy: 0.8438\n",
      "Batch number: 151, Training: Loss: 1045.3553, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 826.5408, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 633.1104, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 659.1626, Accuracy: 0.9062\n",
      "Batch number: 155, Training: Loss: 610.4488, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 1552.8120, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 870.6074, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 1635.4166, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 463.7310, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 2738.0803, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 2660.9897, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 483.8874, Accuracy: 0.7500\n",
      "Validation Batch number: 023, Validation: Loss: 639.1588, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 2847.2180, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 1545.1620, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 63.0474, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 534.6000, Accuracy: 0.7500\n",
      "Validation Batch number: 032, Validation: Loss: 1449.0955, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 1317.0140, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 1787.2961, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 2440.0254, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 586.2264, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 444.3439, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 504.3939, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 1926.3325, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 1850.7542, Accuracy: 0.5000\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 805.7958, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 902.6075, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 1877.5571, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 3250.4504, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 1162.9719, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 959.0248, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 2096.7417, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 1728.4573, Accuracy: 0.5000\n",
      "Validation Batch number: 085, Validation: Loss: 7366.2754, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 215.6230, Accuracy: 0.7500\n",
      "Validation Batch number: 087, Validation: Loss: 1903.0039, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 114.8072, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 1001.3182, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 1912.3964, Accuracy: 0.7500\n",
      "Validation Batch number: 100, Validation: Loss: 1720.3696, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 105, Validation: Loss: 393.0294, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 3635.4248, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 387.2724, Accuracy: 0.7500\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 2511.0044, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 3434.9463, Accuracy: 0.5000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 1612.1724, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 1661.8760, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 522.8378, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 1581.2544, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 597.7161, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 651.0115, Accuracy: 0.7500\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 783.4462, Accuracy: 0.7500\n",
      "Epoch : 014, Training: Loss : 893.8427, Accuracy: 87.1898%\n",
      "Validation : Loss : 526.1121, Accuracy: 89.9281%, Time: 53.2152s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 16/25\n",
      "Batch number: 000, Training: Loss: 998.4744, Accuracy: 0.8125\n",
      "Batch number: 001, Training: Loss: 668.7055, Accuracy: 0.7500\n",
      "Batch number: 002, Training: Loss: 736.3762, Accuracy: 0.7812\n",
      "Batch number: 003, Training: Loss: 1089.7244, Accuracy: 0.8125\n",
      "Batch number: 004, Training: Loss: 1501.5266, Accuracy: 0.7188\n",
      "Batch number: 005, Training: Loss: 343.8076, Accuracy: 0.9062\n",
      "Batch number: 006, Training: Loss: 226.3273, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 261.0811, Accuracy: 0.9062\n",
      "Batch number: 008, Training: Loss: 619.6078, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 279.7291, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 266.0469, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 348.3495, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 379.9769, Accuracy: 0.8438\n",
      "Batch number: 013, Training: Loss: 617.6779, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 354.5288, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 807.3206, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 693.7651, Accuracy: 0.8125\n",
      "Batch number: 018, Training: Loss: 498.2219, Accuracy: 0.9062\n",
      "Batch number: 019, Training: Loss: 262.0576, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 432.4158, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 567.7146, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 932.2476, Accuracy: 0.8125\n",
      "Batch number: 023, Training: Loss: 203.4803, Accuracy: 0.9688\n",
      "Batch number: 024, Training: Loss: 926.7462, Accuracy: 0.8125\n",
      "Batch number: 025, Training: Loss: 40.9239, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 3314.9612, Accuracy: 0.7188\n",
      "Batch number: 027, Training: Loss: 766.0891, Accuracy: 0.9062\n",
      "Batch number: 028, Training: Loss: 117.6107, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 612.6342, Accuracy: 0.8438\n",
      "Batch number: 030, Training: Loss: 587.1759, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 168.4962, Accuracy: 0.9062\n",
      "Batch number: 032, Training: Loss: 320.7653, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 2210.5269, Accuracy: 0.7188\n",
      "Batch number: 034, Training: Loss: 944.1776, Accuracy: 0.8125\n",
      "Batch number: 035, Training: Loss: 547.9816, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 845.4739, Accuracy: 0.8438\n",
      "Batch number: 037, Training: Loss: 248.6668, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 985.5782, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 949.8057, Accuracy: 0.7812\n",
      "Batch number: 040, Training: Loss: 253.9721, Accuracy: 0.9688\n",
      "Batch number: 041, Training: Loss: 11.5206, Accuracy: 0.9688\n",
      "Batch number: 042, Training: Loss: 518.4321, Accuracy: 0.8438\n",
      "Batch number: 043, Training: Loss: 239.7864, Accuracy: 0.9062\n",
      "Batch number: 044, Training: Loss: 1116.6085, Accuracy: 0.7188\n",
      "Batch number: 045, Training: Loss: 901.2068, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 346.8138, Accuracy: 0.9062\n",
      "Batch number: 047, Training: Loss: 893.4473, Accuracy: 0.8438\n",
      "Batch number: 048, Training: Loss: 496.8153, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 450.2324, Accuracy: 0.9062\n",
      "Batch number: 050, Training: Loss: 540.6328, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 190.6745, Accuracy: 0.9688\n",
      "Batch number: 052, Training: Loss: 394.9297, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 252.1413, Accuracy: 0.9688\n",
      "Batch number: 054, Training: Loss: 559.4072, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 544.0652, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 591.5883, Accuracy: 0.9062\n",
      "Batch number: 057, Training: Loss: 222.1134, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 792.0951, Accuracy: 0.7812\n",
      "Batch number: 059, Training: Loss: 597.8265, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 178.1148, Accuracy: 0.9062\n",
      "Batch number: 061, Training: Loss: 797.7054, Accuracy: 0.9062\n",
      "Batch number: 062, Training: Loss: 975.1534, Accuracy: 0.9062\n",
      "Batch number: 063, Training: Loss: 487.0500, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 350.6675, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 444.0856, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 846.4836, Accuracy: 0.8125\n",
      "Batch number: 067, Training: Loss: 946.4891, Accuracy: 0.9062\n",
      "Batch number: 068, Training: Loss: 427.8051, Accuracy: 0.9062\n",
      "Batch number: 069, Training: Loss: 761.3375, Accuracy: 0.9062\n",
      "Batch number: 070, Training: Loss: 1499.0388, Accuracy: 0.8438\n",
      "Batch number: 071, Training: Loss: 900.0313, Accuracy: 0.7812\n",
      "Batch number: 072, Training: Loss: 123.7933, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 327.0557, Accuracy: 0.9062\n",
      "Batch number: 074, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 1135.4922, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 1499.2926, Accuracy: 0.6875\n",
      "Batch number: 077, Training: Loss: 318.7748, Accuracy: 0.9062\n",
      "Batch number: 078, Training: Loss: 514.1342, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 382.4390, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 390.3164, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 889.9203, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 567.9195, Accuracy: 0.8125\n",
      "Batch number: 083, Training: Loss: 806.7408, Accuracy: 0.7188\n",
      "Batch number: 084, Training: Loss: 291.8508, Accuracy: 0.9062\n",
      "Batch number: 085, Training: Loss: 38.0925, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 1306.0813, Accuracy: 0.7188\n",
      "Batch number: 087, Training: Loss: 1404.1868, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 717.2037, Accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 089, Training: Loss: 120.1952, Accuracy: 0.8438\n",
      "Batch number: 090, Training: Loss: 51.3607, Accuracy: 0.9062\n",
      "Batch number: 091, Training: Loss: 648.7834, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 576.8324, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 223.7055, Accuracy: 0.9062\n",
      "Batch number: 094, Training: Loss: 205.1572, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 988.8249, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 554.2583, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 958.6523, Accuracy: 0.8750\n",
      "Batch number: 098, Training: Loss: 368.8390, Accuracy: 0.9062\n",
      "Batch number: 099, Training: Loss: 590.5791, Accuracy: 0.9062\n",
      "Batch number: 100, Training: Loss: 305.1094, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 261.4899, Accuracy: 0.9062\n",
      "Batch number: 102, Training: Loss: 566.0151, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 155.9117, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 770.0029, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 592.0117, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 356.4004, Accuracy: 0.8750\n",
      "Batch number: 107, Training: Loss: 372.8574, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 168.8674, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 675.9517, Accuracy: 0.9062\n",
      "Batch number: 111, Training: Loss: 3583.0200, Accuracy: 0.7812\n",
      "Batch number: 112, Training: Loss: 528.5663, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 204.7435, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 74.9893, Accuracy: 0.9688\n",
      "Batch number: 115, Training: Loss: 914.9528, Accuracy: 0.7188\n",
      "Batch number: 116, Training: Loss: 469.1069, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 144.6941, Accuracy: 0.9062\n",
      "Batch number: 118, Training: Loss: 52.5523, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 161.6361, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 161.5385, Accuracy: 0.9062\n",
      "Batch number: 121, Training: Loss: 1343.3112, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 311.2043, Accuracy: 0.9062\n",
      "Batch number: 123, Training: Loss: 451.6027, Accuracy: 0.9062\n",
      "Batch number: 124, Training: Loss: 602.9692, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 189.6605, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 170.5976, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 770.9644, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 586.9889, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 551.5713, Accuracy: 0.8438\n",
      "Batch number: 130, Training: Loss: 580.5452, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 1299.3318, Accuracy: 0.7812\n",
      "Batch number: 132, Training: Loss: 122.5933, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 169.9507, Accuracy: 0.9688\n",
      "Batch number: 134, Training: Loss: 258.4494, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 202.9760, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 481.9086, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 482.7044, Accuracy: 0.8125\n",
      "Batch number: 138, Training: Loss: 655.6757, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 575.2902, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 325.2656, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 174.5455, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 1199.9924, Accuracy: 0.7812\n",
      "Batch number: 144, Training: Loss: 1310.4651, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 120.8978, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 523.3744, Accuracy: 0.8438\n",
      "Batch number: 147, Training: Loss: 1239.4620, Accuracy: 0.7500\n",
      "Batch number: 148, Training: Loss: 750.3162, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 275.5707, Accuracy: 0.8438\n",
      "Batch number: 150, Training: Loss: 573.9172, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 1294.3859, Accuracy: 0.8438\n",
      "Batch number: 152, Training: Loss: 339.8026, Accuracy: 0.9688\n",
      "Batch number: 153, Training: Loss: 68.5448, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 1222.4379, Accuracy: 0.8750\n",
      "Batch number: 155, Training: Loss: 1107.8931, Accuracy: 0.8125\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 120.0282, Accuracy: 0.7500\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 817.0160, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 2247.2490, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 1833.1866, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 511.3374, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 3210.8660, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 1876.9199, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 392.1477, Accuracy: 0.7500\n",
      "Validation Batch number: 023, Validation: Loss: 677.6976, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 3415.7329, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 2149.8362, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 552.8701, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 1126.6208, Accuracy: 0.7500\n",
      "Validation Batch number: 032, Validation: Loss: 1728.4263, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 504.4510, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 788.9766, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 3764.4692, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 183.8768, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 539.7180, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 360.2176, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 2897.2603, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 1041.7388, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 3508.2244, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 356.5553, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 1514.0120, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 4639.6514, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 363.0018, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 756.3151, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 3049.7603, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 131.8596, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 1885.6707, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 9183.2520, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 1152.7380, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 846.9821, Accuracy: 0.5000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 1032.8938, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 1421.5088, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 1489.8789, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 2216.0784, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 2734.8423, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 19.1183, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 311.6894, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 2119.2959, Accuracy: 0.7500\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 559.3148, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 2062.1587, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 2162.4858, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 243.4868, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 2119.0229, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 328.7173, Accuracy: 0.7500\n",
      "Epoch : 015, Training: Loss : 589.6629, Accuracy: 87.8503%\n",
      "Validation : Loss : 553.5911, Accuracy: 90.1079%, Time: 53.1379s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 17/25\n",
      "Batch number: 000, Training: Loss: 455.7903, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 849.9834, Accuracy: 0.8438\n",
      "Batch number: 002, Training: Loss: 1236.9410, Accuracy: 0.8125\n",
      "Batch number: 003, Training: Loss: 1030.5828, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 839.3536, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 602.8865, Accuracy: 0.8750\n",
      "Batch number: 006, Training: Loss: 550.0820, Accuracy: 0.9062\n",
      "Batch number: 007, Training: Loss: 709.4166, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 280.7657, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 124.5190, Accuracy: 0.9062\n",
      "Batch number: 010, Training: Loss: 193.1929, Accuracy: 0.9062\n",
      "Batch number: 011, Training: Loss: 24.1380, Accuracy: 0.9688\n",
      "Batch number: 012, Training: Loss: 112.4846, Accuracy: 0.9062\n",
      "Batch number: 013, Training: Loss: 371.1992, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 62.1074, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 642.4277, Accuracy: 0.9062\n",
      "Batch number: 016, Training: Loss: 1731.1278, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 183.8203, Accuracy: 0.9688\n",
      "Batch number: 018, Training: Loss: 568.9634, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 486.5914, Accuracy: 0.8438\n",
      "Batch number: 020, Training: Loss: 292.6939, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 653.8049, Accuracy: 0.8750\n",
      "Batch number: 022, Training: Loss: 501.0218, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 794.1476, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 552.4329, Accuracy: 0.9062\n",
      "Batch number: 025, Training: Loss: 770.4401, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 596.5646, Accuracy: 0.8750\n",
      "Batch number: 027, Training: Loss: 172.0756, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 94.5353, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 432.6639, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 211.1084, Accuracy: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 031, Training: Loss: 170.0997, Accuracy: 0.9688\n",
      "Batch number: 032, Training: Loss: 866.5117, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 573.6143, Accuracy: 0.9062\n",
      "Batch number: 034, Training: Loss: 256.7781, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 267.6545, Accuracy: 0.9688\n",
      "Batch number: 036, Training: Loss: 126.5293, Accuracy: 0.9062\n",
      "Batch number: 037, Training: Loss: 125.4073, Accuracy: 0.9688\n",
      "Batch number: 038, Training: Loss: 352.6059, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 143.5885, Accuracy: 0.9688\n",
      "Batch number: 040, Training: Loss: 193.4692, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 042, Training: Loss: 348.9874, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 92.1084, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 146.6676, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 395.7530, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 464.5202, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 1106.3422, Accuracy: 0.8438\n",
      "Batch number: 048, Training: Loss: 168.7287, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 178.0873, Accuracy: 0.9375\n",
      "Batch number: 050, Training: Loss: 3516.9199, Accuracy: 0.8438\n",
      "Batch number: 051, Training: Loss: 827.5522, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 169.1043, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 1097.9506, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 207.5903, Accuracy: 0.9062\n",
      "Batch number: 055, Training: Loss: 61.3188, Accuracy: 0.9062\n",
      "Batch number: 056, Training: Loss: 217.9460, Accuracy: 0.8438\n",
      "Batch number: 057, Training: Loss: 158.6135, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 183.7891, Accuracy: 0.9062\n",
      "Batch number: 059, Training: Loss: 65.3136, Accuracy: 0.9375\n",
      "Batch number: 060, Training: Loss: 304.3032, Accuracy: 0.9062\n",
      "Batch number: 061, Training: Loss: 79.5834, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 620.1333, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 168.4546, Accuracy: 0.9062\n",
      "Batch number: 064, Training: Loss: 479.9846, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 267.1649, Accuracy: 0.8750\n",
      "Batch number: 066, Training: Loss: 259.3191, Accuracy: 0.9688\n",
      "Batch number: 067, Training: Loss: 360.5186, Accuracy: 0.8750\n",
      "Batch number: 068, Training: Loss: 927.5050, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 220.1109, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 496.9438, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 1006.9354, Accuracy: 0.7188\n",
      "Batch number: 072, Training: Loss: 1599.3959, Accuracy: 0.7500\n",
      "Batch number: 073, Training: Loss: 533.1844, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 151.9474, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 1147.0719, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 188.4322, Accuracy: 0.9375\n",
      "Batch number: 077, Training: Loss: 333.2493, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 660.6369, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 73.2903, Accuracy: 0.9688\n",
      "Batch number: 080, Training: Loss: 101.2533, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 225.9668, Accuracy: 0.9062\n",
      "Batch number: 082, Training: Loss: 416.7996, Accuracy: 0.9062\n",
      "Batch number: 083, Training: Loss: 54.4755, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 30.0941, Accuracy: 0.9688\n",
      "Batch number: 085, Training: Loss: 107.8157, Accuracy: 0.9688\n",
      "Batch number: 086, Training: Loss: 36.1265, Accuracy: 0.9375\n",
      "Batch number: 087, Training: Loss: 135.9658, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 481.3056, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 170.3380, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 233.6591, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 201.3763, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 612.3013, Accuracy: 0.7812\n",
      "Batch number: 094, Training: Loss: 517.1877, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 243.5636, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 129.3796, Accuracy: 0.9688\n",
      "Batch number: 097, Training: Loss: 101.5933, Accuracy: 0.9688\n",
      "Batch number: 098, Training: Loss: 481.4606, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 299.8297, Accuracy: 0.9062\n",
      "Batch number: 100, Training: Loss: 340.2619, Accuracy: 0.9375\n",
      "Batch number: 101, Training: Loss: 235.7492, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 178.7568, Accuracy: 0.9062\n",
      "Batch number: 103, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 1181.3433, Accuracy: 0.8125\n",
      "Batch number: 105, Training: Loss: 99.0968, Accuracy: 0.9062\n",
      "Batch number: 106, Training: Loss: 833.7265, Accuracy: 0.8438\n",
      "Batch number: 107, Training: Loss: 629.5984, Accuracy: 0.8125\n",
      "Batch number: 108, Training: Loss: 766.2135, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 391.6057, Accuracy: 0.9062\n",
      "Batch number: 111, Training: Loss: 763.5481, Accuracy: 0.8125\n",
      "Batch number: 112, Training: Loss: 546.8250, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 225.3905, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 683.1084, Accuracy: 0.9375\n",
      "Batch number: 115, Training: Loss: 533.0926, Accuracy: 0.8438\n",
      "Batch number: 116, Training: Loss: 201.2635, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 333.8036, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 1012.6567, Accuracy: 0.9062\n",
      "Batch number: 119, Training: Loss: 732.3774, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 394.2702, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 219.2350, Accuracy: 0.9062\n",
      "Batch number: 122, Training: Loss: 807.1393, Accuracy: 0.7812\n",
      "Batch number: 123, Training: Loss: 234.6825, Accuracy: 0.9062\n",
      "Batch number: 124, Training: Loss: 102.4156, Accuracy: 0.9688\n",
      "Batch number: 125, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 115.6075, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 631.1823, Accuracy: 0.9688\n",
      "Batch number: 128, Training: Loss: 53.5647, Accuracy: 0.9688\n",
      "Batch number: 129, Training: Loss: 323.3253, Accuracy: 0.9062\n",
      "Batch number: 130, Training: Loss: 236.1658, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 293.4342, Accuracy: 0.8438\n",
      "Batch number: 132, Training: Loss: 89.5448, Accuracy: 0.9688\n",
      "Batch number: 133, Training: Loss: 698.6977, Accuracy: 0.8438\n",
      "Batch number: 134, Training: Loss: 542.9393, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 255.6714, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 148.6623, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 382.0107, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 454.9871, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 3.9120, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 464.8238, Accuracy: 0.8750\n",
      "Batch number: 141, Training: Loss: 272.1773, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 114.2054, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 389.6921, Accuracy: 0.8438\n",
      "Batch number: 144, Training: Loss: 364.6599, Accuracy: 0.9062\n",
      "Batch number: 145, Training: Loss: 117.3092, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 153.4059, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 420.7710, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 244.9093, Accuracy: 0.9688\n",
      "Batch number: 149, Training: Loss: 263.8273, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 285.0859, Accuracy: 0.8438\n",
      "Batch number: 151, Training: Loss: 818.5645, Accuracy: 0.7812\n",
      "Batch number: 152, Training: Loss: 64.5334, Accuracy: 0.9062\n",
      "Batch number: 153, Training: Loss: 320.5955, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 461.0792, Accuracy: 0.8438\n",
      "Batch number: 155, Training: Loss: 304.0802, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 1983.0803, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 260.3282, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 527.3446, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 1143.0465, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 1505.2061, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 307.2927, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 2820.9595, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 1627.8121, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 220.3589, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 1649.1318, Accuracy: 0.5000\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 316.2988, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 493.7752, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 2074.3313, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 167.6432, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 952.6740, Accuracy: 0.5000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 351.7902, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 1104.3995, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 1548.4684, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 1679.5405, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 2125.1304, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 1052.5874, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 1730.8945, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 1143.7849, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 2282.8994, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 1181.7437, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 1089.4421, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 5077.3779, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 1159.6393, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 595.4573, Accuracy: 0.5000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 1446.2463, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 423.4818, Accuracy: 0.7500\n",
      "Validation Batch number: 100, Validation: Loss: 86.7368, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 464.7318, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 1681.0813, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 2357.8374, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 815.0376, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 720.9767, Accuracy: 0.7500\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 2411.8252, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 127, Validation: Loss: 1069.4053, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 1229.3850, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 367.2797, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 2341.0537, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 97.9098, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch : 016, Training: Loss : 408.6831, Accuracy: 90.0721%\n",
      "Validation : Loss : 386.2261, Accuracy: 91.1871%, Time: 53.7543s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 18/25\n",
      "Batch number: 000, Training: Loss: 155.9104, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 74.6070, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 274.3696, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 523.1293, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 634.2051, Accuracy: 0.8438\n",
      "Batch number: 005, Training: Loss: 403.8976, Accuracy: 0.9062\n",
      "Batch number: 006, Training: Loss: 325.6117, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 49.7349, Accuracy: 0.9688\n",
      "Batch number: 008, Training: Loss: 60.5561, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 111.5029, Accuracy: 0.9688\n",
      "Batch number: 010, Training: Loss: 113.8982, Accuracy: 0.9688\n",
      "Batch number: 011, Training: Loss: 912.7716, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 498.4029, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 57.0572, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 238.4308, Accuracy: 0.9062\n",
      "Batch number: 016, Training: Loss: 641.1198, Accuracy: 0.7812\n",
      "Batch number: 017, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 018, Training: Loss: 6.5954, Accuracy: 0.9688\n",
      "Batch number: 019, Training: Loss: 46.9580, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 367.1118, Accuracy: 0.9062\n",
      "Batch number: 021, Training: Loss: 1053.8842, Accuracy: 0.8438\n",
      "Batch number: 022, Training: Loss: 716.3454, Accuracy: 0.8125\n",
      "Batch number: 023, Training: Loss: 280.2643, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 252.8935, Accuracy: 0.9062\n",
      "Batch number: 025, Training: Loss: 185.8773, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 500.1370, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 216.7408, Accuracy: 0.9062\n",
      "Batch number: 029, Training: Loss: 48.5153, Accuracy: 0.9688\n",
      "Batch number: 030, Training: Loss: 208.2337, Accuracy: 0.9688\n",
      "Batch number: 031, Training: Loss: 13.1246, Accuracy: 0.9688\n",
      "Batch number: 032, Training: Loss: 34.2172, Accuracy: 0.9688\n",
      "Batch number: 033, Training: Loss: 172.5032, Accuracy: 0.9375\n",
      "Batch number: 034, Training: Loss: 11.4770, Accuracy: 0.9688\n",
      "Batch number: 035, Training: Loss: 389.1611, Accuracy: 0.9062\n",
      "Batch number: 036, Training: Loss: 487.0554, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 119.1182, Accuracy: 0.9062\n",
      "Batch number: 038, Training: Loss: 186.6949, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 121.0472, Accuracy: 0.9688\n",
      "Batch number: 040, Training: Loss: 119.2558, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 344.2869, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 222.5968, Accuracy: 0.9062\n",
      "Batch number: 043, Training: Loss: 200.0569, Accuracy: 0.8125\n",
      "Batch number: 044, Training: Loss: 18.7899, Accuracy: 0.9688\n",
      "Batch number: 045, Training: Loss: 441.9234, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 220.5682, Accuracy: 0.9062\n",
      "Batch number: 047, Training: Loss: 417.9337, Accuracy: 0.9062\n",
      "Batch number: 048, Training: Loss: 51.7310, Accuracy: 0.9688\n",
      "Batch number: 049, Training: Loss: 176.5328, Accuracy: 0.9688\n",
      "Batch number: 050, Training: Loss: 243.2248, Accuracy: 0.9062\n",
      "Batch number: 051, Training: Loss: 220.8254, Accuracy: 0.9062\n",
      "Batch number: 052, Training: Loss: 354.9591, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 254.7392, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 513.0724, Accuracy: 0.9062\n",
      "Batch number: 055, Training: Loss: 4.1375, Accuracy: 0.9688\n",
      "Batch number: 056, Training: Loss: 145.0928, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 76.6386, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 90.8752, Accuracy: 0.9688\n",
      "Batch number: 059, Training: Loss: 573.7573, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 125.2479, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 69.6359, Accuracy: 0.9062\n",
      "Batch number: 062, Training: Loss: 528.1260, Accuracy: 0.8125\n",
      "Batch number: 063, Training: Loss: 55.8617, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 33.4568, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 066, Training: Loss: 701.4035, Accuracy: 0.9062\n",
      "Batch number: 067, Training: Loss: 88.9636, Accuracy: 0.9062\n",
      "Batch number: 068, Training: Loss: 45.8252, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 140.8990, Accuracy: 0.9062\n",
      "Batch number: 071, Training: Loss: 42.5910, Accuracy: 0.9688\n",
      "Batch number: 072, Training: Loss: 56.1100, Accuracy: 0.9688\n",
      "Batch number: 073, Training: Loss: 321.0429, Accuracy: 0.9062\n",
      "Batch number: 074, Training: Loss: 34.2281, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 408.0553, Accuracy: 0.9062\n",
      "Batch number: 076, Training: Loss: 72.8794, Accuracy: 0.9062\n",
      "Batch number: 077, Training: Loss: 451.4707, Accuracy: 0.8750\n",
      "Batch number: 078, Training: Loss: 172.8942, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 198.2439, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 9.6342, Accuracy: 0.9688\n",
      "Batch number: 081, Training: Loss: 331.3442, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 221.9774, Accuracy: 0.9375\n",
      "Batch number: 083, Training: Loss: 394.4877, Accuracy: 0.7812\n",
      "Batch number: 084, Training: Loss: 185.1283, Accuracy: 0.9688\n",
      "Batch number: 085, Training: Loss: 114.3164, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 88.0567, Accuracy: 0.9688\n",
      "Batch number: 087, Training: Loss: 381.2075, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 63.0675, Accuracy: 0.9062\n",
      "Batch number: 089, Training: Loss: 241.3389, Accuracy: 0.9375\n",
      "Batch number: 090, Training: Loss: 2143.0928, Accuracy: 0.8438\n",
      "Batch number: 091, Training: Loss: 9186.4912, Accuracy: 0.8438\n",
      "Batch number: 092, Training: Loss: 156.2358, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 1296.6039, Accuracy: 0.7812\n",
      "Batch number: 094, Training: Loss: 1143.1046, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 2158.2344, Accuracy: 0.8438\n",
      "Batch number: 096, Training: Loss: 686.4498, Accuracy: 0.8438\n",
      "Batch number: 097, Training: Loss: 268.9945, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 184.6228, Accuracy: 0.9688\n",
      "Batch number: 099, Training: Loss: 359.2481, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 528.2263, Accuracy: 0.9062\n",
      "Batch number: 101, Training: Loss: 427.5340, Accuracy: 0.9062\n",
      "Batch number: 102, Training: Loss: 732.0931, Accuracy: 0.8750\n",
      "Batch number: 103, Training: Loss: 115.6114, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 186.7309, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 629.4302, Accuracy: 0.9062\n",
      "Batch number: 106, Training: Loss: 1019.7701, Accuracy: 0.7500\n",
      "Batch number: 107, Training: Loss: 650.0980, Accuracy: 0.8438\n",
      "Batch number: 108, Training: Loss: 315.2052, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 199.5922, Accuracy: 0.9062\n",
      "Batch number: 110, Training: Loss: 919.9688, Accuracy: 0.9375\n",
      "Batch number: 111, Training: Loss: 414.2720, Accuracy: 0.9062\n",
      "Batch number: 112, Training: Loss: 494.7048, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 501.0665, Accuracy: 0.8750\n",
      "Batch number: 114, Training: Loss: 765.0543, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 31.2085, Accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 116, Training: Loss: 239.7393, Accuracy: 0.9688\n",
      "Batch number: 117, Training: Loss: 450.7392, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 917.2341, Accuracy: 0.9062\n",
      "Batch number: 119, Training: Loss: 563.5560, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 739.1437, Accuracy: 0.8438\n",
      "Batch number: 121, Training: Loss: 812.1274, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 1157.7671, Accuracy: 0.7500\n",
      "Batch number: 123, Training: Loss: 237.0170, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 528.3748, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 102.6367, Accuracy: 0.9062\n",
      "Batch number: 126, Training: Loss: 866.0157, Accuracy: 0.8438\n",
      "Batch number: 127, Training: Loss: 667.0942, Accuracy: 0.8438\n",
      "Batch number: 128, Training: Loss: 369.9674, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 764.0792, Accuracy: 0.7500\n",
      "Batch number: 130, Training: Loss: 481.5551, Accuracy: 0.8750\n",
      "Batch number: 131, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 132, Training: Loss: 1296.9642, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 850.9066, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 477.9138, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 341.7773, Accuracy: 0.9062\n",
      "Batch number: 136, Training: Loss: 594.5585, Accuracy: 0.9062\n",
      "Batch number: 137, Training: Loss: 934.3298, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 558.2166, Accuracy: 0.9062\n",
      "Batch number: 139, Training: Loss: 298.6321, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 277.7583, Accuracy: 0.9062\n",
      "Batch number: 141, Training: Loss: 245.5082, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 306.3230, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 187.8951, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 690.5638, Accuracy: 0.9062\n",
      "Batch number: 145, Training: Loss: 640.9575, Accuracy: 0.9375\n",
      "Batch number: 146, Training: Loss: 975.4469, Accuracy: 0.9062\n",
      "Batch number: 147, Training: Loss: 1539.8990, Accuracy: 0.8125\n",
      "Batch number: 148, Training: Loss: 1649.2491, Accuracy: 0.8438\n",
      "Batch number: 149, Training: Loss: 132.4618, Accuracy: 0.9688\n",
      "Batch number: 150, Training: Loss: 65.5340, Accuracy: 0.9375\n",
      "Batch number: 151, Training: Loss: 652.7001, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 223.6798, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 183.3346, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 804.8253, Accuracy: 0.9062\n",
      "Batch number: 155, Training: Loss: 604.7239, Accuracy: 0.8750\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 678.1143, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 937.0402, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 681.8189, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 2255.1533, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 1696.4302, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 2254.3242, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 1123.3689, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 629.7398, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 1632.4186, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 1391.9070, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 50.0675, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 488.4282, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 2045.0222, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 1513.8019, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 3485.9707, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 2326.8999, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 657.4450, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 717.6895, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 1293.8494, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 3036.3604, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 131.7833, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 2530.5234, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 082, Validation: Loss: 1255.3442, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 424.8423, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 7248.9092, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 1593.2810, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 2159.1689, Accuracy: 0.5000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 552.0632, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0052, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 956.9099, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 1422.2070, Accuracy: 0.7500\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 3494.1958, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 35.6582, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 1461.2136, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 481.2491, Accuracy: 0.7500\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 3308.0439, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 1597.7749, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 683.4553, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 827.4541, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 3897.5894, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 1392.8962, Accuracy: 0.7500\n",
      "Epoch : 017, Training: Loss : 446.4788, Accuracy: 90.7726%\n",
      "Validation : Loss : 462.9526, Accuracy: 91.9065%, Time: 52.5935s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 19/25\n",
      "Batch number: 000, Training: Loss: 751.5067, Accuracy: 0.8125\n",
      "Batch number: 001, Training: Loss: 448.5973, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 339.4707, Accuracy: 0.9062\n",
      "Batch number: 003, Training: Loss: 612.9199, Accuracy: 0.8125\n",
      "Batch number: 004, Training: Loss: 1722.0323, Accuracy: 0.7188\n",
      "Batch number: 005, Training: Loss: 97.1839, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 007, Training: Loss: 223.9516, Accuracy: 0.9062\n",
      "Batch number: 008, Training: Loss: 856.8513, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 335.1833, Accuracy: 0.9062\n",
      "Batch number: 010, Training: Loss: 341.1316, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 386.5547, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 110.6022, Accuracy: 0.9688\n",
      "Batch number: 013, Training: Loss: 355.7909, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 215.0964, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 237.8437, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 630.9560, Accuracy: 0.8438\n",
      "Batch number: 017, Training: Loss: 518.6739, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 101.3254, Accuracy: 0.9688\n",
      "Batch number: 019, Training: Loss: 477.5668, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 183.0571, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 706.1174, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 245.2300, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 864.0887, Accuracy: 0.7812\n",
      "Batch number: 024, Training: Loss: 617.3445, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 670.0652, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 471.6785, Accuracy: 0.9062\n",
      "Batch number: 027, Training: Loss: 167.9234, Accuracy: 0.8750\n",
      "Batch number: 028, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 285.1675, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 177.5188, Accuracy: 0.9688\n",
      "Batch number: 033, Training: Loss: 637.7728, Accuracy: 0.7500\n",
      "Batch number: 034, Training: Loss: 213.5475, Accuracy: 0.9062\n",
      "Batch number: 035, Training: Loss: 211.0830, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 420.6082, Accuracy: 0.9062\n",
      "Batch number: 037, Training: Loss: 157.6854, Accuracy: 0.9688\n",
      "Batch number: 038, Training: Loss: 563.3653, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 294.9136, Accuracy: 0.9688\n",
      "Batch number: 040, Training: Loss: 379.9026, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 626.5930, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 1024.7535, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 403.0803, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 045, Training: Loss: 679.0004, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 1826.5819, Accuracy: 0.6875\n",
      "Batch number: 047, Training: Loss: 2474.2488, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 450.1554, Accuracy: 0.9062\n",
      "Batch number: 049, Training: Loss: 548.8794, Accuracy: 0.9062\n",
      "Batch number: 050, Training: Loss: 927.6476, Accuracy: 0.8125\n",
      "Batch number: 051, Training: Loss: 375.9492, Accuracy: 0.8438\n",
      "Batch number: 052, Training: Loss: 222.2260, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 495.9807, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 218.0574, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 307.6050, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 660.6050, Accuracy: 0.7812\n",
      "Batch number: 057, Training: Loss: 239.4006, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 697.9882, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 722.3289, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 154.5312, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 300.5501, Accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 062, Training: Loss: 942.6192, Accuracy: 0.9062\n",
      "Batch number: 063, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 064, Training: Loss: 354.2641, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 108.8832, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 239.8667, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 269.6312, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 641.6285, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 755.3505, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 15.7186, Accuracy: 0.9688\n",
      "Batch number: 071, Training: Loss: 188.4183, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 648.4901, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 498.5467, Accuracy: 0.7812\n",
      "Batch number: 074, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 426.3407, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 357.3841, Accuracy: 0.8750\n",
      "Batch number: 077, Training: Loss: 132.0924, Accuracy: 0.9688\n",
      "Batch number: 078, Training: Loss: 280.5908, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 100.4548, Accuracy: 0.9688\n",
      "Batch number: 080, Training: Loss: 185.2188, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 684.5361, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 639.0539, Accuracy: 0.9062\n",
      "Batch number: 083, Training: Loss: 521.8674, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 170.3754, Accuracy: 0.9062\n",
      "Batch number: 085, Training: Loss: 299.6113, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 725.3321, Accuracy: 0.7500\n",
      "Batch number: 087, Training: Loss: 160.6339, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 719.3941, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 090, Training: Loss: 95.7268, Accuracy: 0.9688\n",
      "Batch number: 091, Training: Loss: 121.1730, Accuracy: 0.9062\n",
      "Batch number: 092, Training: Loss: 70.2527, Accuracy: 0.9375\n",
      "Batch number: 093, Training: Loss: 500.1042, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 260.7152, Accuracy: 0.9062\n",
      "Batch number: 095, Training: Loss: 301.0174, Accuracy: 0.9062\n",
      "Batch number: 096, Training: Loss: 90.9193, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 365.9191, Accuracy: 0.9062\n",
      "Batch number: 099, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 112.5830, Accuracy: 0.9688\n",
      "Batch number: 101, Training: Loss: 104.6043, Accuracy: 0.9688\n",
      "Batch number: 102, Training: Loss: 31.2567, Accuracy: 0.9688\n",
      "Batch number: 103, Training: Loss: 7.7762, Accuracy: 0.9688\n",
      "Batch number: 104, Training: Loss: 441.6378, Accuracy: 0.9062\n",
      "Batch number: 105, Training: Loss: 169.3449, Accuracy: 0.9062\n",
      "Batch number: 106, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 146.6470, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 8657.4902, Accuracy: 0.8125\n",
      "Batch number: 109, Training: Loss: 24.3779, Accuracy: 0.9688\n",
      "Batch number: 110, Training: Loss: 793.7611, Accuracy: 0.8750\n",
      "Batch number: 111, Training: Loss: 1371.4274, Accuracy: 0.7812\n",
      "Batch number: 112, Training: Loss: 893.6464, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 190.7449, Accuracy: 0.9062\n",
      "Batch number: 114, Training: Loss: 1039.9692, Accuracy: 0.8438\n",
      "Batch number: 115, Training: Loss: 536.9708, Accuracy: 0.8438\n",
      "Batch number: 116, Training: Loss: 430.5716, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 482.7369, Accuracy: 0.8750\n",
      "Batch number: 118, Training: Loss: 1229.7266, Accuracy: 0.8125\n",
      "Batch number: 119, Training: Loss: 915.8002, Accuracy: 0.8125\n",
      "Batch number: 120, Training: Loss: 344.3461, Accuracy: 0.9062\n",
      "Batch number: 121, Training: Loss: 601.0968, Accuracy: 0.8438\n",
      "Batch number: 122, Training: Loss: 334.2761, Accuracy: 0.9062\n",
      "Batch number: 123, Training: Loss: 101.5802, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 1505.4838, Accuracy: 0.7812\n",
      "Batch number: 125, Training: Loss: 109.9555, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 665.1292, Accuracy: 0.8438\n",
      "Batch number: 127, Training: Loss: 780.8438, Accuracy: 0.9062\n",
      "Batch number: 128, Training: Loss: 30.4877, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 1998.9839, Accuracy: 0.6875\n",
      "Batch number: 130, Training: Loss: 571.8016, Accuracy: 0.9062\n",
      "Batch number: 131, Training: Loss: 115.1486, Accuracy: 0.9375\n",
      "Batch number: 132, Training: Loss: 541.6747, Accuracy: 0.8750\n",
      "Batch number: 133, Training: Loss: 281.5291, Accuracy: 0.9688\n",
      "Batch number: 134, Training: Loss: 408.0234, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 151.3204, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 649.6105, Accuracy: 0.8438\n",
      "Batch number: 137, Training: Loss: 535.8203, Accuracy: 0.9062\n",
      "Batch number: 138, Training: Loss: 1393.0298, Accuracy: 0.8438\n",
      "Batch number: 139, Training: Loss: 14.2687, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 594.7274, Accuracy: 0.8438\n",
      "Batch number: 141, Training: Loss: 727.4758, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 537.2659, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 591.5447, Accuracy: 0.9062\n",
      "Batch number: 144, Training: Loss: 3566.9204, Accuracy: 0.7188\n",
      "Batch number: 145, Training: Loss: 366.4386, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 1969.8494, Accuracy: 0.7812\n",
      "Batch number: 147, Training: Loss: 1503.3740, Accuracy: 0.9062\n",
      "Batch number: 148, Training: Loss: 835.9789, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 260.6233, Accuracy: 0.9688\n",
      "Batch number: 150, Training: Loss: 640.5496, Accuracy: 0.8125\n",
      "Batch number: 151, Training: Loss: 2520.4309, Accuracy: 0.7812\n",
      "Batch number: 152, Training: Loss: 104.2262, Accuracy: 0.9062\n",
      "Batch number: 153, Training: Loss: 187.1686, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 788.8129, Accuracy: 0.9062\n",
      "Batch number: 155, Training: Loss: 332.1398, Accuracy: 0.9375\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 1356.2507, Accuracy: 0.5000\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 1811.0553, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 424.9761, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 202.8885, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 2651.8535, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 4512.0781, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 3257.7583, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 1301.9558, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 2561.4756, Accuracy: 0.7500\n",
      "Validation Batch number: 032, Validation: Loss: 339.4682, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 035, Validation: Loss: 1191.4673, Accuracy: 0.7500\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 1243.7544, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 585.8358, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 3231.5444, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 743.8012, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 866.7827, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 1352.6433, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 891.8837, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 1618.3999, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 62.0240, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 1926.6447, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 2057.7141, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 487.1570, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 561.5035, Accuracy: 0.5000\n",
      "Validation Batch number: 084, Validation: Loss: 930.3231, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 5641.1479, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 2440.4839, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 626.2205, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 788.0841, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 2110.1587, Accuracy: 0.7500\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 105.2911, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 2867.5901, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 546.4578, Accuracy: 0.7500\n",
      "Validation Batch number: 116, Validation: Loss: 1478.3994, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 5036.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 1708.5754, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 3087.6030, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 4188.0293, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 223.2460, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 119.6364, Accuracy: 0.7500\n",
      "Epoch : 018, Training: Loss : 547.6484, Accuracy: 89.7118%\n",
      "Validation : Loss : 483.0109, Accuracy: 91.7266%, Time: 52.2312s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 20/25\n",
      "Batch number: 000, Training: Loss: 649.1033, Accuracy: 0.9062\n",
      "Batch number: 001, Training: Loss: 1075.2087, Accuracy: 0.8438\n",
      "Batch number: 002, Training: Loss: 665.5326, Accuracy: 0.9062\n",
      "Batch number: 003, Training: Loss: 721.7396, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 266.4031, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 188.0797, Accuracy: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 006, Training: Loss: 218.6170, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 1422.0974, Accuracy: 0.7188\n",
      "Batch number: 008, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 513.8029, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 655.9670, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 383.3313, Accuracy: 0.8750\n",
      "Batch number: 012, Training: Loss: 210.3274, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 709.7268, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 56.1725, Accuracy: 0.9375\n",
      "Batch number: 015, Training: Loss: 33.0231, Accuracy: 0.9688\n",
      "Batch number: 016, Training: Loss: 1164.3135, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 310.2179, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 747.8157, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 187.1796, Accuracy: 0.8750\n",
      "Batch number: 020, Training: Loss: 522.6301, Accuracy: 0.9062\n",
      "Batch number: 021, Training: Loss: 576.6691, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 112.5430, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 298.7237, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 348.0596, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 389.5519, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 133.6931, Accuracy: 0.9062\n",
      "Batch number: 027, Training: Loss: 110.6637, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 029, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 82.8016, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 464.1498, Accuracy: 0.9062\n",
      "Batch number: 032, Training: Loss: 292.3879, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 594.7915, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 035, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 036, Training: Loss: 479.5098, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 181.5337, Accuracy: 0.9062\n",
      "Batch number: 038, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 039, Training: Loss: 194.0477, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 479.9637, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 8.0649, Accuracy: 0.9688\n",
      "Batch number: 042, Training: Loss: 525.5828, Accuracy: 0.8438\n",
      "Batch number: 043, Training: Loss: 89.3733, Accuracy: 0.9688\n",
      "Batch number: 044, Training: Loss: 376.5777, Accuracy: 0.9062\n",
      "Batch number: 045, Training: Loss: 481.7170, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 877.8301, Accuracy: 0.8438\n",
      "Batch number: 047, Training: Loss: 409.1297, Accuracy: 0.9062\n",
      "Batch number: 048, Training: Loss: 176.1606, Accuracy: 0.9375\n",
      "Batch number: 049, Training: Loss: 675.2316, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 1199.4524, Accuracy: 0.8438\n",
      "Batch number: 051, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 052, Training: Loss: 75.6254, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 114.4658, Accuracy: 0.9688\n",
      "Batch number: 054, Training: Loss: 336.0901, Accuracy: 0.8750\n",
      "Batch number: 055, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 056, Training: Loss: 71.7519, Accuracy: 0.8750\n",
      "Batch number: 057, Training: Loss: 323.8694, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 113.9560, Accuracy: 0.9688\n",
      "Batch number: 059, Training: Loss: 149.9351, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 580.0479, Accuracy: 0.8125\n",
      "Batch number: 061, Training: Loss: 372.7280, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 1266.6733, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 139.3254, Accuracy: 0.9688\n",
      "Batch number: 064, Training: Loss: 115.6491, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 46.0279, Accuracy: 0.9688\n",
      "Batch number: 066, Training: Loss: 114.4740, Accuracy: 0.9688\n",
      "Batch number: 067, Training: Loss: 383.1748, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 817.9327, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 705.7013, Accuracy: 0.8438\n",
      "Batch number: 071, Training: Loss: 1125.4388, Accuracy: 0.7812\n",
      "Batch number: 072, Training: Loss: 542.1964, Accuracy: 0.9688\n",
      "Batch number: 073, Training: Loss: 482.9416, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 457.2088, Accuracy: 0.9062\n",
      "Batch number: 075, Training: Loss: 714.8363, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 1542.1438, Accuracy: 0.7500\n",
      "Batch number: 077, Training: Loss: 212.0231, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 079, Training: Loss: 15.3652, Accuracy: 0.9688\n",
      "Batch number: 080, Training: Loss: 229.5430, Accuracy: 0.9062\n",
      "Batch number: 081, Training: Loss: 1577.5999, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 493.2509, Accuracy: 0.8438\n",
      "Batch number: 083, Training: Loss: 161.0166, Accuracy: 0.9062\n",
      "Batch number: 084, Training: Loss: 44.7392, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 205.3954, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 98.0194, Accuracy: 0.9688\n",
      "Batch number: 087, Training: Loss: 648.8488, Accuracy: 0.9062\n",
      "Batch number: 088, Training: Loss: 496.7976, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 393.3481, Accuracy: 0.8750\n",
      "Batch number: 090, Training: Loss: 34.8626, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 128.7515, Accuracy: 0.9375\n",
      "Batch number: 092, Training: Loss: 105.2464, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 897.8367, Accuracy: 0.8125\n",
      "Batch number: 094, Training: Loss: 97.4006, Accuracy: 0.9062\n",
      "Batch number: 095, Training: Loss: 624.8970, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 1.6352, Accuracy: 0.9688\n",
      "Batch number: 097, Training: Loss: 237.3484, Accuracy: 0.8750\n",
      "Batch number: 098, Training: Loss: 37477.9648, Accuracy: 0.9688\n",
      "Batch number: 099, Training: Loss: 520.4431, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 949.7656, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 462.5036, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 101.3014, Accuracy: 0.9688\n",
      "Batch number: 103, Training: Loss: 346.8095, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 109.1715, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 429.1549, Accuracy: 0.9062\n",
      "Batch number: 106, Training: Loss: 3021.3267, Accuracy: 0.8125\n",
      "Batch number: 107, Training: Loss: 394.3326, Accuracy: 0.8750\n",
      "Batch number: 108, Training: Loss: 426.8335, Accuracy: 0.8125\n",
      "Batch number: 109, Training: Loss: 117.0932, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 450.2894, Accuracy: 0.9375\n",
      "Batch number: 112, Training: Loss: 514.5913, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 139.1331, Accuracy: 0.9062\n",
      "Batch number: 114, Training: Loss: 657.2970, Accuracy: 0.9062\n",
      "Batch number: 115, Training: Loss: 284.0156, Accuracy: 0.9062\n",
      "Batch number: 116, Training: Loss: 489.0582, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 312.5878, Accuracy: 0.8438\n",
      "Batch number: 118, Training: Loss: 408.3972, Accuracy: 0.9375\n",
      "Batch number: 119, Training: Loss: 2912.1926, Accuracy: 0.7188\n",
      "Batch number: 120, Training: Loss: 625.9193, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 127.3907, Accuracy: 0.9688\n",
      "Batch number: 122, Training: Loss: 2058.4600, Accuracy: 0.9062\n",
      "Batch number: 123, Training: Loss: 1499.7504, Accuracy: 0.8438\n",
      "Batch number: 124, Training: Loss: 966.1839, Accuracy: 0.9062\n",
      "Batch number: 125, Training: Loss: 239.4266, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 98.1322, Accuracy: 0.9688\n",
      "Batch number: 127, Training: Loss: 852.8124, Accuracy: 0.8438\n",
      "Batch number: 128, Training: Loss: 128.8650, Accuracy: 0.9375\n",
      "Batch number: 129, Training: Loss: 51903.9023, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 284.4902, Accuracy: 0.9688\n",
      "Batch number: 131, Training: Loss: 1924.0762, Accuracy: 0.7812\n",
      "Batch number: 132, Training: Loss: 150.0943, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 312.0195, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 585.9985, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 136, Training: Loss: 529.1022, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 1554.1301, Accuracy: 0.8125\n",
      "Batch number: 138, Training: Loss: 1085.0227, Accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 139, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 487.3689, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 1109.0909, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 214.1795, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 1846.8853, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 1466.7867, Accuracy: 0.9062\n",
      "Batch number: 145, Training: Loss: 1348.2256, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 726.3729, Accuracy: 0.8438\n",
      "Batch number: 147, Training: Loss: 1448.6782, Accuracy: 0.8125\n",
      "Batch number: 148, Training: Loss: 887.7825, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 770.0676, Accuracy: 0.8125\n",
      "Batch number: 151, Training: Loss: 1613.0234, Accuracy: 0.8750\n",
      "Batch number: 152, Training: Loss: 834.1474, Accuracy: 0.9062\n",
      "Batch number: 153, Training: Loss: 1103.4789, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 415.0493, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 452.2720, Accuracy: 0.9375\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 928.7587, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 868.7393, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 987.0267, Accuracy: 0.7500\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 1476.0172, Accuracy: 0.5000\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 2362.8674, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 2589.5898, Accuracy: 0.7500\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 2970.4438, Accuracy: 0.5000\n",
      "Validation Batch number: 021, Validation: Loss: 2406.4224, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 497.4698, Accuracy: 0.7500\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 2914.0149, Accuracy: 0.7500\n",
      "Validation Batch number: 027, Validation: Loss: 86.3852, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 812.7450, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 696.1536, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 3112.9192, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 1782.0195, Accuracy: 0.7500\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 1460.9153, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 449.6665, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 2607.3730, Accuracy: 0.2500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 7260.6040, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 978.2534, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 775.2339, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 8458.5869, Accuracy: 0.2500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 1796.9073, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 2277.9351, Accuracy: 0.5000\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 1496.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 903.6959, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 7154.9512, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 127.4373, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 1497.8938, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 5679.5586, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 1499.1392, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 1571.7053, Accuracy: 0.2500\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 2776.4072, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 1031.0515, Accuracy: 0.7500\n",
      "Validation Batch number: 099, Validation: Loss: 225.2650, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 100, Validation: Loss: 708.0759, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 331.0117, Accuracy: 0.7500\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 903.6959, Accuracy: 0.7500\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 1483.8940, Accuracy: 0.7500\n",
      "Validation Batch number: 110, Validation: Loss: 278.3820, Accuracy: 0.7500\n",
      "Validation Batch number: 111, Validation: Loss: 12.4157, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 988.9001, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 1546.4822, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 6861.3555, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 181.5783, Accuracy: 0.7500\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 1415.5889, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 220.9265, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 9149.5762, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 12873.5928, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 603.6821, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 903.6959, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 544.6697, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 2161.4636, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch : 019, Training: Loss : 1074.3255, Accuracy: 90.3122%\n",
      "Validation : Loss : 832.2985, Accuracy: 87.7698%, Time: 52.7497s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 21/25\n",
      "Batch number: 000, Training: Loss: 313.9909, Accuracy: 0.9062\n",
      "Batch number: 001, Training: Loss: 327.8203, Accuracy: 0.9375\n",
      "Batch number: 002, Training: Loss: 1441.5342, Accuracy: 0.9062\n",
      "Batch number: 003, Training: Loss: 2454.9863, Accuracy: 0.8438\n",
      "Batch number: 004, Training: Loss: 1304.9042, Accuracy: 0.8750\n",
      "Batch number: 005, Training: Loss: 1610.2726, Accuracy: 0.9062\n",
      "Batch number: 006, Training: Loss: 642.4755, Accuracy: 0.8750\n",
      "Batch number: 007, Training: Loss: 1041.6824, Accuracy: 0.8750\n",
      "Batch number: 008, Training: Loss: 632.0872, Accuracy: 0.8438\n",
      "Batch number: 009, Training: Loss: 805.6685, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 630.3493, Accuracy: 0.9062\n",
      "Batch number: 011, Training: Loss: 224.4182, Accuracy: 0.9688\n",
      "Batch number: 012, Training: Loss: 441.8608, Accuracy: 0.9062\n",
      "Batch number: 013, Training: Loss: 994.4218, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 914.9040, Accuracy: 0.8750\n",
      "Batch number: 015, Training: Loss: 419.5616, Accuracy: 0.9062\n",
      "Batch number: 016, Training: Loss: 114.0923, Accuracy: 0.9688\n",
      "Batch number: 017, Training: Loss: 139.8483, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 1725.7135, Accuracy: 0.8125\n",
      "Batch number: 019, Training: Loss: 1473.7258, Accuracy: 0.7812\n",
      "Batch number: 020, Training: Loss: 61.8958, Accuracy: 0.9688\n",
      "Batch number: 021, Training: Loss: 326.7744, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 298.0485, Accuracy: 0.9062\n",
      "Batch number: 023, Training: Loss: 221.4131, Accuracy: 0.9062\n",
      "Batch number: 024, Training: Loss: 1278.1558, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 622.5676, Accuracy: 0.8438\n",
      "Batch number: 026, Training: Loss: 413.5469, Accuracy: 0.9062\n",
      "Batch number: 027, Training: Loss: 738.7441, Accuracy: 0.9062\n",
      "Batch number: 028, Training: Loss: 1308.9563, Accuracy: 0.9062\n",
      "Batch number: 029, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 030, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 895.5560, Accuracy: 0.9062\n",
      "Batch number: 032, Training: Loss: 7507.3311, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 4195.6875, Accuracy: 0.8750\n",
      "Batch number: 034, Training: Loss: 912.5748, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 422.4658, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 2032.5488, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 274.5819, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 1048.3484, Accuracy: 0.9375\n",
      "Batch number: 039, Training: Loss: 1042.4908, Accuracy: 0.8438\n",
      "Batch number: 040, Training: Loss: 602.7954, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 44.8397, Accuracy: 0.9688\n",
      "Batch number: 042, Training: Loss: 397.9839, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 225.3313, Accuracy: 0.9062\n",
      "Batch number: 044, Training: Loss: 421.0659, Accuracy: 0.9688\n",
      "Batch number: 045, Training: Loss: 1370.4160, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 229.6317, Accuracy: 0.9375\n",
      "Batch number: 047, Training: Loss: 1618.6901, Accuracy: 0.7812\n",
      "Batch number: 048, Training: Loss: 1802.0918, Accuracy: 0.7812\n",
      "Batch number: 049, Training: Loss: 1094.4866, Accuracy: 0.8438\n",
      "Batch number: 050, Training: Loss: 795.0133, Accuracy: 0.9062\n",
      "Batch number: 051, Training: Loss: 780.3917, Accuracy: 0.9062\n",
      "Batch number: 052, Training: Loss: 713.2203, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 247.7959, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 534.3298, Accuracy: 0.8438\n",
      "Batch number: 055, Training: Loss: 816.8661, Accuracy: 0.9062\n",
      "Batch number: 056, Training: Loss: 255.1636, Accuracy: 0.9375\n",
      "Batch number: 057, Training: Loss: 454.7250, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 593.6179, Accuracy: 0.8750\n",
      "Batch number: 059, Training: Loss: 1149.9602, Accuracy: 0.7812\n",
      "Batch number: 060, Training: Loss: 118.5326, Accuracy: 0.9688\n",
      "Batch number: 061, Training: Loss: 316.9295, Accuracy: 0.9375\n",
      "Batch number: 062, Training: Loss: 1381.9283, Accuracy: 0.7812\n",
      "Batch number: 063, Training: Loss: 380.8324, Accuracy: 0.9688\n",
      "Batch number: 064, Training: Loss: 2264.4343, Accuracy: 0.7812\n",
      "Batch number: 065, Training: Loss: 529.0805, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 236.0826, Accuracy: 0.9375\n",
      "Batch number: 067, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 068, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 260.7365, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 298.5737, Accuracy: 0.9062\n",
      "Batch number: 071, Training: Loss: 827.4179, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 1407.8210, Accuracy: 0.8438\n",
      "Batch number: 073, Training: Loss: 353.5370, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 226.9294, Accuracy: 0.9375\n",
      "Batch number: 075, Training: Loss: 657.0202, Accuracy: 0.9062\n",
      "Batch number: 076, Training: Loss: 554.5583, Accuracy: 0.9062\n",
      "Batch number: 077, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 078, Training: Loss: 721.8856, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 146.2844, Accuracy: 0.9688\n",
      "Batch number: 080, Training: Loss: 541.2660, Accuracy: 0.9375\n",
      "Batch number: 081, Training: Loss: 1810.5215, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 814.3276, Accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 083, Training: Loss: 814.7430, Accuracy: 0.8438\n",
      "Batch number: 084, Training: Loss: 1201.7908, Accuracy: 0.8438\n",
      "Batch number: 085, Training: Loss: 418.2386, Accuracy: 0.9375\n",
      "Batch number: 086, Training: Loss: 841.6611, Accuracy: 0.8125\n",
      "Batch number: 087, Training: Loss: 25.5200, Accuracy: 0.9688\n",
      "Batch number: 088, Training: Loss: 641.8154, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 42.8073, Accuracy: 0.9688\n",
      "Batch number: 090, Training: Loss: 526.7615, Accuracy: 0.9062\n",
      "Batch number: 091, Training: Loss: 1009.1237, Accuracy: 0.8438\n",
      "Batch number: 092, Training: Loss: 1.0047, Accuracy: 0.9688\n",
      "Batch number: 093, Training: Loss: 884.3159, Accuracy: 0.8125\n",
      "Batch number: 094, Training: Loss: 407.3969, Accuracy: 0.9062\n",
      "Batch number: 095, Training: Loss: 41.9545, Accuracy: 0.9375\n",
      "Batch number: 096, Training: Loss: 7864.1860, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 6.0123, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 9290.2988, Accuracy: 0.8750\n",
      "Batch number: 099, Training: Loss: 882.6069, Accuracy: 0.8750\n",
      "Batch number: 100, Training: Loss: 616.5717, Accuracy: 0.8438\n",
      "Batch number: 101, Training: Loss: 558.1780, Accuracy: 0.9375\n",
      "Batch number: 102, Training: Loss: 504.0226, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 104, Training: Loss: 798.8015, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 37.5365, Accuracy: 0.9688\n",
      "Batch number: 106, Training: Loss: 72570.4688, Accuracy: 0.7500\n",
      "Batch number: 107, Training: Loss: 1859.1224, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 1707.8257, Accuracy: 0.9062\n",
      "Batch number: 109, Training: Loss: 4432.7178, Accuracy: 0.7812\n",
      "Batch number: 110, Training: Loss: 10314.1973, Accuracy: 0.7188\n",
      "Batch number: 111, Training: Loss: 5023.6313, Accuracy: 0.7812\n",
      "Batch number: 112, Training: Loss: 769.4890, Accuracy: 0.8438\n",
      "Batch number: 113, Training: Loss: 2565.4875, Accuracy: 0.7188\n",
      "Batch number: 114, Training: Loss: 2616.5796, Accuracy: 0.7500\n",
      "Batch number: 115, Training: Loss: 6031.9478, Accuracy: 0.7188\n",
      "Batch number: 116, Training: Loss: 840.9328, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 1653.6816, Accuracy: 0.8125\n",
      "Batch number: 118, Training: Loss: 4180.4131, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 3751.7207, Accuracy: 0.9062\n",
      "Batch number: 120, Training: Loss: 1814.2554, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 2193.3611, Accuracy: 0.8125\n",
      "Batch number: 122, Training: Loss: 2297.9937, Accuracy: 0.7500\n",
      "Batch number: 123, Training: Loss: 1087.5934, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 4091.8757, Accuracy: 0.8438\n",
      "Batch number: 125, Training: Loss: 47.7276, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 1479.3954, Accuracy: 0.9062\n",
      "Batch number: 127, Training: Loss: 1156.9048, Accuracy: 0.9062\n",
      "Batch number: 128, Training: Loss: 1345.5675, Accuracy: 0.7812\n",
      "Batch number: 129, Training: Loss: 697.9918, Accuracy: 0.8438\n",
      "Batch number: 130, Training: Loss: 447.7076, Accuracy: 0.9062\n",
      "Batch number: 131, Training: Loss: 1081.4263, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 4734.4639, Accuracy: 0.7812\n",
      "Batch number: 133, Training: Loss: 2960.9116, Accuracy: 0.8750\n",
      "Batch number: 134, Training: Loss: 867.9865, Accuracy: 0.9688\n",
      "Batch number: 135, Training: Loss: 2110.4631, Accuracy: 0.8438\n",
      "Batch number: 136, Training: Loss: 2065.7878, Accuracy: 0.8438\n",
      "Batch number: 137, Training: Loss: 3367.4058, Accuracy: 0.7188\n",
      "Batch number: 138, Training: Loss: 2110.5996, Accuracy: 0.8750\n",
      "Batch number: 139, Training: Loss: 906.9592, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 733.8462, Accuracy: 0.9062\n",
      "Batch number: 141, Training: Loss: 533.7086, Accuracy: 0.9062\n",
      "Batch number: 142, Training: Loss: 1271.7366, Accuracy: 0.8125\n",
      "Batch number: 143, Training: Loss: 769.2919, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 1133.8242, Accuracy: 0.9062\n",
      "Batch number: 145, Training: Loss: 1312.7311, Accuracy: 0.8438\n",
      "Batch number: 146, Training: Loss: 197.9307, Accuracy: 0.9688\n",
      "Batch number: 147, Training: Loss: 1822.9385, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 3648.5505, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 150, Training: Loss: 1265.2079, Accuracy: 0.9062\n",
      "Batch number: 151, Training: Loss: 84.8890, Accuracy: 0.9688\n",
      "Batch number: 152, Training: Loss: 1299.6360, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 3425.2080, Accuracy: 0.8438\n",
      "Batch number: 154, Training: Loss: 1007.1603, Accuracy: 0.9375\n",
      "Batch number: 155, Training: Loss: 998.9723, Accuracy: 0.8750\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 5144.8442, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 1710.9659, Accuracy: 0.7500\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 1051.2573, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 5140.2842, Accuracy: 0.5000\n",
      "Validation Batch number: 021, Validation: Loss: 5280.8765, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 221.4798, Accuracy: 0.7500\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 3350.2488, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 3485.5581, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 2602.3240, Accuracy: 0.5000\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 1273.2200, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 2407.3379, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 2017.6294, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 62.5771, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 919.8002, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 2472.2292, Accuracy: 0.5000\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 4315.8428, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 1870.7189, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 210.7719, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 4695.5039, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 211.1267, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 3318.1665, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 736.3007, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 4172.4707, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 210.7719, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 6031.8555, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 586.6016, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 2345.9639, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 10005.3262, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 4297.9766, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 210.7719, Accuracy: 0.7500\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 652.3618, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 384.4048, Accuracy: 0.7500\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 322.7226, Accuracy: 0.7500\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 5550.9805, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 1354.2723, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 55.4304, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 527.5256, Accuracy: 0.7500\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 8480.3730, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 2781.4609, Accuracy: 0.5000\n",
      "Validation Batch number: 128, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 8261.1357, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 11728.3340, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 210.7719, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 3383.3022, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 1242.3293, Accuracy: 0.7500\n",
      "Epoch : 020, Training: Loss : 1738.0861, Accuracy: 88.6309%\n",
      "Validation : Loss : 901.4116, Accuracy: 90.2878%, Time: 52.2119s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 22/25\n",
      "Batch number: 000, Training: Loss: 658.3804, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 1413.9253, Accuracy: 0.9062\n",
      "Batch number: 002, Training: Loss: 2930.7979, Accuracy: 0.8438\n",
      "Batch number: 003, Training: Loss: 318.7907, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 640.0360, Accuracy: 0.9375\n",
      "Batch number: 005, Training: Loss: 1879.8451, Accuracy: 0.9062\n",
      "Batch number: 006, Training: Loss: 602.3710, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 1086.9530, Accuracy: 0.8438\n",
      "Batch number: 008, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 009, Training: Loss: 541.0135, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 189.1028, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 767.1689, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 1755.5813, Accuracy: 0.9062\n",
      "Batch number: 013, Training: Loss: 1666.4022, Accuracy: 0.9062\n",
      "Batch number: 014, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 016, Training: Loss: 1154.5525, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 83.3623, Accuracy: 0.9375\n",
      "Batch number: 018, Training: Loss: 1037.9974, Accuracy: 0.9062\n",
      "Batch number: 019, Training: Loss: 993.8088, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 183.2657, Accuracy: 0.9688\n",
      "Batch number: 021, Training: Loss: 2220.7319, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 364.1092, Accuracy: 0.9062\n",
      "Batch number: 023, Training: Loss: 326.2212, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 702.1005, Accuracy: 0.9062\n",
      "Batch number: 025, Training: Loss: 1238.0610, Accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 026, Training: Loss: 3452.5405, Accuracy: 0.7500\n",
      "Batch number: 027, Training: Loss: 1138.2550, Accuracy: 0.8438\n",
      "Batch number: 028, Training: Loss: 161.2383, Accuracy: 0.9688\n",
      "Batch number: 029, Training: Loss: 320.9048, Accuracy: 0.9688\n",
      "Batch number: 030, Training: Loss: 1623.9270, Accuracy: 0.9062\n",
      "Batch number: 031, Training: Loss: 37897.9492, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 726.5247, Accuracy: 0.9375\n",
      "Batch number: 033, Training: Loss: 1445.0040, Accuracy: 0.7812\n",
      "Batch number: 034, Training: Loss: 3956.4390, Accuracy: 0.8438\n",
      "Batch number: 035, Training: Loss: 2214.0112, Accuracy: 0.8438\n",
      "Batch number: 036, Training: Loss: 2873.8730, Accuracy: 0.7812\n",
      "Batch number: 037, Training: Loss: 1286.0057, Accuracy: 0.9062\n",
      "Batch number: 038, Training: Loss: 1412.1578, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 1628.1036, Accuracy: 0.8438\n",
      "Batch number: 040, Training: Loss: 1240.2087, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 3525.1006, Accuracy: 0.8750\n",
      "Batch number: 042, Training: Loss: 1165.9048, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 2181.7622, Accuracy: 0.8438\n",
      "Batch number: 044, Training: Loss: 3193.3337, Accuracy: 0.8125\n",
      "Batch number: 045, Training: Loss: 4434.6279, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 3173.2502, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 2095.7578, Accuracy: 0.9062\n",
      "Batch number: 048, Training: Loss: 1974.8899, Accuracy: 0.9062\n",
      "Batch number: 049, Training: Loss: 1984.9905, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 1426.3495, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 2050.0303, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 41.3154, Accuracy: 0.9688\n",
      "Batch number: 053, Training: Loss: 330.7986, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 3331.0381, Accuracy: 0.8438\n",
      "Batch number: 055, Training: Loss: 427.1204, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 3128.4294, Accuracy: 0.6250\n",
      "Batch number: 057, Training: Loss: 1587.9834, Accuracy: 0.8438\n",
      "Batch number: 058, Training: Loss: 2029.9364, Accuracy: 0.8438\n",
      "Batch number: 059, Training: Loss: 1435.8022, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 2622.7278, Accuracy: 0.7188\n",
      "Batch number: 061, Training: Loss: 8922.2617, Accuracy: 0.7500\n",
      "Batch number: 062, Training: Loss: 1205.6111, Accuracy: 0.9375\n",
      "Batch number: 063, Training: Loss: 1751.3018, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 4079.1343, Accuracy: 0.8438\n",
      "Batch number: 065, Training: Loss: 4040.0034, Accuracy: 0.8438\n",
      "Batch number: 066, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 487.2077, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 441.9585, Accuracy: 0.9062\n",
      "Batch number: 069, Training: Loss: 1863.4645, Accuracy: 0.7812\n",
      "Batch number: 070, Training: Loss: 3029.5378, Accuracy: 0.8750\n",
      "Batch number: 071, Training: Loss: 2199.6582, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 1530.1694, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 1207.4219, Accuracy: 0.9062\n",
      "Batch number: 074, Training: Loss: 41.4260, Accuracy: 0.9688\n",
      "Batch number: 075, Training: Loss: 1972.9099, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 1154.5481, Accuracy: 0.9062\n",
      "Batch number: 077, Training: Loss: 211.9485, Accuracy: 0.9688\n",
      "Batch number: 078, Training: Loss: 2023.9553, Accuracy: 0.8125\n",
      "Batch number: 079, Training: Loss: 623.6334, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 640.9890, Accuracy: 0.9688\n",
      "Batch number: 081, Training: Loss: 1959.1182, Accuracy: 0.8750\n",
      "Batch number: 082, Training: Loss: 1329.3870, Accuracy: 0.8438\n",
      "Batch number: 083, Training: Loss: 1234.6340, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 562.5721, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 782.1711, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 2016.6306, Accuracy: 0.7812\n",
      "Batch number: 087, Training: Loss: 2399.1873, Accuracy: 0.8750\n",
      "Batch number: 088, Training: Loss: 937.0977, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 1527.6488, Accuracy: 0.9688\n",
      "Batch number: 090, Training: Loss: 2611.8052, Accuracy: 0.7812\n",
      "Batch number: 091, Training: Loss: 4036.0291, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 1806.1536, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 770.8702, Accuracy: 0.8438\n",
      "Batch number: 094, Training: Loss: 2130.3088, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 4876.2319, Accuracy: 0.7812\n",
      "Batch number: 096, Training: Loss: 1497.5276, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 259.7477, Accuracy: 0.9375\n",
      "Batch number: 098, Training: Loss: 587.8456, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 1847.4580, Accuracy: 0.8125\n",
      "Batch number: 100, Training: Loss: 1816.6538, Accuracy: 0.7812\n",
      "Batch number: 101, Training: Loss: 584.1114, Accuracy: 0.9688\n",
      "Batch number: 102, Training: Loss: 726.1808, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 3100.8489, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 1122.3218, Accuracy: 0.9062\n",
      "Batch number: 105, Training: Loss: 1261.6436, Accuracy: 0.8438\n",
      "Batch number: 106, Training: Loss: 984.4604, Accuracy: 0.9688\n",
      "Batch number: 107, Training: Loss: 1.8878, Accuracy: 0.9688\n",
      "Batch number: 108, Training: Loss: 1288.9597, Accuracy: 0.9375\n",
      "Batch number: 109, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 111, Training: Loss: 616.8311, Accuracy: 0.9688\n",
      "Batch number: 112, Training: Loss: 1602.6721, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 114, Training: Loss: 338.3295, Accuracy: 0.9688\n",
      "Batch number: 115, Training: Loss: 1743.9995, Accuracy: 0.9062\n",
      "Batch number: 116, Training: Loss: 342.5706, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 950.3959, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 381.2798, Accuracy: 0.9688\n",
      "Batch number: 119, Training: Loss: 3466.0400, Accuracy: 0.8438\n",
      "Batch number: 120, Training: Loss: 1469.1978, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 1532.0087, Accuracy: 0.9062\n",
      "Batch number: 122, Training: Loss: 1299.2838, Accuracy: 0.9062\n",
      "Batch number: 123, Training: Loss: 797.3494, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 2620.5654, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 788.0572, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 285.5943, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 1125.6935, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 5059.0444, Accuracy: 0.7812\n",
      "Batch number: 130, Training: Loss: 941.7394, Accuracy: 0.8438\n",
      "Batch number: 131, Training: Loss: 491.3494, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 452.4989, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 2817.1843, Accuracy: 0.8438\n",
      "Batch number: 134, Training: Loss: 497.8578, Accuracy: 0.9375\n",
      "Batch number: 135, Training: Loss: 323.1505, Accuracy: 0.9375\n",
      "Batch number: 136, Training: Loss: 1041.1013, Accuracy: 0.9062\n",
      "Batch number: 137, Training: Loss: 1573.6670, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 195.7767, Accuracy: 0.9688\n",
      "Batch number: 139, Training: Loss: 515.9982, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 3962.8237, Accuracy: 0.7188\n",
      "Batch number: 141, Training: Loss: 1662.6083, Accuracy: 0.8750\n",
      "Batch number: 142, Training: Loss: 205.9532, Accuracy: 0.9688\n",
      "Batch number: 143, Training: Loss: 978.3859, Accuracy: 0.8750\n",
      "Batch number: 144, Training: Loss: 2594.3582, Accuracy: 0.8750\n",
      "Batch number: 145, Training: Loss: 250.1957, Accuracy: 0.9688\n",
      "Batch number: 146, Training: Loss: 1684.5170, Accuracy: 0.8438\n",
      "Batch number: 147, Training: Loss: 881.8463, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 559.6676, Accuracy: 0.9375\n",
      "Batch number: 149, Training: Loss: 420.8685, Accuracy: 0.9062\n",
      "Batch number: 150, Training: Loss: 503.3973, Accuracy: 0.8125\n",
      "Batch number: 151, Training: Loss: 3529.4329, Accuracy: 0.7188\n",
      "Batch number: 152, Training: Loss: 346.4383, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 1076.4891, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 1288.1711, Accuracy: 0.9062\n",
      "Batch number: 155, Training: Loss: 469.8302, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 3493.4265, Accuracy: 0.7500\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 5588.7148, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 4099.4922, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 2047.5527, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 2917.4211, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 1138.1335, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 8231.1543, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 5516.5303, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 3150.7876, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 10211.9609, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 6553.6494, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 2656.0176, Accuracy: 0.7500\n",
      "Validation Batch number: 032, Validation: Loss: 5886.3325, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 2584.8813, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 4762.6719, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 3518.4043, Accuracy: 0.5000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 1554.3752, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 3483.9722, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 7628.3428, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 9025.1562, Accuracy: 0.5000\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 1515.3245, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 5446.9756, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 7462.3428, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 4698.8618, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 7938.0244, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 4628.7217, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 2705.8752, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 6102.7129, Accuracy: 0.5000\n",
      "Validation Batch number: 085, Validation: Loss: 19272.2520, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 4044.7100, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 2585.0161, Accuracy: 0.2500\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 343.2957, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 2723.9705, Accuracy: 0.7500\n",
      "Validation Batch number: 100, Validation: Loss: 1511.2112, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 571.1317, Accuracy: 0.7500\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 8218.1445, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 5683.7383, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 1819.1499, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 6650.3027, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 6426.9844, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 2750.2537, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 3413.1155, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 6066.1162, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 10156.5479, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch : 021, Training: Loss : 1674.5795, Accuracy: 88.7910%\n",
      "Validation : Loss : 1559.5954, Accuracy: 89.9281%, Time: 52.5963s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 23/25\n",
      "Batch number: 000, Training: Loss: 602.8489, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 5621.2070, Accuracy: 0.7188\n",
      "Batch number: 002, Training: Loss: 3081.9128, Accuracy: 0.8438\n",
      "Batch number: 003, Training: Loss: 2056.3984, Accuracy: 0.8750\n",
      "Batch number: 004, Training: Loss: 1316.6919, Accuracy: 0.8125\n",
      "Batch number: 005, Training: Loss: 988.6534, Accuracy: 0.9062\n",
      "Batch number: 006, Training: Loss: 193.8351, Accuracy: 0.9688\n",
      "Batch number: 007, Training: Loss: 2922.4158, Accuracy: 0.7188\n",
      "Batch number: 008, Training: Loss: 530.0145, Accuracy: 0.9062\n",
      "Batch number: 009, Training: Loss: 274.9041, Accuracy: 0.9375\n",
      "Batch number: 010, Training: Loss: 667.1758, Accuracy: 0.9062\n",
      "Batch number: 011, Training: Loss: 99.6757, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 1158.5723, Accuracy: 0.9375\n",
      "Batch number: 013, Training: Loss: 1464.5537, Accuracy: 0.9062\n",
      "Batch number: 014, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 5029.7725, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 3564.4285, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 748.9323, Accuracy: 0.9062\n",
      "Batch number: 018, Training: Loss: 131.5921, Accuracy: 0.9688\n",
      "Batch number: 019, Training: Loss: 1639.1145, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 612.8643, Accuracy: 0.9375\n",
      "Batch number: 021, Training: Loss: 1035.7048, Accuracy: 0.9062\n",
      "Batch number: 022, Training: Loss: 619.8667, Accuracy: 0.9375\n",
      "Batch number: 023, Training: Loss: 267.0356, Accuracy: 0.8750\n",
      "Batch number: 024, Training: Loss: 635.7556, Accuracy: 0.9688\n",
      "Batch number: 025, Training: Loss: 1309.7483, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 676.9003, Accuracy: 0.9062\n",
      "Batch number: 027, Training: Loss: 726.2313, Accuracy: 0.9062\n",
      "Batch number: 028, Training: Loss: 969.6599, Accuracy: 0.9062\n",
      "Batch number: 029, Training: Loss: 112.3482, Accuracy: 0.9375\n",
      "Batch number: 030, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 031, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 032, Training: Loss: 587.0022, Accuracy: 0.9062\n",
      "Batch number: 033, Training: Loss: 419.5653, Accuracy: 0.9062\n",
      "Batch number: 034, Training: Loss: 2014.3081, Accuracy: 0.7812\n",
      "Batch number: 035, Training: Loss: 26.8826, Accuracy: 0.9688\n",
      "Batch number: 036, Training: Loss: 678.7385, Accuracy: 0.9062\n",
      "Batch number: 037, Training: Loss: 44.3680, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 1160.6060, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 1212.7787, Accuracy: 0.8750\n",
      "Batch number: 040, Training: Loss: 466.7537, Accuracy: 0.9375\n",
      "Batch number: 041, Training: Loss: 125.3457, Accuracy: 0.9688\n",
      "Batch number: 042, Training: Loss: 359.7110, Accuracy: 0.9688\n",
      "Batch number: 043, Training: Loss: 1146.6691, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 628.3751, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 500.3253, Accuracy: 0.9375\n",
      "Batch number: 046, Training: Loss: 1955.7970, Accuracy: 0.8125\n",
      "Batch number: 047, Training: Loss: 971.0399, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 2044.1471, Accuracy: 0.9062\n",
      "Batch number: 049, Training: Loss: 1349.8998, Accuracy: 0.9062\n",
      "Batch number: 050, Training: Loss: 1466.2405, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 494.7442, Accuracy: 0.9375\n",
      "Batch number: 052, Training: Loss: 324.5594, Accuracy: 0.9688\n",
      "Batch number: 053, Training: Loss: 1023.7192, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 246.4463, Accuracy: 0.9375\n",
      "Batch number: 055, Training: Loss: 820.1876, Accuracy: 0.7812\n",
      "Batch number: 056, Training: Loss: 1324.9489, Accuracy: 0.7812\n",
      "Batch number: 057, Training: Loss: 53.8452, Accuracy: 0.9688\n",
      "Batch number: 058, Training: Loss: 573.3402, Accuracy: 0.9062\n",
      "Batch number: 059, Training: Loss: 290.5027, Accuracy: 0.9062\n",
      "Batch number: 060, Training: Loss: 820.3700, Accuracy: 0.9375\n",
      "Batch number: 061, Training: Loss: 282.6256, Accuracy: 0.9688\n",
      "Batch number: 062, Training: Loss: 1104.2858, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 260.3523, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 980.5425, Accuracy: 0.9375\n",
      "Batch number: 065, Training: Loss: 329.3236, Accuracy: 0.9375\n",
      "Batch number: 066, Training: Loss: 175.7636, Accuracy: 0.9688\n",
      "Batch number: 067, Training: Loss: 547.8569, Accuracy: 0.9688\n",
      "Batch number: 068, Training: Loss: 1894.8463, Accuracy: 0.9375\n",
      "Batch number: 069, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 070, Training: Loss: 1369.2772, Accuracy: 0.8125\n",
      "Batch number: 071, Training: Loss: 1580.5970, Accuracy: 0.9062\n",
      "Batch number: 072, Training: Loss: 360.8345, Accuracy: 0.9375\n",
      "Batch number: 073, Training: Loss: 742.0906, Accuracy: 0.9062\n",
      "Batch number: 074, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 2098.6162, Accuracy: 0.8438\n",
      "Batch number: 076, Training: Loss: 2637.8340, Accuracy: 0.8125\n",
      "Batch number: 077, Training: Loss: 254.2442, Accuracy: 0.9062\n",
      "Batch number: 078, Training: Loss: 1962.4128, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 080, Training: Loss: 20.4406, Accuracy: 0.9688\n",
      "Batch number: 081, Training: Loss: 1954.7501, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 1012.1933, Accuracy: 0.8438\n",
      "Batch number: 083, Training: Loss: 1153.8381, Accuracy: 0.8125\n",
      "Batch number: 084, Training: Loss: 1369.7423, Accuracy: 0.8125\n",
      "Batch number: 085, Training: Loss: 432.6574, Accuracy: 0.9062\n",
      "Batch number: 086, Training: Loss: 192.7098, Accuracy: 0.9062\n",
      "Batch number: 087, Training: Loss: 467.8191, Accuracy: 0.8438\n",
      "Batch number: 088, Training: Loss: 398.7223, Accuracy: 0.9688\n",
      "Batch number: 089, Training: Loss: 673.4841, Accuracy: 0.9688\n",
      "Batch number: 090, Training: Loss: 8.0174, Accuracy: 0.9688\n",
      "Batch number: 091, Training: Loss: 556.3361, Accuracy: 0.9062\n",
      "Batch number: 092, Training: Loss: 731.7187, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 818.4117, Accuracy: 0.9062\n",
      "Batch number: 094, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 095, Training: Loss: 181.9776, Accuracy: 0.9688\n",
      "Batch number: 096, Training: Loss: 193.8282, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 159.6058, Accuracy: 0.9688\n",
      "Batch number: 098, Training: Loss: 196.1391, Accuracy: 0.9688\n",
      "Batch number: 099, Training: Loss: 237.6073, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 805.7668, Accuracy: 0.9062\n",
      "Batch number: 101, Training: Loss: 163.2587, Accuracy: 0.9688\n",
      "Batch number: 102, Training: Loss: 943.8832, Accuracy: 0.9062\n",
      "Batch number: 103, Training: Loss: 605.0979, Accuracy: 0.9062\n",
      "Batch number: 104, Training: Loss: 793.8160, Accuracy: 0.8750\n",
      "Batch number: 105, Training: Loss: 2038.0386, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 830.5443, Accuracy: 0.9062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 107, Training: Loss: 103.3768, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 1853.9941, Accuracy: 0.8750\n",
      "Batch number: 109, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 5.8524, Accuracy: 0.9688\n",
      "Batch number: 111, Training: Loss: 3395.7566, Accuracy: 0.7500\n",
      "Batch number: 112, Training: Loss: 2357.3496, Accuracy: 0.8750\n",
      "Batch number: 113, Training: Loss: 16.8869, Accuracy: 0.9688\n",
      "Batch number: 114, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 115, Training: Loss: 17953.1328, Accuracy: 0.8438\n",
      "Batch number: 116, Training: Loss: 475.5752, Accuracy: 0.8750\n",
      "Batch number: 117, Training: Loss: 202.2314, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 1365.5906, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 91.0419, Accuracy: 0.9688\n",
      "Batch number: 120, Training: Loss: 177.7306, Accuracy: 0.9688\n",
      "Batch number: 121, Training: Loss: 1588.7354, Accuracy: 0.7812\n",
      "Batch number: 122, Training: Loss: 537.2804, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 540.1599, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 1543.3835, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 416.2184, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 1218.7531, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 2074.6804, Accuracy: 0.9062\n",
      "Batch number: 128, Training: Loss: 2020.5962, Accuracy: 0.8125\n",
      "Batch number: 129, Training: Loss: 9715.5381, Accuracy: 0.9062\n",
      "Batch number: 130, Training: Loss: 3292.9214, Accuracy: 0.8438\n",
      "Batch number: 131, Training: Loss: 43.4665, Accuracy: 0.9688\n",
      "Batch number: 132, Training: Loss: 286.8999, Accuracy: 0.9375\n",
      "Batch number: 133, Training: Loss: 636.1359, Accuracy: 0.9375\n",
      "Batch number: 134, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 2581.6060, Accuracy: 0.8750\n",
      "Batch number: 136, Training: Loss: 593.4096, Accuracy: 0.9375\n",
      "Batch number: 137, Training: Loss: 1032.4850, Accuracy: 0.9062\n",
      "Batch number: 138, Training: Loss: 475.9485, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 2298.2817, Accuracy: 0.8125\n",
      "Batch number: 141, Training: Loss: 321.6433, Accuracy: 0.9375\n",
      "Batch number: 142, Training: Loss: 1488.1410, Accuracy: 0.9062\n",
      "Batch number: 143, Training: Loss: 349.1432, Accuracy: 0.9688\n",
      "Batch number: 144, Training: Loss: 352.9482, Accuracy: 0.9688\n",
      "Batch number: 145, Training: Loss: 1874.5791, Accuracy: 0.8438\n",
      "Batch number: 146, Training: Loss: 541.7472, Accuracy: 0.8438\n",
      "Batch number: 147, Training: Loss: 1333.6721, Accuracy: 0.8438\n",
      "Batch number: 148, Training: Loss: 941.8287, Accuracy: 0.8438\n",
      "Batch number: 149, Training: Loss: 534.2445, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 438.9571, Accuracy: 0.9062\n",
      "Batch number: 151, Training: Loss: 1137.7682, Accuracy: 0.9062\n",
      "Batch number: 152, Training: Loss: 382.6132, Accuracy: 0.8438\n",
      "Batch number: 153, Training: Loss: 72.8190, Accuracy: 0.9688\n",
      "Batch number: 154, Training: Loss: 998.3877, Accuracy: 0.8438\n",
      "Batch number: 155, Training: Loss: 931.7665, Accuracy: 0.8438\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 2206.4966, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 158.9190, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 5648.8062, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 5659.8320, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 7658.9399, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 4170.0527, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 1979.6521, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 2280.7446, Accuracy: 0.7500\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 861.9774, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 2230.5498, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 1460.8452, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 2266.5742, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 3663.6694, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 600.0731, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 3906.9575, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 2102.7075, Accuracy: 0.5000\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 3676.8711, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 144.3810, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 3145.1216, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 4897.3276, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 1137.2578, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 3839.4316, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 623.5554, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 4114.2485, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 11038.2354, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 3160.4253, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 3001.4043, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 4864.2451, Accuracy: 0.7500\n",
      "Validation Batch number: 110, Validation: Loss: 1305.9868, Accuracy: 0.7500\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 4557.9092, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 2076.2019, Accuracy: 0.7500\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 700.7704, Accuracy: 0.7500\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 10112.6582, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 4096.5010, Accuracy: 0.5000\n",
      "Validation Batch number: 128, Validation: Loss: 3173.2935, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 6151.2061, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 6583.3613, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 379.6520, Accuracy: 0.7500\n",
      "Epoch : 022, Training: Loss : 1065.2184, Accuracy: 90.5124%\n",
      "Validation : Loss : 932.6392, Accuracy: 92.0863%, Time: 52.6058s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 24/25\n",
      "Batch number: 000, Training: Loss: 480.0356, Accuracy: 0.9062\n",
      "Batch number: 001, Training: Loss: 1702.5264, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 1176.9702, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 417.1888, Accuracy: 0.9375\n",
      "Batch number: 004, Training: Loss: 933.5538, Accuracy: 0.8438\n",
      "Batch number: 005, Training: Loss: 168.8913, Accuracy: 0.9062\n",
      "Batch number: 006, Training: Loss: 502.0013, Accuracy: 0.9375\n",
      "Batch number: 007, Training: Loss: 1146.5017, Accuracy: 0.8438\n",
      "Batch number: 008, Training: Loss: 10.1069, Accuracy: 0.9688\n",
      "Batch number: 009, Training: Loss: 179.9806, Accuracy: 0.9688\n",
      "Batch number: 010, Training: Loss: 398.6450, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 1113.1235, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 013, Training: Loss: 970.6365, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 741.5696, Accuracy: 0.9062\n",
      "Batch number: 015, Training: Loss: 25.5480, Accuracy: 0.9688\n",
      "Batch number: 016, Training: Loss: 514.0631, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 597.2361, Accuracy: 0.9062\n",
      "Batch number: 018, Training: Loss: 472.1481, Accuracy: 0.8750\n",
      "Batch number: 019, Training: Loss: 509.3192, Accuracy: 0.9688\n",
      "Batch number: 020, Training: Loss: 235.4739, Accuracy: 0.9688\n",
      "Batch number: 021, Training: Loss: 460.7857, Accuracy: 0.9062\n",
      "Batch number: 022, Training: Loss: 1111.9059, Accuracy: 0.8438\n",
      "Batch number: 023, Training: Loss: 817.5797, Accuracy: 0.8438\n",
      "Batch number: 024, Training: Loss: 1424.2739, Accuracy: 0.8438\n",
      "Batch number: 025, Training: Loss: 817.9448, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 520.8218, Accuracy: 0.9688\n",
      "Batch number: 027, Training: Loss: 286.2359, Accuracy: 0.9375\n",
      "Batch number: 028, Training: Loss: 255.9079, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 200.5889, Accuracy: 0.9062\n",
      "Batch number: 030, Training: Loss: 131.3702, Accuracy: 0.9375\n",
      "Batch number: 031, Training: Loss: 746.5818, Accuracy: 0.9688\n",
      "Batch number: 032, Training: Loss: 1898.3281, Accuracy: 0.9062\n",
      "Batch number: 033, Training: Loss: 357.4223, Accuracy: 0.9688\n",
      "Batch number: 034, Training: Loss: 304.5340, Accuracy: 0.9375\n",
      "Batch number: 035, Training: Loss: 299.8452, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 1214.3135, Accuracy: 0.8438\n",
      "Batch number: 037, Training: Loss: 365.2011, Accuracy: 0.9062\n",
      "Batch number: 038, Training: Loss: 632.5342, Accuracy: 0.8750\n",
      "Batch number: 039, Training: Loss: 743.4753, Accuracy: 0.9375\n",
      "Batch number: 040, Training: Loss: 1723.1973, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 1351.3306, Accuracy: 0.9062\n",
      "Batch number: 042, Training: Loss: 1899.6909, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 1373.2170, Accuracy: 0.9062\n",
      "Batch number: 044, Training: Loss: 2205.4048, Accuracy: 0.9062\n",
      "Batch number: 045, Training: Loss: 922.3907, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 78.2313, Accuracy: 0.9688\n",
      "Batch number: 047, Training: Loss: 1029.8956, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 1448.9860, Accuracy: 0.9062\n",
      "Batch number: 049, Training: Loss: 5153.1768, Accuracy: 0.6875\n",
      "Batch number: 050, Training: Loss: 3656.0891, Accuracy: 0.7812\n",
      "Batch number: 051, Training: Loss: 144.8429, Accuracy: 0.9688\n",
      "Batch number: 052, Training: Loss: 44.4753, Accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 053, Training: Loss: 2288.5918, Accuracy: 0.9375\n",
      "Batch number: 054, Training: Loss: 3600.8755, Accuracy: 0.7812\n",
      "Batch number: 055, Training: Loss: 1105.8179, Accuracy: 0.9375\n",
      "Batch number: 056, Training: Loss: 1695.8337, Accuracy: 0.8438\n",
      "Batch number: 057, Training: Loss: 212.6229, Accuracy: 0.9688\n",
      "Batch number: 058, Training: Loss: 1166.9780, Accuracy: 0.9375\n",
      "Batch number: 059, Training: Loss: 52.2572, Accuracy: 0.9688\n",
      "Batch number: 060, Training: Loss: 1145.5114, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 3586.8533, Accuracy: 0.8438\n",
      "Batch number: 062, Training: Loss: 894.6426, Accuracy: 0.8438\n",
      "Batch number: 063, Training: Loss: 171.0847, Accuracy: 0.9688\n",
      "Batch number: 064, Training: Loss: 917.0974, Accuracy: 0.9062\n",
      "Batch number: 065, Training: Loss: 386.8343, Accuracy: 0.9688\n",
      "Batch number: 066, Training: Loss: 793.9835, Accuracy: 0.9688\n",
      "Batch number: 067, Training: Loss: 618.8636, Accuracy: 0.9375\n",
      "Batch number: 068, Training: Loss: 358.2497, Accuracy: 0.9688\n",
      "Batch number: 069, Training: Loss: 1152.4512, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 829.5560, Accuracy: 0.9062\n",
      "Batch number: 071, Training: Loss: 1222.2379, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 113.2931, Accuracy: 0.9688\n",
      "Batch number: 073, Training: Loss: 671.1191, Accuracy: 0.8750\n",
      "Batch number: 074, Training: Loss: 312.9023, Accuracy: 0.8750\n",
      "Batch number: 075, Training: Loss: 884.0328, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 2138.8955, Accuracy: 0.6875\n",
      "Batch number: 077, Training: Loss: 440.1913, Accuracy: 0.9375\n",
      "Batch number: 078, Training: Loss: 127.2299, Accuracy: 0.9062\n",
      "Batch number: 079, Training: Loss: 500.3852, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 162.6265, Accuracy: 0.9688\n",
      "Batch number: 081, Training: Loss: 2463.5286, Accuracy: 0.8438\n",
      "Batch number: 082, Training: Loss: 424.4212, Accuracy: 0.9062\n",
      "Batch number: 083, Training: Loss: 155.0331, Accuracy: 0.9375\n",
      "Batch number: 084, Training: Loss: 847.8557, Accuracy: 0.9375\n",
      "Batch number: 085, Training: Loss: 20.0824, Accuracy: 0.9688\n",
      "Batch number: 086, Training: Loss: 8.0917, Accuracy: 0.9688\n",
      "Batch number: 087, Training: Loss: 65.0299, Accuracy: 0.9375\n",
      "Batch number: 088, Training: Loss: 341.0494, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 853.8376, Accuracy: 0.8125\n",
      "Batch number: 090, Training: Loss: 47.4502, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 79.4225, Accuracy: 0.9688\n",
      "Batch number: 092, Training: Loss: 193.5737, Accuracy: 0.9062\n",
      "Batch number: 093, Training: Loss: 566.9921, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 190.6557, Accuracy: 0.9062\n",
      "Batch number: 095, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 096, Training: Loss: 682.3291, Accuracy: 0.9688\n",
      "Batch number: 097, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 098, Training: Loss: 202.2511, Accuracy: 0.9688\n",
      "Batch number: 099, Training: Loss: 199.3064, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 747.4185, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 165.5391, Accuracy: 0.9688\n",
      "Batch number: 102, Training: Loss: 293.9857, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 126.5064, Accuracy: 0.9688\n",
      "Batch number: 104, Training: Loss: 11168.1230, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 1227.5873, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 841.7694, Accuracy: 0.9062\n",
      "Batch number: 107, Training: Loss: 20.0026, Accuracy: 0.9688\n",
      "Batch number: 108, Training: Loss: 1345.4014, Accuracy: 0.9062\n",
      "Batch number: 109, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 110, Training: Loss: 1475.1537, Accuracy: 0.9062\n",
      "Batch number: 111, Training: Loss: 892.3457, Accuracy: 0.8750\n",
      "Batch number: 112, Training: Loss: 37.3761, Accuracy: 0.9688\n",
      "Batch number: 113, Training: Loss: 186.2931, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 64.8302, Accuracy: 0.9688\n",
      "Batch number: 115, Training: Loss: 79.9991, Accuracy: 0.9688\n",
      "Batch number: 116, Training: Loss: 294.2465, Accuracy: 0.9062\n",
      "Batch number: 117, Training: Loss: 395.2837, Accuracy: 0.9062\n",
      "Batch number: 118, Training: Loss: 247.8765, Accuracy: 0.9688\n",
      "Batch number: 119, Training: Loss: 1035.0570, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 379.1149, Accuracy: 0.9375\n",
      "Batch number: 121, Training: Loss: 1776.1150, Accuracy: 0.7812\n",
      "Batch number: 122, Training: Loss: 434.8105, Accuracy: 0.9688\n",
      "Batch number: 123, Training: Loss: 474.0447, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 929.3521, Accuracy: 0.8750\n",
      "Batch number: 125, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 126, Training: Loss: 588.6597, Accuracy: 0.8750\n",
      "Batch number: 127, Training: Loss: 1475.7271, Accuracy: 0.7812\n",
      "Batch number: 128, Training: Loss: 773.4902, Accuracy: 0.8438\n",
      "Batch number: 129, Training: Loss: 6609.2515, Accuracy: 0.7500\n",
      "Batch number: 130, Training: Loss: 403.9056, Accuracy: 0.9375\n",
      "Batch number: 131, Training: Loss: 463.2363, Accuracy: 0.9062\n",
      "Batch number: 132, Training: Loss: 603.0793, Accuracy: 0.9062\n",
      "Batch number: 133, Training: Loss: 1952.1605, Accuracy: 0.9062\n",
      "Batch number: 134, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 502.5035, Accuracy: 0.9062\n",
      "Batch number: 136, Training: Loss: 1871.3091, Accuracy: 0.9062\n",
      "Batch number: 137, Training: Loss: 327.2328, Accuracy: 0.9688\n",
      "Batch number: 138, Training: Loss: 445.3890, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 16.4598, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 393.6658, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 1636.9508, Accuracy: 0.8438\n",
      "Batch number: 142, Training: Loss: 125.5149, Accuracy: 0.9375\n",
      "Batch number: 143, Training: Loss: 395.8112, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 421.4172, Accuracy: 0.9688\n",
      "Batch number: 145, Training: Loss: 139.5579, Accuracy: 0.8750\n",
      "Batch number: 146, Training: Loss: 571.7529, Accuracy: 0.9062\n",
      "Batch number: 147, Training: Loss: 801.1591, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 127.7052, Accuracy: 0.9688\n",
      "Batch number: 149, Training: Loss: 164.0809, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 458.2401, Accuracy: 0.9062\n",
      "Batch number: 151, Training: Loss: 211.7941, Accuracy: 0.9688\n",
      "Batch number: 152, Training: Loss: 537.8651, Accuracy: 0.9062\n",
      "Batch number: 153, Training: Loss: 758.2086, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 704.6501, Accuracy: 0.9062\n",
      "Batch number: 155, Training: Loss: 16.6818, Accuracy: 0.9688\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 004, Validation: Loss: 2149.6296, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 349.8835, Accuracy: 0.7500\n",
      "Validation Batch number: 009, Validation: Loss: 436.4832, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 5114.0156, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 3556.5024, Accuracy: 0.5000\n",
      "Validation Batch number: 021, Validation: Loss: 8172.2725, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 3358.1147, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 2333.7454, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 315.6157, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 3414.3911, Accuracy: 0.5000\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 1512.2068, Accuracy: 0.7500\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 752.4789, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 303.1946, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 201.7655, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 3666.1257, Accuracy: 0.5000\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 2699.9307, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 7917.0552, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 1903.6975, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 4914.3062, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 38.8765, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 1134.8123, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 8093.6680, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 1412.6523, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 3071.1772, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 10790.7305, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 2866.8574, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 3406.4204, Accuracy: 0.5000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 194.0598, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 5982.6450, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 138.5091, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 4405.6758, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 14878.1787, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 2523.1704, Accuracy: 0.7500\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 2485.3521, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 1904.6093, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 5698.9741, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 9552.9453, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 1502.2028, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 2589.2075, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 473.5294, Accuracy: 0.7500\n",
      "Epoch : 023, Training: Loss : 825.9005, Accuracy: 91.3531%\n",
      "Validation : Loss : 979.9688, Accuracy: 91.3669%, Time: 52.3013s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n",
      "Epoch: 25/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 000, Training: Loss: 1126.4047, Accuracy: 0.9375\n",
      "Batch number: 001, Training: Loss: 1566.6686, Accuracy: 0.9062\n",
      "Batch number: 002, Training: Loss: 1773.1196, Accuracy: 0.8750\n",
      "Batch number: 003, Training: Loss: 1545.3770, Accuracy: 0.8438\n",
      "Batch number: 004, Training: Loss: 614.6591, Accuracy: 0.9062\n",
      "Batch number: 005, Training: Loss: 124.2816, Accuracy: 0.9375\n",
      "Batch number: 006, Training: Loss: 750.5677, Accuracy: 0.9688\n",
      "Batch number: 007, Training: Loss: 735.0603, Accuracy: 0.9375\n",
      "Batch number: 008, Training: Loss: 187.8767, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 613.2218, Accuracy: 0.9062\n",
      "Batch number: 010, Training: Loss: 842.2296, Accuracy: 0.9375\n",
      "Batch number: 011, Training: Loss: 339.5148, Accuracy: 0.9375\n",
      "Batch number: 012, Training: Loss: 752.2887, Accuracy: 0.9688\n",
      "Batch number: 013, Training: Loss: 617.0780, Accuracy: 0.8750\n",
      "Batch number: 014, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 015, Training: Loss: 297.6903, Accuracy: 0.9375\n",
      "Batch number: 016, Training: Loss: 652.8350, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 1226.9202, Accuracy: 0.8438\n",
      "Batch number: 018, Training: Loss: 693.8294, Accuracy: 0.8125\n",
      "Batch number: 019, Training: Loss: 86.1630, Accuracy: 0.9375\n",
      "Batch number: 020, Training: Loss: 367.6565, Accuracy: 0.9062\n",
      "Batch number: 021, Training: Loss: 83.0134, Accuracy: 0.9375\n",
      "Batch number: 022, Training: Loss: 764.2525, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 024, Training: Loss: 739.1473, Accuracy: 0.8750\n",
      "Batch number: 025, Training: Loss: 346.2178, Accuracy: 0.9375\n",
      "Batch number: 026, Training: Loss: 751.2133, Accuracy: 0.9375\n",
      "Batch number: 027, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 028, Training: Loss: 716.1975, Accuracy: 0.9375\n",
      "Batch number: 029, Training: Loss: 332.2139, Accuracy: 0.9688\n",
      "Batch number: 030, Training: Loss: 36.2750, Accuracy: 0.9688\n",
      "Batch number: 031, Training: Loss: 342.6965, Accuracy: 0.9375\n",
      "Batch number: 032, Training: Loss: 1454.3280, Accuracy: 0.8750\n",
      "Batch number: 033, Training: Loss: 1051.0503, Accuracy: 0.8438\n",
      "Batch number: 034, Training: Loss: 385.8968, Accuracy: 0.8750\n",
      "Batch number: 035, Training: Loss: 791.7025, Accuracy: 0.9375\n",
      "Batch number: 036, Training: Loss: 1576.0315, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 548.0319, Accuracy: 0.9375\n",
      "Batch number: 038, Training: Loss: 1152.9601, Accuracy: 0.9062\n",
      "Batch number: 039, Training: Loss: 341.8296, Accuracy: 0.9062\n",
      "Batch number: 040, Training: Loss: 86.4476, Accuracy: 0.9688\n",
      "Batch number: 041, Training: Loss: 276.4566, Accuracy: 0.9688\n",
      "Batch number: 042, Training: Loss: 775.9100, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 84.3969, Accuracy: 0.9375\n",
      "Batch number: 044, Training: Loss: 436.9048, Accuracy: 0.9375\n",
      "Batch number: 045, Training: Loss: 141.5127, Accuracy: 0.9062\n",
      "Batch number: 046, Training: Loss: 310.8657, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 2097.1196, Accuracy: 0.8438\n",
      "Batch number: 048, Training: Loss: 2098.2175, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 428.8416, Accuracy: 0.9062\n",
      "Batch number: 050, Training: Loss: 1245.1191, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 2362.8938, Accuracy: 0.8125\n",
      "Batch number: 052, Training: Loss: 414.0896, Accuracy: 0.9375\n",
      "Batch number: 053, Training: Loss: 1862.5573, Accuracy: 0.9062\n",
      "Batch number: 054, Training: Loss: 1250.5203, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 109.4699, Accuracy: 0.9688\n",
      "Batch number: 056, Training: Loss: 917.2117, Accuracy: 0.8438\n",
      "Batch number: 057, Training: Loss: 627.9427, Accuracy: 0.9062\n",
      "Batch number: 058, Training: Loss: 1649.6052, Accuracy: 0.7500\n",
      "Batch number: 059, Training: Loss: 704.6036, Accuracy: 0.9062\n",
      "Batch number: 060, Training: Loss: 308.2345, Accuracy: 0.9688\n",
      "Batch number: 061, Training: Loss: 1409.9263, Accuracy: 0.8750\n",
      "Batch number: 062, Training: Loss: 669.6041, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 535.5962, Accuracy: 0.9375\n",
      "Batch number: 064, Training: Loss: 1162.8420, Accuracy: 0.8750\n",
      "Batch number: 065, Training: Loss: 776.4967, Accuracy: 0.9688\n",
      "Batch number: 066, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 067, Training: Loss: 343.5327, Accuracy: 0.9688\n",
      "Batch number: 068, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 069, Training: Loss: 470.2383, Accuracy: 0.9375\n",
      "Batch number: 070, Training: Loss: 1344.0769, Accuracy: 0.9062\n",
      "Batch number: 071, Training: Loss: 444.5649, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 130.8774, Accuracy: 0.9688\n",
      "Batch number: 073, Training: Loss: 739.3770, Accuracy: 0.8438\n",
      "Batch number: 074, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 075, Training: Loss: 467.0168, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 547.2170, Accuracy: 0.8438\n",
      "Batch number: 077, Training: Loss: 30.8384, Accuracy: 0.9688\n",
      "Batch number: 078, Training: Loss: 166.2630, Accuracy: 0.9375\n",
      "Batch number: 079, Training: Loss: 180.7336, Accuracy: 0.9375\n",
      "Batch number: 080, Training: Loss: 95.8633, Accuracy: 0.9688\n",
      "Batch number: 081, Training: Loss: 1111.3402, Accuracy: 0.9062\n",
      "Batch number: 082, Training: Loss: 788.2692, Accuracy: 0.9062\n",
      "Batch number: 083, Training: Loss: 414.4504, Accuracy: 0.8750\n",
      "Batch number: 084, Training: Loss: 807.9294, Accuracy: 0.9062\n",
      "Batch number: 085, Training: Loss: 419.6704, Accuracy: 0.9062\n",
      "Batch number: 086, Training: Loss: 17.6612, Accuracy: 0.9688\n",
      "Batch number: 087, Training: Loss: 128.5235, Accuracy: 0.9688\n",
      "Batch number: 088, Training: Loss: 879.6235, Accuracy: 0.9375\n",
      "Batch number: 089, Training: Loss: 1025.4797, Accuracy: 0.9062\n",
      "Batch number: 090, Training: Loss: 184.3677, Accuracy: 0.9375\n",
      "Batch number: 091, Training: Loss: 682.0568, Accuracy: 0.9062\n",
      "Batch number: 092, Training: Loss: 606.0396, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 1419.1987, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 561.7534, Accuracy: 0.9062\n",
      "Batch number: 095, Training: Loss: 502.9601, Accuracy: 0.9062\n",
      "Batch number: 096, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 097, Training: Loss: 646.0851, Accuracy: 0.9062\n",
      "Batch number: 098, Training: Loss: 167.9911, Accuracy: 0.9688\n",
      "Batch number: 099, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 496.9484, Accuracy: 0.9062\n",
      "Batch number: 101, Training: Loss: 557.2406, Accuracy: 0.9062\n",
      "Batch number: 102, Training: Loss: 210.8301, Accuracy: 0.9375\n",
      "Batch number: 103, Training: Loss: 261.3129, Accuracy: 0.9375\n",
      "Batch number: 104, Training: Loss: 949.5719, Accuracy: 0.9375\n",
      "Batch number: 105, Training: Loss: 133.3858, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 107, Training: Loss: 794.4056, Accuracy: 0.9375\n",
      "Batch number: 108, Training: Loss: 1418.2823, Accuracy: 0.8125\n",
      "Batch number: 109, Training: Loss: 150.3439, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 708.0732, Accuracy: 0.9062\n",
      "Batch number: 111, Training: Loss: 1323.2194, Accuracy: 0.8125\n",
      "Batch number: 112, Training: Loss: 224.2181, Accuracy: 0.9375\n",
      "Batch number: 113, Training: Loss: 315.0218, Accuracy: 0.9375\n",
      "Batch number: 114, Training: Loss: 782.2381, Accuracy: 0.9062\n",
      "Batch number: 115, Training: Loss: 1224.9752, Accuracy: 0.9062\n",
      "Batch number: 116, Training: Loss: 459.3904, Accuracy: 0.9375\n",
      "Batch number: 117, Training: Loss: 278.0684, Accuracy: 0.9375\n",
      "Batch number: 118, Training: Loss: 76.6175, Accuracy: 0.9688\n",
      "Batch number: 119, Training: Loss: 279.9781, Accuracy: 0.9688\n",
      "Batch number: 120, Training: Loss: 476.1797, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 1058.7537, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 214.0581, Accuracy: 0.9375\n",
      "Batch number: 123, Training: Loss: 246.6454, Accuracy: 0.9375\n",
      "Batch number: 124, Training: Loss: 552.7756, Accuracy: 0.9375\n",
      "Batch number: 125, Training: Loss: 142.7901, Accuracy: 0.9688\n",
      "Batch number: 126, Training: Loss: 73.3954, Accuracy: 0.9375\n",
      "Batch number: 127, Training: Loss: 213.6078, Accuracy: 0.9375\n",
      "Batch number: 128, Training: Loss: 363.9468, Accuracy: 0.9062\n",
      "Batch number: 129, Training: Loss: 231.9186, Accuracy: 0.9375\n",
      "Batch number: 130, Training: Loss: 898.5106, Accuracy: 0.9062\n",
      "Batch number: 131, Training: Loss: 908.2509, Accuracy: 0.9062\n",
      "Batch number: 132, Training: Loss: 228.4445, Accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 133, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 134, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 135, Training: Loss: 459.9122, Accuracy: 0.9062\n",
      "Batch number: 136, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 137, Training: Loss: 553.1854, Accuracy: 0.9062\n",
      "Batch number: 138, Training: Loss: 81.9441, Accuracy: 0.9375\n",
      "Batch number: 139, Training: Loss: 0.0000, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 700.0387, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 695.4446, Accuracy: 0.9062\n",
      "Batch number: 142, Training: Loss: 206.9900, Accuracy: 0.9688\n",
      "Batch number: 143, Training: Loss: 1930.8655, Accuracy: 0.8438\n",
      "Batch number: 144, Training: Loss: 2084.5349, Accuracy: 0.9375\n",
      "Batch number: 145, Training: Loss: 221.1168, Accuracy: 0.9688\n",
      "Batch number: 146, Training: Loss: 200.4106, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 803.3661, Accuracy: 0.8750\n",
      "Batch number: 148, Training: Loss: 95.2358, Accuracy: 0.9688\n",
      "Batch number: 149, Training: Loss: 136.0146, Accuracy: 0.9375\n",
      "Batch number: 150, Training: Loss: 1326.1062, Accuracy: 0.7500\n",
      "Batch number: 151, Training: Loss: 538.2515, Accuracy: 0.9375\n",
      "Batch number: 152, Training: Loss: 164.0825, Accuracy: 0.9375\n",
      "Batch number: 153, Training: Loss: 194.6862, Accuracy: 0.9375\n",
      "Batch number: 154, Training: Loss: 663.3289, Accuracy: 0.9062\n",
      "Batch number: 155, Training: Loss: 374.4761, Accuracy: 0.9062\n",
      "Validation Batch number: 000, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 328.6601, Accuracy: 0.7500\n",
      "Validation Batch number: 002, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 4746.7104, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 3315.6289, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 007, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 008, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 009, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 011, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 012, Validation: Loss: 244.9088, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 295.9561, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 016, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 019, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 4796.6748, Accuracy: 0.7500\n",
      "Validation Batch number: 021, Validation: Loss: 9413.2266, Accuracy: 0.5000\n",
      "Validation Batch number: 022, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 1330.3147, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 025, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 4330.6626, Accuracy: 0.5000\n",
      "Validation Batch number: 027, Validation: Loss: 845.9628, Accuracy: 0.7500\n",
      "Validation Batch number: 028, Validation: Loss: 693.0648, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 035, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 039, Validation: Loss: 825.0519, Accuracy: 0.7500\n",
      "Validation Batch number: 040, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 041, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 042, Validation: Loss: 625.7874, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 045, Validation: Loss: 928.7549, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 048, Validation: Loss: 61.7476, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 050, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 053, Validation: Loss: 5108.2388, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 4027.5117, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 6718.6826, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 059, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 1019.3344, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 1780.8844, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 068, Validation: Loss: 2584.6201, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 071, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 4850.1035, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 077, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 079, Validation: Loss: 4331.3159, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 082, Validation: Loss: 4342.0991, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 2062.2595, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 13968.2275, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 3987.1299, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 089, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 093, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 847.5986, Accuracy: 0.7500\n",
      "Validation Batch number: 095, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 097, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 098, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 2640.6270, Accuracy: 0.7500\n",
      "Validation Batch number: 101, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 6824.1177, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 114, Validation: Loss: 5355.6309, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 2039963.3750, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 125, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 127, Validation: Loss: 1245.0791, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 1315.7612, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 130, Validation: Loss: 3189.3262, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 10347.2441, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 137, Validation: Loss: 376.2643, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Epoch : 024, Training: Loss : 589.7664, Accuracy: 92.0136%\n",
      "Validation : Loss : 15537.1838, Accuracy: 92.2662%, Time: 52.2349s\n",
      "Best accuracy achieved so far : 0.9371 on epoch 10\n"
     ]
    }
   ],
   "source": [
    "history = trainValid(vgg19, lossFunc, optimizer, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "culacpOcgcUj"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotCost(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmqA9osy49ac"
   },
   "source": [
    "Select the best model (i.e. the weights file saved on the max epoch) to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEDTZ_VmkEK6"
   },
   "outputs": [],
   "source": [
    "saved_model = torch.load('src_model_10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39gAloATgHQl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4996, 556]\n",
      "5552\n",
      "Test Batch number: 000, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 001, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 002, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 003, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 004, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 005, Test: Loss: 164.0612, Accuracy: 0.0000\n",
      "Test Batch number: 006, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 007, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 008, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 009, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 010, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 011, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 012, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 013, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 014, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 015, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 016, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 017, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 018, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 019, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 020, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 021, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 022, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 023, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 024, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 025, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 026, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 027, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 028, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 029, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 030, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 031, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 032, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 033, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 034, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 035, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 036, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 037, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 038, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 039, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 040, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 041, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 042, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 043, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 044, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 045, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 046, Test: Loss: 7143.8574, Accuracy: 0.0000\n",
      "Test Batch number: 047, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 048, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 049, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 050, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 051, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 052, Test: Loss: 1511.3516, Accuracy: 0.0000\n",
      "Test Batch number: 053, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 054, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 055, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 056, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 057, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 058, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 059, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 060, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 061, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 062, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 063, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 064, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 065, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 066, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 067, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 068, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 069, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 070, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 071, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 072, Test: Loss: 1051.6118, Accuracy: 0.0000\n",
      "Test Batch number: 073, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 074, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 075, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 076, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 077, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 078, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 079, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 080, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 081, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 082, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 083, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 084, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 085, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 086, Test: Loss: 1506.3787, Accuracy: 0.0000\n",
      "Test Batch number: 087, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 088, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 089, Test: Loss: 121.3641, Accuracy: 0.0000\n",
      "Test Batch number: 090, Test: Loss: 1249.5067, Accuracy: 0.0000\n",
      "Test Batch number: 091, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 092, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 093, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 094, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 095, Test: Loss: 2300.5029, Accuracy: 0.0000\n",
      "Test Batch number: 096, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 097, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 098, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 099, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 100, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 101, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 102, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 103, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 104, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 105, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 106, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 107, Test: Loss: 789.3578, Accuracy: 0.0000\n",
      "Test Batch number: 108, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 109, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 110, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 111, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 112, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 113, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 114, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 115, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 116, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 117, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 118, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 119, Test: Loss: 2370.8853, Accuracy: 0.0000\n",
      "Test Batch number: 120, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 121, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 122, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 123, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 124, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 125, Test: Loss: 169.7732, Accuracy: 0.0000\n",
      "Test Batch number: 126, Test: Loss: 771.3419, Accuracy: 0.0000\n",
      "Test Batch number: 127, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 128, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 129, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 130, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 131, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 132, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 133, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 134, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 135, Test: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 136, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 137, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 138, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 139, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 140, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 141, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 142, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 143, Test: Loss: 1256.8713, Accuracy: 0.0000\n",
      "Test Batch number: 144, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 145, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 146, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 147, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 148, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 149, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 150, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 151, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 152, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 153, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 154, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 155, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 156, Test: Loss: 519.1090, Accuracy: 0.0000\n",
      "Test Batch number: 157, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 158, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 159, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 160, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 161, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 162, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 163, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 164, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 165, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 166, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 167, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 168, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 169, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 170, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 171, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 172, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 173, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 174, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 175, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 176, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 177, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 178, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 179, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 180, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 181, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 182, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 183, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 184, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 185, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 186, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 187, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 188, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 189, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 190, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 191, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 192, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 193, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 194, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 195, Test: Loss: 1253.7183, Accuracy: 0.0000\n",
      "Test Batch number: 196, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 197, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 198, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 199, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 200, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 201, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 202, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 203, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 204, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 205, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 206, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 207, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 208, Test: Loss: 863.4447, Accuracy: 0.0000\n",
      "Test Batch number: 209, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 210, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 211, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 212, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 213, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 214, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 215, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 216, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 217, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 218, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 219, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 220, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 221, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 222, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 223, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 224, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 225, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 226, Test: Loss: 160.9766, Accuracy: 0.0000\n",
      "Test Batch number: 227, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 228, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 229, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 230, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 231, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 232, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 233, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 234, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 235, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 236, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 237, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 238, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 239, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 240, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 241, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 242, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 243, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 244, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 245, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 246, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 247, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 248, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 249, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 250, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 251, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 252, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 253, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 254, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 255, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 256, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 257, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 258, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 259, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 260, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 261, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 262, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 263, Test: Loss: 3419.7300, Accuracy: 0.0000\n",
      "Test Batch number: 264, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 265, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 266, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 267, Test: Loss: 643.4044, Accuracy: 0.0000\n",
      "Test Batch number: 268, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 269, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 270, Test: Loss: 3247.3086, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 271, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 272, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 273, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 274, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 275, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 276, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 277, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 278, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 279, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 280, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 281, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 282, Test: Loss: 3000.3423, Accuracy: 0.0000\n",
      "Test Batch number: 283, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 284, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 285, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 286, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 287, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 288, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 289, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 290, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 291, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 292, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 293, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 294, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 295, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 296, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 297, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 298, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 299, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 300, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 301, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 302, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 303, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 304, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 305, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 306, Test: Loss: 4698.7808, Accuracy: 0.0000\n",
      "Test Batch number: 307, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 308, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 309, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 310, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 311, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 312, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 313, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 314, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 315, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 316, Test: Loss: 932.0049, Accuracy: 0.0000\n",
      "Test Batch number: 317, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 318, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 319, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 320, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 321, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 322, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 323, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 324, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 325, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 326, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 327, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 328, Test: Loss: 3054.2407, Accuracy: 0.0000\n",
      "Test Batch number: 329, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 330, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 331, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 332, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 333, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 334, Test: Loss: 1086.8914, Accuracy: 0.0000\n",
      "Test Batch number: 335, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 336, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 337, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 338, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 339, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 340, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 341, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 342, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 343, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 344, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 345, Test: Loss: 1399.9967, Accuracy: 0.0000\n",
      "Test Batch number: 346, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 347, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 348, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 349, Test: Loss: 75.5522, Accuracy: 0.0000\n",
      "Test Batch number: 350, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 351, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 352, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 353, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 354, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 355, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 356, Test: Loss: 191.7715, Accuracy: 0.0000\n",
      "Test Batch number: 357, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 358, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 359, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 360, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 361, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 362, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 363, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 364, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 365, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 366, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 367, Test: Loss: 1225.9365, Accuracy: 0.0000\n",
      "Test Batch number: 368, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 369, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 370, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 371, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 372, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 373, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 374, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 375, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 376, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 377, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 378, Test: Loss: 1414.6477, Accuracy: 0.0000\n",
      "Test Batch number: 379, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 380, Test: Loss: 1243.2341, Accuracy: 0.0000\n",
      "Test Batch number: 381, Test: Loss: 373.2083, Accuracy: 0.0000\n",
      "Test Batch number: 382, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 383, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 384, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 385, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 386, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 387, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 388, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 389, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 390, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 391, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 392, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 393, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 394, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 395, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 396, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 397, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 398, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 399, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 400, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 401, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 402, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 403, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 404, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 405, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 406, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 407, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 408, Test: Loss: 433.2963, Accuracy: 0.0000\n",
      "Test Batch number: 409, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 410, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 411, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 412, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 413, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 414, Test: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 415, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 416, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 417, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 418, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 419, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 420, Test: Loss: 736.7958, Accuracy: 0.0000\n",
      "Test Batch number: 421, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 422, Test: Loss: 2631.3601, Accuracy: 0.0000\n",
      "Test Batch number: 423, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 424, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 425, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 426, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 427, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 428, Test: Loss: 1704.5182, Accuracy: 0.0000\n",
      "Test Batch number: 429, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 430, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 431, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 432, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 433, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 434, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 435, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 436, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 437, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 438, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 439, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 440, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 441, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 442, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 443, Test: Loss: 1047.1611, Accuracy: 0.0000\n",
      "Test Batch number: 444, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 445, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 446, Test: Loss: 2872.3110, Accuracy: 0.0000\n",
      "Test Batch number: 447, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 448, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 449, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 450, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 451, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 452, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 453, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 454, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 455, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 456, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 457, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 458, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 459, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 460, Test: Loss: 16704.8809, Accuracy: 0.0000\n",
      "Test Batch number: 461, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 462, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 463, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 464, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 465, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 466, Test: Loss: 2098.9124, Accuracy: 0.0000\n",
      "Test Batch number: 467, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 468, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 469, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 470, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 471, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 472, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 473, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 474, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 475, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 476, Test: Loss: 212.2057, Accuracy: 0.0000\n",
      "Test Batch number: 477, Test: Loss: 2184.7119, Accuracy: 0.0000\n",
      "Test Batch number: 478, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 479, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 480, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 481, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 482, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 483, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 484, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 485, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 486, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 487, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 488, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 489, Test: Loss: 529.4703, Accuracy: 0.0000\n",
      "Test Batch number: 490, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 491, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 492, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 493, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 494, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 495, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 496, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 497, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 498, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 499, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 500, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 501, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 502, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 503, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 504, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 505, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 506, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 507, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 508, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 509, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 510, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 511, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 512, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 513, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 514, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 515, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 516, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 517, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 518, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 519, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 520, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 521, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 522, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 523, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 524, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 525, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 526, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 527, Test: Loss: 1609.0962, Accuracy: 0.0000\n",
      "Test Batch number: 528, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 529, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 530, Test: Loss: 2395.7632, Accuracy: 0.0000\n",
      "Test Batch number: 531, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 532, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 533, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 534, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 535, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 536, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 537, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 538, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 539, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 540, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 541, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 542, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 543, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 544, Test: Loss: 2918.9026, Accuracy: 0.0000\n",
      "Test Batch number: 545, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 546, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 547, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 548, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 549, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 550, Test: Loss: 340.2742, Accuracy: 0.0000\n",
      "Test Batch number: 551, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 552, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 553, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 554, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 555, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 556, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 557, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 558, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 559, Test: Loss: 2319.0601, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 560, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 561, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 562, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 563, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 564, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 565, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 566, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 567, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 568, Test: Loss: 451.5653, Accuracy: 0.0000\n",
      "Test Batch number: 569, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 570, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 571, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 572, Test: Loss: 546.8104, Accuracy: 0.0000\n",
      "Test Batch number: 573, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 574, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 575, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 576, Test: Loss: 3398.5293, Accuracy: 0.0000\n",
      "Test Batch number: 577, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 578, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 579, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 580, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 581, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 582, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 583, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 584, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 585, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 586, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 587, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 588, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 589, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 590, Test: Loss: 1793.8940, Accuracy: 0.0000\n",
      "Test Batch number: 591, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 592, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 593, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 594, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 595, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 596, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 597, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 598, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 599, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 600, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 601, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 602, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 603, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 604, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 605, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 606, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 607, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 608, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 609, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 610, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 611, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 612, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 613, Test: Loss: 762.3511, Accuracy: 0.0000\n",
      "Test Batch number: 614, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 615, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 616, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 617, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 618, Test: Loss: 198.6800, Accuracy: 0.0000\n",
      "Test Batch number: 619, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 620, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 621, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 622, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 623, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 624, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 625, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 626, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 627, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 628, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 629, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 630, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 631, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 632, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 633, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 634, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 635, Test: Loss: 2264.5229, Accuracy: 0.0000\n",
      "Test Batch number: 636, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 637, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 638, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 639, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 640, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 641, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 642, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 643, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 644, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 645, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 646, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 647, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 648, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 649, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 650, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 651, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 652, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 653, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 654, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 655, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 656, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 657, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 658, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 659, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 660, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 661, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 662, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 663, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 664, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 665, Test: Loss: 1121.7729, Accuracy: 0.0000\n",
      "Test Batch number: 666, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 667, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 668, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 669, Test: Loss: 1866.2915, Accuracy: 0.0000\n",
      "Test Batch number: 670, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 671, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 672, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 673, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 674, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 675, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 676, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 677, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 678, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 679, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 680, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 681, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 682, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 683, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 684, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 685, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 686, Test: Loss: 501.9332, Accuracy: 0.0000\n",
      "Test Batch number: 687, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 688, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 689, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 690, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 691, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 692, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 693, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 694, Test: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 695, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 696, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 697, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 698, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 699, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 700, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 701, Test: Loss: 4945.7388, Accuracy: 0.0000\n",
      "Test Batch number: 702, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 703, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 704, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 705, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 706, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 707, Test: Loss: 422.4275, Accuracy: 0.0000\n",
      "Test Batch number: 708, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 709, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 710, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 711, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 712, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 713, Test: Loss: 546.8104, Accuracy: 0.0000\n",
      "Test Batch number: 714, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 715, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 716, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 717, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 718, Test: Loss: 1977.8074, Accuracy: 0.0000\n",
      "Test Batch number: 719, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 720, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 721, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 722, Test: Loss: 20.4995, Accuracy: 0.0000\n",
      "Test Batch number: 723, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 724, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 725, Test: Loss: 2716.3125, Accuracy: 0.0000\n",
      "Test Batch number: 726, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 727, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 728, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 729, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 730, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 731, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 732, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 733, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 734, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 735, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 736, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 737, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 738, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 739, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 740, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 741, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 742, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 743, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 744, Test: Loss: 448.7476, Accuracy: 0.0000\n",
      "Test Batch number: 745, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 746, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 747, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 748, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 749, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 750, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 751, Test: Loss: 2140.5547, Accuracy: 0.0000\n",
      "Test Batch number: 752, Test: Loss: 1130.0909, Accuracy: 0.0000\n",
      "Test Batch number: 753, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 754, Test: Loss: 246.2321, Accuracy: 0.0000\n",
      "Test Batch number: 755, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 756, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 757, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 758, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 759, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 760, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 761, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 762, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 763, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 764, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 765, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 766, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 767, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 768, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 769, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 770, Test: Loss: 1720.6764, Accuracy: 0.0000\n",
      "Test Batch number: 771, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 772, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 773, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 774, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 775, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 776, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 777, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 778, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 779, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 780, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 781, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 782, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 783, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 784, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 785, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 786, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 787, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 788, Test: Loss: 145.6346, Accuracy: 0.0000\n",
      "Test Batch number: 789, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 790, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 791, Test: Loss: 385.4984, Accuracy: 0.0000\n",
      "Test Batch number: 792, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 793, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 794, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 795, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 796, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 797, Test: Loss: 2605.7300, Accuracy: 0.0000\n",
      "Test Batch number: 798, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 799, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 800, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 801, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 802, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 803, Test: Loss: 1356.9121, Accuracy: 0.0000\n",
      "Test Batch number: 804, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 805, Test: Loss: 3847.4583, Accuracy: 0.0000\n",
      "Test Batch number: 806, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 807, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 808, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 809, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 810, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 811, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 812, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 813, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 814, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 815, Test: Loss: 3717.9102, Accuracy: 0.0000\n",
      "Test Batch number: 816, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 817, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 818, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 819, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 820, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 821, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 822, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 823, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 824, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 825, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 826, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 827, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 828, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 829, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 830, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 831, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 832, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 833, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 834, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 835, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 836, Test: Loss: 2001.7134, Accuracy: 0.0000\n",
      "Test Batch number: 837, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 838, Test: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 839, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 840, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 841, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 842, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 843, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 844, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 845, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 846, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 847, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 848, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 849, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 850, Test: Loss: 181.4211, Accuracy: 0.0000\n",
      "Test Batch number: 851, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 852, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 853, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 854, Test: Loss: 781.1559, Accuracy: 0.0000\n",
      "Test Batch number: 855, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 856, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 857, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 858, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 859, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 860, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 861, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 862, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 863, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 864, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 865, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 866, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 867, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 868, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 869, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 870, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 871, Test: Loss: 1328.5913, Accuracy: 0.0000\n",
      "Test Batch number: 872, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 873, Test: Loss: 627.5164, Accuracy: 0.0000\n",
      "Test Batch number: 874, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 875, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 876, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 877, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 878, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 879, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 880, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 881, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 882, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 883, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 884, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 885, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 886, Test: Loss: 102.5715, Accuracy: 0.0000\n",
      "Test Batch number: 887, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 888, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 889, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 890, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 891, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 892, Test: Loss: 1784.7473, Accuracy: 0.0000\n",
      "Test Batch number: 893, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 894, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 895, Test: Loss: 2244.0894, Accuracy: 0.0000\n",
      "Test Batch number: 896, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 897, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 898, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 899, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 900, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 901, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 902, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 903, Test: Loss: 63.4957, Accuracy: 0.0000\n",
      "Test Batch number: 904, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 905, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 906, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 907, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 908, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 909, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 910, Test: Loss: 800.6788, Accuracy: 0.0000\n",
      "Test Batch number: 911, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 912, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 913, Test: Loss: 373.9802, Accuracy: 0.0000\n",
      "Test Batch number: 914, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 915, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 916, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 917, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 918, Test: Loss: 2126.3613, Accuracy: 0.0000\n",
      "Test Batch number: 919, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 920, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 921, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 922, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 923, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 924, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 925, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 926, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 927, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 928, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 929, Test: Loss: 1160.3013, Accuracy: 0.0000\n",
      "Test Batch number: 930, Test: Loss: 2409.8916, Accuracy: 0.0000\n",
      "Test Batch number: 931, Test: Loss: 3781.7690, Accuracy: 0.0000\n",
      "Test Batch number: 932, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 933, Test: Loss: 781.3559, Accuracy: 0.0000\n",
      "Test Batch number: 934, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 935, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 936, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 937, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 938, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 939, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 940, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 941, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 942, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 943, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 944, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 945, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 946, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 947, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 948, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 949, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 950, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 951, Test: Loss: 1664.3984, Accuracy: 0.0000\n",
      "Test Batch number: 952, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 953, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 954, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 955, Test: Loss: 936.0492, Accuracy: 0.0000\n",
      "Test Batch number: 956, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 957, Test: Loss: 2585.4121, Accuracy: 0.0000\n",
      "Test Batch number: 958, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 959, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 960, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 961, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 962, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 963, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 964, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 965, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 966, Test: Loss: 1253.2310, Accuracy: 0.0000\n",
      "Test Batch number: 967, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 968, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 969, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 970, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 971, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 972, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 973, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 974, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 975, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 976, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 977, Test: Loss: 2737.5771, Accuracy: 0.0000\n",
      "Test Batch number: 978, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 979, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 980, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 981, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 982, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 983, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 984, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 985, Test: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 986, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 987, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 988, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 989, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 990, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 991, Test: Loss: 1336.1343, Accuracy: 0.0000\n",
      "Test Batch number: 992, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 993, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 994, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 995, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 996, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 997, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 998, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 999, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1000, Test: Loss: 76.2650, Accuracy: 0.0000\n",
      "Test Batch number: 1001, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1002, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1003, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1004, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1005, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1006, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1007, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1008, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1009, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1010, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1011, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1012, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1013, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1014, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1015, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1016, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1017, Test: Loss: 574.4670, Accuracy: 0.0000\n",
      "Test Batch number: 1018, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1019, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1020, Test: Loss: 447.8983, Accuracy: 0.0000\n",
      "Test Batch number: 1021, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1022, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1023, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1024, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1025, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1026, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1027, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1028, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1029, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1030, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1031, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1032, Test: Loss: 2724.9351, Accuracy: 0.0000\n",
      "Test Batch number: 1033, Test: Loss: 758.0947, Accuracy: 0.0000\n",
      "Test Batch number: 1034, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1035, Test: Loss: 722.8798, Accuracy: 0.0000\n",
      "Test Batch number: 1036, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1037, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1038, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1039, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1040, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1041, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1042, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1043, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1044, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1045, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1046, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1047, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1048, Test: Loss: 200.1617, Accuracy: 0.0000\n",
      "Test Batch number: 1049, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1050, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1051, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1052, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1053, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1054, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1055, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1056, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1057, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1058, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1059, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1060, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1061, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1062, Test: Loss: 452.2992, Accuracy: 0.0000\n",
      "Test Batch number: 1063, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1064, Test: Loss: 373.5082, Accuracy: 0.0000\n",
      "Test Batch number: 1065, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1066, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1067, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1068, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1069, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1070, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1071, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1072, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1073, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1074, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1075, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1076, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1077, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1078, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1079, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1080, Test: Loss: 50.7863, Accuracy: 0.0000\n",
      "Test Batch number: 1081, Test: Loss: 1721.7456, Accuracy: 0.0000\n",
      "Test Batch number: 1082, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1083, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1084, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1085, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1086, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1087, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1088, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1089, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1090, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1091, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1092, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1093, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1094, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1095, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1096, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1097, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1098, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1099, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1100, Test: Loss: 546.8104, Accuracy: 0.0000\n",
      "Test Batch number: 1101, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1102, Test: Loss: 2067.3875, Accuracy: 0.0000\n",
      "Test Batch number: 1103, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1104, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1105, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1106, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1107, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1108, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1109, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1110, Test: Loss: 680.3711, Accuracy: 0.0000\n",
      "Test Batch number: 1111, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1112, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1113, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1114, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1115, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1116, Test: Loss: 2054.5742, Accuracy: 0.0000\n",
      "Test Batch number: 1117, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1118, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1119, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1120, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1121, Test: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 1122, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1123, Test: Loss: 751.3156, Accuracy: 0.0000\n",
      "Test Batch number: 1124, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1125, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1126, Test: Loss: 2021.8967, Accuracy: 0.0000\n",
      "Test Batch number: 1127, Test: Loss: 674.9396, Accuracy: 0.0000\n",
      "Test Batch number: 1128, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1129, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1130, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1131, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1132, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1133, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1134, Test: Loss: 516.2684, Accuracy: 0.0000\n",
      "Test Batch number: 1135, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1136, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1137, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1138, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1139, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1140, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1141, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1142, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1143, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1144, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1145, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1146, Test: Loss: 1575.6758, Accuracy: 0.0000\n",
      "Test Batch number: 1147, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1148, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1149, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1150, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1151, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1152, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1153, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1154, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1155, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1156, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1157, Test: Loss: 1782.1731, Accuracy: 0.0000\n",
      "Test Batch number: 1158, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1159, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1160, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1161, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1162, Test: Loss: 1334.8760, Accuracy: 0.0000\n",
      "Test Batch number: 1163, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1164, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1165, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1166, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1167, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1168, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1169, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1170, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1171, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1172, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1173, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1174, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1175, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1176, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1177, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1178, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1179, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1180, Test: Loss: 1709.9797, Accuracy: 0.0000\n",
      "Test Batch number: 1181, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1182, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1183, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1184, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1185, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1186, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1187, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1188, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1189, Test: Loss: 1857.2311, Accuracy: 0.0000\n",
      "Test Batch number: 1190, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1191, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1192, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1193, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1194, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1195, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1196, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1197, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1198, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1199, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1200, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1201, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1202, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1203, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1204, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1205, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1206, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1207, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1208, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1209, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1210, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1211, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1212, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1213, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1214, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1215, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1216, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1217, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1218, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1219, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1220, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1221, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1222, Test: Loss: 437.9221, Accuracy: 0.0000\n",
      "Test Batch number: 1223, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1224, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1225, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1226, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1227, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1228, Test: Loss: 1592.1520, Accuracy: 0.0000\n",
      "Test Batch number: 1229, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1230, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1231, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1232, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1233, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1234, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1235, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1236, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1237, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1238, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1239, Test: Loss: 3631.1235, Accuracy: 0.0000\n",
      "Test Batch number: 1240, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1241, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1242, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1243, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1244, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1245, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1246, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1247, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1248, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1249, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1250, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1251, Test: Loss: 2394.7305, Accuracy: 0.0000\n",
      "Test Batch number: 1252, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1253, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1254, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1255, Test: Loss: 130.4115, Accuracy: 0.0000\n",
      "Test Batch number: 1256, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1257, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1258, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1259, Test: Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 1260, Test: Loss: 1708.4701, Accuracy: 0.0000\n",
      "Test Batch number: 1261, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1262, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1263, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1264, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1265, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1266, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1267, Test: Loss: 1492.5879, Accuracy: 0.0000\n",
      "Test Batch number: 1268, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1269, Test: Loss: 260.3631, Accuracy: 0.0000\n",
      "Test Batch number: 1270, Test: Loss: 457.2861, Accuracy: 0.0000\n",
      "Test Batch number: 1271, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1272, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1273, Test: Loss: 3268.1455, Accuracy: 0.0000\n",
      "Test Batch number: 1274, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1275, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1276, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1277, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1278, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1279, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1280, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1281, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1282, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1283, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1284, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1285, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1286, Test: Loss: 2981.6284, Accuracy: 0.0000\n",
      "Test Batch number: 1287, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1288, Test: Loss: 900.0375, Accuracy: 0.0000\n",
      "Test Batch number: 1289, Test: Loss: 259.7331, Accuracy: 0.0000\n",
      "Test Batch number: 1290, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1291, Test: Loss: 259.4730, Accuracy: 0.0000\n",
      "Test Batch number: 1292, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1293, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1294, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1295, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1296, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1297, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1298, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1299, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1300, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1301, Test: Loss: 5034.2578, Accuracy: 0.0000\n",
      "Test Batch number: 1302, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1303, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1304, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1305, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1306, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1307, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1308, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1309, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1310, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1311, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1312, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1313, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1314, Test: Loss: 6142.6558, Accuracy: 0.0000\n",
      "Test Batch number: 1315, Test: Loss: 5569.7583, Accuracy: 0.0000\n",
      "Test Batch number: 1316, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1317, Test: Loss: 4125.3433, Accuracy: 0.0000\n",
      "Test Batch number: 1318, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1319, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1320, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1321, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1322, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1323, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1324, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1325, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1326, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1327, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1328, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1329, Test: Loss: 398.0469, Accuracy: 0.0000\n",
      "Test Batch number: 1330, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1331, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1332, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1333, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1334, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1335, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1336, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1337, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1338, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1339, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1340, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1341, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1342, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1343, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1344, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1345, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1346, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1347, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1348, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1349, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1350, Test: Loss: 1342.4402, Accuracy: 0.0000\n",
      "Test Batch number: 1351, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1352, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1353, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1354, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1355, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1356, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1357, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1358, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1359, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1360, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1361, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1362, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1363, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1364, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1365, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1366, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1367, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1368, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1369, Test: Loss: 1519.4368, Accuracy: 0.0000\n",
      "Test Batch number: 1370, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1371, Test: Loss: 294.9209, Accuracy: 0.0000\n",
      "Test Batch number: 1372, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1373, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1374, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1375, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1376, Test: Loss: 229.7477, Accuracy: 0.0000\n",
      "Test Batch number: 1377, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1378, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1379, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1380, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1381, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1382, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1383, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1384, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1385, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1386, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1387, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test accuracy : 0.8962536023054755\n"
     ]
    }
   ],
   "source": [
    "computeTestSetAccuracy(saved_model, lossFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_example = transforms.Compose([transforms.Resize(size=256),\n",
    "                            transforms.CenterCrop(size=224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                 [0.229, 0.224, 0.225])])\n",
    "\n",
    "dataset = datasets.ImageFolder('examples', transform=transform_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Disable grad\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Retrieve item\n",
    "    index = 0\n",
    "    item = dataset[index]\n",
    "    image = item[0]\n",
    "    #true_target = item[1]\n",
    "    imshow(image)\n",
    "    image=image.to(device)\n",
    "    # Loading the saved model\n",
    "\n",
    "    \n",
    "    # Generate prediction\n",
    "    prediction = saved_model(image.unsqueeze(0))\n",
    "    \n",
    "    # Predicted class value using argmax\n",
    "    predicted_class = np.argmax(prediction.cpu())\n",
    "    \n",
    "    # Reshape image\n",
    "\n",
    "    image=image.cpu()\n",
    "    image=image.swapaxes(0,1)\n",
    "    image=image.swapaxes(1,2)\n",
    "    \n",
    "    # Show result\n",
    "    plt.imshow(image.cpu(), cmap='nipy_spectral')\n",
    "    plt.title(f'Prediction: {predicted_class} - Actual target: 1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "RS-Image-Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
