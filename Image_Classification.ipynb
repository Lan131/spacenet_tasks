{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hZu4HqOEt5aP",
    "outputId": "39d7103b-aeab-4545-d225-21113698f8ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x221c3aed230>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from utils_jnb import *\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z1kqZz_v1T2y",
    "outputId": "748f4701-56ea-47f5-f02f-a1e026beceb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uiVw_drJsMwn"
   },
   "outputs": [],
   "source": [
    "directory = \"./src\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2sgdxa1sNbM"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "vgg19 = models.vgg19_bn(pretrained=True)\n",
    "\n",
    "# Freeze model parameters\n",
    "# Same for all\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "vgg19.classifier = nn.Sequential(nn.Linear(25088, 4096),\n",
    "nn.ReLU(),\n",
    "nn.Dropout(0.4),\n",
    "nn.Linear(4096, 1024),\n",
    "nn.ReLU(),\n",
    "nn.Dropout(0.4),\n",
    "nn.Linear(1024, num_classes),\n",
    "nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2X4kzWXv3b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace=True)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace=True)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace=True)\n",
       "    (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (38): ReLU(inplace=True)\n",
       "    (39): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace=True)\n",
       "    (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (45): ReLU(inplace=True)\n",
       "    (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (48): ReLU(inplace=True)\n",
       "    (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (51): ReLU(inplace=True)\n",
       "    (52): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.4, inplace=False)\n",
       "    (6): Linear(in_features=1024, out_features=2, bias=True)\n",
       "    (7): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "lossFunc = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg19.parameters(), lr=1e-2)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "expLrScheduler = lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)\n",
    "\n",
    "vgg19.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K-Sg3anS0PTH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Sequential: 1-1                        --\n",
      "|    └─Conv2d: 2-1                       (1,792)\n",
      "|    └─BatchNorm2d: 2-2                  (128)\n",
      "|    └─ReLU: 2-3                         --\n",
      "|    └─Conv2d: 2-4                       (36,928)\n",
      "|    └─BatchNorm2d: 2-5                  (128)\n",
      "|    └─ReLU: 2-6                         --\n",
      "|    └─MaxPool2d: 2-7                    --\n",
      "|    └─Conv2d: 2-8                       (73,856)\n",
      "|    └─BatchNorm2d: 2-9                  (256)\n",
      "|    └─ReLU: 2-10                        --\n",
      "|    └─Conv2d: 2-11                      (147,584)\n",
      "|    └─BatchNorm2d: 2-12                 (256)\n",
      "|    └─ReLU: 2-13                        --\n",
      "|    └─MaxPool2d: 2-14                   --\n",
      "|    └─Conv2d: 2-15                      (295,168)\n",
      "|    └─BatchNorm2d: 2-16                 (512)\n",
      "|    └─ReLU: 2-17                        --\n",
      "|    └─Conv2d: 2-18                      (590,080)\n",
      "|    └─BatchNorm2d: 2-19                 (512)\n",
      "|    └─ReLU: 2-20                        --\n",
      "|    └─Conv2d: 2-21                      (590,080)\n",
      "|    └─BatchNorm2d: 2-22                 (512)\n",
      "|    └─ReLU: 2-23                        --\n",
      "|    └─Conv2d: 2-24                      (590,080)\n",
      "|    └─BatchNorm2d: 2-25                 (512)\n",
      "|    └─ReLU: 2-26                        --\n",
      "|    └─MaxPool2d: 2-27                   --\n",
      "|    └─Conv2d: 2-28                      (1,180,160)\n",
      "|    └─BatchNorm2d: 2-29                 (1,024)\n",
      "|    └─ReLU: 2-30                        --\n",
      "|    └─Conv2d: 2-31                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-32                 (1,024)\n",
      "|    └─ReLU: 2-33                        --\n",
      "|    └─Conv2d: 2-34                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-35                 (1,024)\n",
      "|    └─ReLU: 2-36                        --\n",
      "|    └─Conv2d: 2-37                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-38                 (1,024)\n",
      "|    └─ReLU: 2-39                        --\n",
      "|    └─MaxPool2d: 2-40                   --\n",
      "|    └─Conv2d: 2-41                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-42                 (1,024)\n",
      "|    └─ReLU: 2-43                        --\n",
      "|    └─Conv2d: 2-44                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-45                 (1,024)\n",
      "|    └─ReLU: 2-46                        --\n",
      "|    └─Conv2d: 2-47                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-48                 (1,024)\n",
      "|    └─ReLU: 2-49                        --\n",
      "|    └─Conv2d: 2-50                      (2,359,808)\n",
      "|    └─BatchNorm2d: 2-51                 (1,024)\n",
      "|    └─ReLU: 2-52                        --\n",
      "|    └─MaxPool2d: 2-53                   --\n",
      "├─AdaptiveAvgPool2d: 1-2                 --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Linear: 2-54                      102,764,544\n",
      "|    └─ReLU: 2-55                        --\n",
      "|    └─Dropout: 2-56                     --\n",
      "|    └─Linear: 2-57                      4,195,328\n",
      "|    └─ReLU: 2-58                        --\n",
      "|    └─Dropout: 2-59                     --\n",
      "|    └─Linear: 2-60                      2,050\n",
      "|    └─LogSoftmax: 2-61                  --\n",
      "=================================================================\n",
      "Total params: 126,997,314\n",
      "Trainable params: 106,961,922\n",
      "Non-trainable params: 20,035,392\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Sequential: 1-1                        --\n",
       "|    └─Conv2d: 2-1                       (1,792)\n",
       "|    └─BatchNorm2d: 2-2                  (128)\n",
       "|    └─ReLU: 2-3                         --\n",
       "|    └─Conv2d: 2-4                       (36,928)\n",
       "|    └─BatchNorm2d: 2-5                  (128)\n",
       "|    └─ReLU: 2-6                         --\n",
       "|    └─MaxPool2d: 2-7                    --\n",
       "|    └─Conv2d: 2-8                       (73,856)\n",
       "|    └─BatchNorm2d: 2-9                  (256)\n",
       "|    └─ReLU: 2-10                        --\n",
       "|    └─Conv2d: 2-11                      (147,584)\n",
       "|    └─BatchNorm2d: 2-12                 (256)\n",
       "|    └─ReLU: 2-13                        --\n",
       "|    └─MaxPool2d: 2-14                   --\n",
       "|    └─Conv2d: 2-15                      (295,168)\n",
       "|    └─BatchNorm2d: 2-16                 (512)\n",
       "|    └─ReLU: 2-17                        --\n",
       "|    └─Conv2d: 2-18                      (590,080)\n",
       "|    └─BatchNorm2d: 2-19                 (512)\n",
       "|    └─ReLU: 2-20                        --\n",
       "|    └─Conv2d: 2-21                      (590,080)\n",
       "|    └─BatchNorm2d: 2-22                 (512)\n",
       "|    └─ReLU: 2-23                        --\n",
       "|    └─Conv2d: 2-24                      (590,080)\n",
       "|    └─BatchNorm2d: 2-25                 (512)\n",
       "|    └─ReLU: 2-26                        --\n",
       "|    └─MaxPool2d: 2-27                   --\n",
       "|    └─Conv2d: 2-28                      (1,180,160)\n",
       "|    └─BatchNorm2d: 2-29                 (1,024)\n",
       "|    └─ReLU: 2-30                        --\n",
       "|    └─Conv2d: 2-31                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-32                 (1,024)\n",
       "|    └─ReLU: 2-33                        --\n",
       "|    └─Conv2d: 2-34                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-35                 (1,024)\n",
       "|    └─ReLU: 2-36                        --\n",
       "|    └─Conv2d: 2-37                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-38                 (1,024)\n",
       "|    └─ReLU: 2-39                        --\n",
       "|    └─MaxPool2d: 2-40                   --\n",
       "|    └─Conv2d: 2-41                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-42                 (1,024)\n",
       "|    └─ReLU: 2-43                        --\n",
       "|    └─Conv2d: 2-44                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-45                 (1,024)\n",
       "|    └─ReLU: 2-46                        --\n",
       "|    └─Conv2d: 2-47                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-48                 (1,024)\n",
       "|    └─ReLU: 2-49                        --\n",
       "|    └─Conv2d: 2-50                      (2,359,808)\n",
       "|    └─BatchNorm2d: 2-51                 (1,024)\n",
       "|    └─ReLU: 2-52                        --\n",
       "|    └─MaxPool2d: 2-53                   --\n",
       "├─AdaptiveAvgPool2d: 1-2                 --\n",
       "├─Sequential: 1-3                        --\n",
       "|    └─Linear: 2-54                      102,764,544\n",
       "|    └─ReLU: 2-55                        --\n",
       "|    └─Dropout: 2-56                     --\n",
       "|    └─Linear: 2-57                      4,195,328\n",
       "|    └─ReLU: 2-58                        --\n",
       "|    └─Dropout: 2-59                     --\n",
       "|    └─Linear: 2-60                      2,050\n",
       "|    └─LogSoftmax: 2-61                  --\n",
       "=================================================================\n",
       "Total params: 126,997,314\n",
       "Trainable params: 106,961,922\n",
       "Non-trainable params: 20,035,392\n",
       "================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vgg19, input_size=(3, 224, 224), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npNkQm7W0Pr5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4996, 556]\n",
      "5552\n",
      "Epoch: 1/25\n",
      "Batch number: 000, Training: Loss: 0.6831, Accuracy: 0.5312\n",
      "Batch number: 001, Training: Loss: 474.5312, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 105.5856, Accuracy: 0.3750\n",
      "Batch number: 003, Training: Loss: 22.3058, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 4.1532, Accuracy: 0.8125\n",
      "Batch number: 005, Training: Loss: 29.2498, Accuracy: 0.3125\n",
      "Batch number: 006, Training: Loss: 7.8342, Accuracy: 0.6562\n",
      "Batch number: 007, Training: Loss: 10.8726, Accuracy: 0.6875\n",
      "Batch number: 008, Training: Loss: 1.8348, Accuracy: 0.8125\n",
      "Batch number: 009, Training: Loss: 22.0495, Accuracy: 0.5000\n",
      "Batch number: 010, Training: Loss: 1.6410, Accuracy: 0.8438\n",
      "Batch number: 011, Training: Loss: 8.5229, Accuracy: 0.6250\n",
      "Batch number: 012, Training: Loss: 8.9124, Accuracy: 0.5625\n",
      "Batch number: 013, Training: Loss: 3.6875, Accuracy: 0.6875\n",
      "Batch number: 014, Training: Loss: 11.8969, Accuracy: 0.5625\n",
      "Batch number: 015, Training: Loss: 0.6116, Accuracy: 0.7812\n",
      "Batch number: 016, Training: Loss: 0.5767, Accuracy: 0.9062\n",
      "Batch number: 017, Training: Loss: 1.0682, Accuracy: 0.8125\n",
      "Batch number: 018, Training: Loss: 4.1511, Accuracy: 0.5000\n",
      "Batch number: 019, Training: Loss: 1.1086, Accuracy: 0.6875\n",
      "Batch number: 020, Training: Loss: 2.0089, Accuracy: 0.6875\n",
      "Batch number: 021, Training: Loss: 4.5160, Accuracy: 0.6250\n",
      "Batch number: 022, Training: Loss: 1.5748, Accuracy: 0.8750\n",
      "Batch number: 023, Training: Loss: 0.9840, Accuracy: 0.7500\n",
      "Batch number: 024, Training: Loss: 0.5367, Accuracy: 0.8125\n",
      "Batch number: 025, Training: Loss: 0.4222, Accuracy: 0.9062\n",
      "Batch number: 026, Training: Loss: 1.0868, Accuracy: 0.7500\n",
      "Batch number: 027, Training: Loss: 0.5124, Accuracy: 0.8438\n",
      "Batch number: 028, Training: Loss: 1.1694, Accuracy: 0.6562\n",
      "Batch number: 029, Training: Loss: 1.2036, Accuracy: 0.7500\n",
      "Batch number: 030, Training: Loss: 0.5112, Accuracy: 0.8438\n",
      "Batch number: 031, Training: Loss: 0.2054, Accuracy: 0.8750\n",
      "Batch number: 032, Training: Loss: 0.6212, Accuracy: 0.7812\n",
      "Batch number: 033, Training: Loss: 0.7487, Accuracy: 0.8438\n",
      "Batch number: 034, Training: Loss: 0.5278, Accuracy: 0.8438\n",
      "Batch number: 035, Training: Loss: 0.2972, Accuracy: 0.8438\n",
      "Batch number: 036, Training: Loss: 0.4955, Accuracy: 0.7500\n",
      "Batch number: 037, Training: Loss: 0.8325, Accuracy: 0.8438\n",
      "Batch number: 038, Training: Loss: 0.5709, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 0.4313, Accuracy: 0.9062\n",
      "Batch number: 040, Training: Loss: 0.2129, Accuracy: 0.9062\n",
      "Batch number: 041, Training: Loss: 0.3443, Accuracy: 0.7812\n",
      "Batch number: 042, Training: Loss: 2.4249, Accuracy: 0.5000\n",
      "Batch number: 043, Training: Loss: 0.4111, Accuracy: 0.9062\n",
      "Batch number: 044, Training: Loss: 0.3787, Accuracy: 0.8125\n",
      "Batch number: 045, Training: Loss: 0.7254, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 0.8148, Accuracy: 0.7500\n",
      "Batch number: 047, Training: Loss: 0.3609, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 0.3888, Accuracy: 0.7500\n",
      "Batch number: 049, Training: Loss: 0.4083, Accuracy: 0.7500\n",
      "Batch number: 050, Training: Loss: 0.4776, Accuracy: 0.8438\n",
      "Batch number: 051, Training: Loss: 0.3397, Accuracy: 0.8750\n",
      "Batch number: 052, Training: Loss: 0.2356, Accuracy: 0.9062\n",
      "Batch number: 053, Training: Loss: 0.4033, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 0.5843, Accuracy: 0.7812\n",
      "Batch number: 055, Training: Loss: 0.4469, Accuracy: 0.7812\n",
      "Batch number: 056, Training: Loss: 0.1149, Accuracy: 0.9688\n",
      "Batch number: 057, Training: Loss: 0.2420, Accuracy: 0.8438\n",
      "Batch number: 058, Training: Loss: 0.4099, Accuracy: 0.7812\n",
      "Batch number: 059, Training: Loss: 0.2927, Accuracy: 0.8750\n",
      "Batch number: 060, Training: Loss: 0.3791, Accuracy: 0.8438\n",
      "Batch number: 061, Training: Loss: 0.5049, Accuracy: 0.8125\n",
      "Batch number: 062, Training: Loss: 0.0785, Accuracy: 0.9688\n",
      "Batch number: 063, Training: Loss: 0.5876, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 0.3921, Accuracy: 0.8438\n",
      "Batch number: 065, Training: Loss: 0.2403, Accuracy: 0.9062\n",
      "Batch number: 066, Training: Loss: 0.4259, Accuracy: 0.8438\n",
      "Batch number: 067, Training: Loss: 0.1729, Accuracy: 0.9062\n",
      "Batch number: 068, Training: Loss: 0.5304, Accuracy: 0.8438\n",
      "Batch number: 069, Training: Loss: 0.1944, Accuracy: 0.9062\n",
      "Batch number: 070, Training: Loss: 0.3605, Accuracy: 0.8438\n",
      "Batch number: 071, Training: Loss: 0.3039, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.2888, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.4794, Accuracy: 0.8438\n",
      "Batch number: 074, Training: Loss: 0.1582, Accuracy: 0.9688\n",
      "Batch number: 075, Training: Loss: 1.7110, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 0.9757, Accuracy: 0.8438\n",
      "Batch number: 077, Training: Loss: 0.7768, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 0.4841, Accuracy: 0.7812\n",
      "Batch number: 079, Training: Loss: 0.8953, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.4298, Accuracy: 0.7500\n",
      "Batch number: 081, Training: Loss: 0.6021, Accuracy: 0.6875\n",
      "Batch number: 082, Training: Loss: 0.3856, Accuracy: 0.8125\n",
      "Batch number: 083, Training: Loss: 0.4151, Accuracy: 0.7188\n",
      "Batch number: 084, Training: Loss: 0.3791, Accuracy: 0.8125\n",
      "Batch number: 085, Training: Loss: 0.3801, Accuracy: 0.8438\n",
      "Batch number: 086, Training: Loss: 0.4171, Accuracy: 0.7188\n",
      "Batch number: 087, Training: Loss: 0.6840, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.2580, Accuracy: 0.9062\n",
      "Batch number: 089, Training: Loss: 0.4665, Accuracy: 0.7812\n",
      "Batch number: 090, Training: Loss: 0.3792, Accuracy: 0.7188\n",
      "Batch number: 091, Training: Loss: 0.4065, Accuracy: 0.7500\n",
      "Batch number: 092, Training: Loss: 0.2690, Accuracy: 0.8438\n",
      "Batch number: 093, Training: Loss: 0.2993, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.3251, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 0.6290, Accuracy: 0.8125\n",
      "Batch number: 096, Training: Loss: 0.1856, Accuracy: 0.9375\n",
      "Batch number: 097, Training: Loss: 0.8766, Accuracy: 0.7812\n",
      "Batch number: 098, Training: Loss: 0.1772, Accuracy: 0.9375\n",
      "Batch number: 099, Training: Loss: 0.2296, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 0.2582, Accuracy: 0.9062\n",
      "Batch number: 101, Training: Loss: 0.5802, Accuracy: 0.7812\n",
      "Batch number: 102, Training: Loss: 0.5749, Accuracy: 0.7188\n",
      "Batch number: 103, Training: Loss: 0.3332, Accuracy: 0.9062\n",
      "Batch number: 104, Training: Loss: 0.6052, Accuracy: 0.6562\n",
      "Batch number: 105, Training: Loss: 0.4453, Accuracy: 0.8438\n",
      "Batch number: 106, Training: Loss: 0.4628, Accuracy: 0.7500\n",
      "Batch number: 107, Training: Loss: 0.4435, Accuracy: 0.7812\n",
      "Batch number: 108, Training: Loss: 0.4691, Accuracy: 0.6875\n",
      "Batch number: 109, Training: Loss: 0.2721, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.5060, Accuracy: 0.8438\n",
      "Batch number: 111, Training: Loss: 0.4441, Accuracy: 0.8125\n",
      "Batch number: 112, Training: Loss: 0.7917, Accuracy: 0.8125\n",
      "Batch number: 113, Training: Loss: 0.6368, Accuracy: 0.7812\n",
      "Batch number: 114, Training: Loss: 0.2807, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.2461, Accuracy: 0.8750\n",
      "Batch number: 116, Training: Loss: 0.6312, Accuracy: 0.6250\n",
      "Batch number: 117, Training: Loss: 0.2560, Accuracy: 0.8438\n",
      "Batch number: 118, Training: Loss: 0.4274, Accuracy: 0.8125\n",
      "Batch number: 119, Training: Loss: 0.3886, Accuracy: 0.8125\n",
      "Batch number: 120, Training: Loss: 0.3151, Accuracy: 0.8750\n",
      "Batch number: 121, Training: Loss: 0.2126, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 0.5218, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.3182, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.5578, Accuracy: 0.7812\n",
      "Batch number: 125, Training: Loss: 0.5916, Accuracy: 0.8125\n",
      "Batch number: 126, Training: Loss: 0.2912, Accuracy: 0.7812\n",
      "Batch number: 127, Training: Loss: 0.3612, Accuracy: 0.8750\n",
      "Batch number: 128, Training: Loss: 0.8143, Accuracy: 0.7500\n",
      "Batch number: 129, Training: Loss: 0.4536, Accuracy: 0.8750\n",
      "Batch number: 130, Training: Loss: 0.6036, Accuracy: 0.7188\n",
      "Batch number: 131, Training: Loss: 0.3466, Accuracy: 0.8750\n",
      "Batch number: 132, Training: Loss: 0.4520, Accuracy: 0.8438\n",
      "Batch number: 133, Training: Loss: 0.3807, Accuracy: 0.7188\n",
      "Batch number: 134, Training: Loss: 0.3688, Accuracy: 0.8125\n",
      "Batch number: 135, Training: Loss: 0.3750, Accuracy: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 136, Training: Loss: 0.3096, Accuracy: 0.8750\n",
      "Batch number: 137, Training: Loss: 0.3415, Accuracy: 0.8750\n",
      "Batch number: 138, Training: Loss: 0.3194, Accuracy: 0.7812\n",
      "Batch number: 139, Training: Loss: 0.4601, Accuracy: 0.8438\n",
      "Batch number: 140, Training: Loss: 0.1489, Accuracy: 0.9375\n",
      "Batch number: 141, Training: Loss: 0.2442, Accuracy: 0.8438\n",
      "Batch number: 142, Training: Loss: 0.2606, Accuracy: 0.8750\n",
      "Batch number: 143, Training: Loss: 0.1885, Accuracy: 0.9375\n",
      "Batch number: 144, Training: Loss: 1.1089, Accuracy: 0.7188\n",
      "Batch number: 145, Training: Loss: 0.5301, Accuracy: 0.7812\n",
      "Batch number: 146, Training: Loss: 0.1305, Accuracy: 0.9375\n",
      "Batch number: 147, Training: Loss: 0.4247, Accuracy: 0.8438\n",
      "Batch number: 148, Training: Loss: 0.2685, Accuracy: 0.9062\n",
      "Batch number: 149, Training: Loss: 0.4062, Accuracy: 0.8750\n",
      "Batch number: 150, Training: Loss: 0.2560, Accuracy: 0.9062\n",
      "Batch number: 151, Training: Loss: 0.3428, Accuracy: 0.8125\n",
      "Batch number: 152, Training: Loss: 0.2749, Accuracy: 0.8750\n",
      "Batch number: 153, Training: Loss: 0.5455, Accuracy: 0.7500\n",
      "Batch number: 154, Training: Loss: 0.3317, Accuracy: 0.8438\n",
      "Batch number: 155, Training: Loss: 0.5168, Accuracy: 0.7500\n",
      "Validation Batch number: 000, Validation: Loss: 0.1541, Accuracy: 1.0000\n",
      "Validation Batch number: 001, Validation: Loss: 0.2859, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.2101, Accuracy: 1.0000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3277, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.8211, Accuracy: 0.5000\n",
      "Validation Batch number: 005, Validation: Loss: 0.1336, Accuracy: 1.0000\n",
      "Validation Batch number: 006, Validation: Loss: 0.4708, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.2242, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.4572, Accuracy: 0.7500\n",
      "Validation Batch number: 009, Validation: Loss: 0.1882, Accuracy: 1.0000\n",
      "Validation Batch number: 010, Validation: Loss: 0.3900, Accuracy: 0.7500\n",
      "Validation Batch number: 011, Validation: Loss: 0.3209, Accuracy: 0.7500\n",
      "Validation Batch number: 012, Validation: Loss: 0.1110, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.1827, Accuracy: 1.0000\n",
      "Validation Batch number: 014, Validation: Loss: 0.4539, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.2927, Accuracy: 0.7500\n",
      "Validation Batch number: 016, Validation: Loss: 0.1573, Accuracy: 1.0000\n",
      "Validation Batch number: 017, Validation: Loss: 0.3334, Accuracy: 1.0000\n",
      "Validation Batch number: 018, Validation: Loss: 0.3017, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.0069, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 0.3565, Accuracy: 1.0000\n",
      "Validation Batch number: 021, Validation: Loss: 0.7110, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.3617, Accuracy: 0.7500\n",
      "Validation Batch number: 023, Validation: Loss: 0.0971, Accuracy: 1.0000\n",
      "Validation Batch number: 024, Validation: Loss: 0.2436, Accuracy: 0.7500\n",
      "Validation Batch number: 025, Validation: Loss: 0.2283, Accuracy: 0.7500\n",
      "Validation Batch number: 026, Validation: Loss: 0.3126, Accuracy: 0.7500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.1453, Accuracy: 1.0000\n",
      "Validation Batch number: 029, Validation: Loss: 0.2635, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.1976, Accuracy: 1.0000\n",
      "Validation Batch number: 031, Validation: Loss: 0.1721, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.0337, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.2800, Accuracy: 1.0000\n",
      "Validation Batch number: 034, Validation: Loss: 0.5442, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.2677, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.6874, Accuracy: 0.5000\n",
      "Validation Batch number: 037, Validation: Loss: 0.0697, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.5101, Accuracy: 0.7500\n",
      "Validation Batch number: 039, Validation: Loss: 0.2448, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.3402, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5154, Accuracy: 0.7500\n",
      "Validation Batch number: 042, Validation: Loss: 1.1939, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.2456, Accuracy: 1.0000\n",
      "Validation Batch number: 044, Validation: Loss: 0.7541, Accuracy: 0.5000\n",
      "Validation Batch number: 045, Validation: Loss: 0.2488, Accuracy: 1.0000\n",
      "Validation Batch number: 046, Validation: Loss: 0.1547, Accuracy: 1.0000\n",
      "Validation Batch number: 047, Validation: Loss: 0.2780, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 1.2871, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.5177, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.1745, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.2700, Accuracy: 0.7500\n",
      "Validation Batch number: 052, Validation: Loss: 0.5845, Accuracy: 0.7500\n",
      "Validation Batch number: 053, Validation: Loss: 1.0820, Accuracy: 0.5000\n",
      "Validation Batch number: 054, Validation: Loss: 0.0344, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 0.1081, Accuracy: 1.0000\n",
      "Validation Batch number: 056, Validation: Loss: 0.0434, Accuracy: 1.0000\n",
      "Validation Batch number: 057, Validation: Loss: 0.0707, Accuracy: 1.0000\n",
      "Validation Batch number: 058, Validation: Loss: 0.5963, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.0794, Accuracy: 1.0000\n",
      "Validation Batch number: 060, Validation: Loss: 0.5365, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.4016, Accuracy: 0.7500\n",
      "Validation Batch number: 062, Validation: Loss: 0.3529, Accuracy: 0.7500\n",
      "Validation Batch number: 063, Validation: Loss: 0.5622, Accuracy: 0.7500\n",
      "Validation Batch number: 064, Validation: Loss: 0.1822, Accuracy: 1.0000\n",
      "Validation Batch number: 065, Validation: Loss: 0.2333, Accuracy: 1.0000\n",
      "Validation Batch number: 066, Validation: Loss: 0.0356, Accuracy: 1.0000\n",
      "Validation Batch number: 067, Validation: Loss: 0.4199, Accuracy: 0.7500\n",
      "Validation Batch number: 068, Validation: Loss: 0.1756, Accuracy: 1.0000\n",
      "Validation Batch number: 069, Validation: Loss: 0.2130, Accuracy: 1.0000\n",
      "Validation Batch number: 070, Validation: Loss: 0.4501, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.1414, Accuracy: 1.0000\n",
      "Validation Batch number: 072, Validation: Loss: 0.3925, Accuracy: 0.7500\n",
      "Validation Batch number: 073, Validation: Loss: 0.3001, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.1125, Accuracy: 1.0000\n",
      "Validation Batch number: 075, Validation: Loss: 0.0273, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.4621, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.1752, Accuracy: 1.0000\n",
      "Validation Batch number: 078, Validation: Loss: 1.0275, Accuracy: 0.5000\n",
      "Validation Batch number: 079, Validation: Loss: 0.1198, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.2472, Accuracy: 1.0000\n",
      "Validation Batch number: 081, Validation: Loss: 0.6216, Accuracy: 0.5000\n",
      "Validation Batch number: 082, Validation: Loss: 0.6775, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.1847, Accuracy: 1.0000\n",
      "Validation Batch number: 084, Validation: Loss: 0.4599, Accuracy: 0.7500\n",
      "Validation Batch number: 085, Validation: Loss: 0.0717, Accuracy: 1.0000\n",
      "Validation Batch number: 086, Validation: Loss: 0.1510, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1534, Accuracy: 1.0000\n",
      "Validation Batch number: 088, Validation: Loss: 0.2328, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.0712, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.1011, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0011, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.3257, Accuracy: 1.0000\n",
      "Validation Batch number: 093, Validation: Loss: 0.2968, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.0675, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.1541, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.3831, Accuracy: 0.7500\n",
      "Validation Batch number: 097, Validation: Loss: 0.0880, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 098, Validation: Loss: 0.2958, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.0001, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.2436, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.1984, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.0046, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.0455, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.2153, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.1183, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.2059, Accuracy: 1.0000\n",
      "Validation Batch number: 107, Validation: Loss: 0.3518, Accuracy: 0.7500\n",
      "Validation Batch number: 108, Validation: Loss: 0.0099, Accuracy: 1.0000\n",
      "Validation Batch number: 109, Validation: Loss: 0.3226, Accuracy: 0.7500\n",
      "Validation Batch number: 110, Validation: Loss: 0.0456, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.1708, Accuracy: 1.0000\n",
      "Validation Batch number: 112, Validation: Loss: 0.2123, Accuracy: 0.7500\n",
      "Validation Batch number: 113, Validation: Loss: 0.3386, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.2347, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.0428, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.1299, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.0800, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.1489, Accuracy: 1.0000\n",
      "Validation Batch number: 119, Validation: Loss: 0.0917, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0019, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.3964, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.1454, Accuracy: 1.0000\n",
      "Validation Batch number: 123, Validation: Loss: 0.0869, Accuracy: 1.0000\n",
      "Validation Batch number: 124, Validation: Loss: 0.3045, Accuracy: 0.7500\n",
      "Validation Batch number: 125, Validation: Loss: 0.0460, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5764, Accuracy: 0.5000\n",
      "Validation Batch number: 127, Validation: Loss: 0.2220, Accuracy: 0.7500\n",
      "Validation Batch number: 128, Validation: Loss: 0.1389, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.3262, Accuracy: 0.7500\n",
      "Validation Batch number: 130, Validation: Loss: 0.1356, Accuracy: 1.0000\n",
      "Validation Batch number: 131, Validation: Loss: 0.1377, Accuracy: 1.0000\n",
      "Validation Batch number: 132, Validation: Loss: 0.1467, Accuracy: 1.0000\n",
      "Validation Batch number: 133, Validation: Loss: 0.0295, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.4160, Accuracy: 1.0000\n",
      "Validation Batch number: 135, Validation: Loss: 0.1713, Accuracy: 1.0000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6088, Accuracy: 0.5000\n",
      "Validation Batch number: 137, Validation: Loss: 0.4432, Accuracy: 0.7500\n",
      "Validation Batch number: 138, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Epoch : 000, Training: Loss : 5.0736, Accuracy: 79.6837%\n",
      "Validation : Loss : 0.2787, Accuracy: 88.1295%, Time: 55.8508s\n",
      "model for epoch 0 saved\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 2/25\n",
      "Batch number: 000, Training: Loss: 17.0065, Accuracy: 0.6562\n",
      "Batch number: 001, Training: Loss: 0.3051, Accuracy: 0.8125\n",
      "Batch number: 002, Training: Loss: 0.3621, Accuracy: 0.7812\n",
      "Batch number: 003, Training: Loss: 1.0263, Accuracy: 0.6875\n",
      "Batch number: 004, Training: Loss: 0.7943, Accuracy: 0.7500\n",
      "Batch number: 005, Training: Loss: 0.9328, Accuracy: 0.7188\n",
      "Batch number: 006, Training: Loss: 2.9421, Accuracy: 0.7188\n",
      "Batch number: 007, Training: Loss: 1.3026, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 0.5125, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.0906, Accuracy: 0.9688\n",
      "Batch number: 010, Training: Loss: 1.1226, Accuracy: 0.7812\n",
      "Batch number: 011, Training: Loss: 1.4559, Accuracy: 0.8125\n",
      "Batch number: 012, Training: Loss: 3.6213, Accuracy: 0.6875\n",
      "Batch number: 013, Training: Loss: 3.2343, Accuracy: 0.6875\n",
      "Batch number: 014, Training: Loss: 1.9490, Accuracy: 0.8438\n",
      "Batch number: 015, Training: Loss: 1.1828, Accuracy: 0.7812\n",
      "Batch number: 016, Training: Loss: 1.0533, Accuracy: 0.8438\n",
      "Batch number: 017, Training: Loss: 0.3248, Accuracy: 0.8438\n",
      "Batch number: 018, Training: Loss: 0.6649, Accuracy: 0.5312\n",
      "Batch number: 019, Training: Loss: 0.5211, Accuracy: 0.7500\n",
      "Batch number: 020, Training: Loss: 1.0537, Accuracy: 0.5938\n",
      "Batch number: 021, Training: Loss: 0.6551, Accuracy: 0.6875\n",
      "Batch number: 022, Training: Loss: 1.2015, Accuracy: 0.7812\n",
      "Batch number: 023, Training: Loss: 0.5581, Accuracy: 0.7812\n",
      "Batch number: 024, Training: Loss: 0.8239, Accuracy: 0.8438\n",
      "Batch number: 025, Training: Loss: 0.4290, Accuracy: 0.7500\n",
      "Batch number: 026, Training: Loss: 0.4031, Accuracy: 0.7188\n",
      "Batch number: 027, Training: Loss: 0.4567, Accuracy: 0.6875\n",
      "Batch number: 028, Training: Loss: 0.8570, Accuracy: 0.6562\n",
      "Batch number: 029, Training: Loss: 0.6359, Accuracy: 0.5938\n",
      "Batch number: 030, Training: Loss: 0.6314, Accuracy: 0.6250\n",
      "Batch number: 031, Training: Loss: 0.6427, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 1.5293, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.7325, Accuracy: 0.7188\n",
      "Batch number: 034, Training: Loss: 0.4263, Accuracy: 0.7500\n",
      "Batch number: 035, Training: Loss: 0.3627, Accuracy: 0.8750\n",
      "Batch number: 036, Training: Loss: 0.8673, Accuracy: 0.7188\n",
      "Batch number: 037, Training: Loss: 0.4657, Accuracy: 0.7812\n",
      "Batch number: 038, Training: Loss: 0.6499, Accuracy: 0.7188\n",
      "Batch number: 039, Training: Loss: 0.4739, Accuracy: 0.8125\n",
      "Batch number: 040, Training: Loss: 0.3486, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.8818, Accuracy: 0.7188\n",
      "Batch number: 042, Training: Loss: 1.0439, Accuracy: 0.6562\n",
      "Batch number: 043, Training: Loss: 0.4186, Accuracy: 0.8125\n",
      "Batch number: 044, Training: Loss: 0.4908, Accuracy: 0.6875\n",
      "Batch number: 045, Training: Loss: 0.4817, Accuracy: 0.6562\n",
      "Batch number: 046, Training: Loss: 0.5472, Accuracy: 0.5938\n",
      "Batch number: 047, Training: Loss: 0.3776, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 0.6737, Accuracy: 0.7812\n",
      "Batch number: 049, Training: Loss: 0.5483, Accuracy: 0.6562\n",
      "Batch number: 050, Training: Loss: 0.5900, Accuracy: 0.8438\n",
      "Batch number: 051, Training: Loss: 0.6142, Accuracy: 0.9062\n",
      "Batch number: 052, Training: Loss: 0.4786, Accuracy: 0.7812\n",
      "Batch number: 053, Training: Loss: 0.3706, Accuracy: 0.7500\n",
      "Batch number: 054, Training: Loss: 0.3914, Accuracy: 0.7500\n",
      "Batch number: 055, Training: Loss: 0.5166, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 0.3892, Accuracy: 0.7812\n",
      "Batch number: 057, Training: Loss: 0.3952, Accuracy: 0.7812\n",
      "Batch number: 058, Training: Loss: 0.5341, Accuracy: 0.7500\n",
      "Batch number: 059, Training: Loss: 0.3749, Accuracy: 0.8438\n",
      "Batch number: 060, Training: Loss: 0.4681, Accuracy: 0.7188\n",
      "Batch number: 061, Training: Loss: 0.3828, Accuracy: 0.7812\n",
      "Batch number: 062, Training: Loss: 0.2473, Accuracy: 0.8750\n",
      "Batch number: 063, Training: Loss: 0.7133, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 0.2951, Accuracy: 0.9062\n",
      "Batch number: 065, Training: Loss: 0.4204, Accuracy: 0.7812\n",
      "Batch number: 066, Training: Loss: 0.3338, Accuracy: 0.8125\n",
      "Batch number: 067, Training: Loss: 0.3807, Accuracy: 0.7500\n",
      "Batch number: 068, Training: Loss: 0.2794, Accuracy: 0.8438\n",
      "Batch number: 069, Training: Loss: 1.1943, Accuracy: 0.8125\n",
      "Batch number: 070, Training: Loss: 0.4764, Accuracy: 0.6875\n",
      "Batch number: 071, Training: Loss: 0.3938, Accuracy: 0.7500\n",
      "Batch number: 072, Training: Loss: 0.3001, Accuracy: 0.9062\n",
      "Batch number: 073, Training: Loss: 1.2230, Accuracy: 0.8438\n",
      "Batch number: 074, Training: Loss: 0.2886, Accuracy: 0.7812\n",
      "Batch number: 075, Training: Loss: 0.6126, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 0.3657, Accuracy: 0.8125\n",
      "Batch number: 077, Training: Loss: 0.3489, Accuracy: 0.8438\n",
      "Batch number: 078, Training: Loss: 0.3437, Accuracy: 0.8438\n",
      "Batch number: 079, Training: Loss: 1.2503, Accuracy: 0.8125\n",
      "Batch number: 080, Training: Loss: 0.6485, Accuracy: 0.8125\n",
      "Batch number: 081, Training: Loss: 1.7112, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 0.7053, Accuracy: 0.5938\n",
      "Batch number: 083, Training: Loss: 0.3396, Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 084, Training: Loss: 0.3979, Accuracy: 0.8125\n",
      "Batch number: 085, Training: Loss: 0.7633, Accuracy: 0.7500\n",
      "Batch number: 086, Training: Loss: 0.4243, Accuracy: 0.7812\n",
      "Batch number: 087, Training: Loss: 0.4536, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.4220, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.4261, Accuracy: 0.7500\n",
      "Batch number: 090, Training: Loss: 0.3542, Accuracy: 0.8125\n",
      "Batch number: 091, Training: Loss: 0.4294, Accuracy: 0.7812\n",
      "Batch number: 092, Training: Loss: 0.2849, Accuracy: 0.8750\n",
      "Batch number: 093, Training: Loss: 0.2998, Accuracy: 0.9062\n",
      "Batch number: 094, Training: Loss: 0.4143, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 0.4634, Accuracy: 0.7500\n",
      "Batch number: 096, Training: Loss: 0.2840, Accuracy: 0.8438\n",
      "Batch number: 097, Training: Loss: 0.3250, Accuracy: 0.8438\n",
      "Batch number: 098, Training: Loss: 0.1747, Accuracy: 1.0000\n",
      "Batch number: 099, Training: Loss: 0.2110, Accuracy: 0.9062\n",
      "Batch number: 100, Training: Loss: 0.2757, Accuracy: 0.8125\n",
      "Batch number: 101, Training: Loss: 0.3711, Accuracy: 0.8125\n",
      "Batch number: 102, Training: Loss: 3.8109, Accuracy: 0.7188\n",
      "Batch number: 103, Training: Loss: 0.3431, Accuracy: 0.7812\n",
      "Batch number: 104, Training: Loss: 0.6938, Accuracy: 0.6875\n",
      "Batch number: 105, Training: Loss: 0.8943, Accuracy: 0.7812\n",
      "Batch number: 106, Training: Loss: 0.5635, Accuracy: 0.6250\n",
      "Batch number: 107, Training: Loss: 0.5872, Accuracy: 0.6562\n",
      "Batch number: 108, Training: Loss: 0.5143, Accuracy: 0.5938\n",
      "Batch number: 109, Training: Loss: 0.5653, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.5254, Accuracy: 0.6875\n",
      "Batch number: 111, Training: Loss: 0.6301, Accuracy: 0.5312\n",
      "Batch number: 112, Training: Loss: 0.7528, Accuracy: 0.5000\n",
      "Batch number: 113, Training: Loss: 0.5745, Accuracy: 0.5625\n",
      "Batch number: 114, Training: Loss: 0.6121, Accuracy: 0.5312\n",
      "Batch number: 115, Training: Loss: 0.4944, Accuracy: 0.6562\n",
      "Batch number: 116, Training: Loss: 0.6268, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.6079, Accuracy: 0.4375\n",
      "Batch number: 118, Training: Loss: 0.8253, Accuracy: 0.4375\n",
      "Batch number: 119, Training: Loss: 0.5683, Accuracy: 0.5625\n",
      "Batch number: 120, Training: Loss: 0.3955, Accuracy: 0.8125\n",
      "Batch number: 121, Training: Loss: 0.4137, Accuracy: 0.6875\n",
      "Batch number: 122, Training: Loss: 0.4569, Accuracy: 0.7188\n",
      "Batch number: 123, Training: Loss: 0.4827, Accuracy: 0.6562\n",
      "Batch number: 124, Training: Loss: 0.4238, Accuracy: 0.8438\n",
      "Batch number: 125, Training: Loss: 0.3377, Accuracy: 0.8438\n",
      "Batch number: 126, Training: Loss: 0.8244, Accuracy: 0.5625\n",
      "Batch number: 127, Training: Loss: 0.4681, Accuracy: 0.7500\n",
      "Batch number: 128, Training: Loss: 1.7411, Accuracy: 0.7500\n",
      "Batch number: 129, Training: Loss: 0.6044, Accuracy: 0.5938\n",
      "Batch number: 130, Training: Loss: 0.6623, Accuracy: 0.3750\n",
      "Batch number: 131, Training: Loss: 0.4493, Accuracy: 0.6875\n",
      "Batch number: 132, Training: Loss: 0.5535, Accuracy: 0.6875\n",
      "Batch number: 133, Training: Loss: 0.6545, Accuracy: 0.4688\n",
      "Batch number: 134, Training: Loss: 0.5500, Accuracy: 0.6250\n",
      "Batch number: 135, Training: Loss: 0.5711, Accuracy: 0.5312\n",
      "Batch number: 136, Training: Loss: 0.4528, Accuracy: 0.6875\n",
      "Batch number: 137, Training: Loss: 0.7813, Accuracy: 0.5938\n",
      "Batch number: 138, Training: Loss: 0.4158, Accuracy: 0.7188\n",
      "Batch number: 139, Training: Loss: 0.5148, Accuracy: 0.6875\n",
      "Batch number: 140, Training: Loss: 0.4761, Accuracy: 0.7188\n",
      "Batch number: 141, Training: Loss: 0.9516, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5054, Accuracy: 0.6250\n",
      "Batch number: 143, Training: Loss: 0.4656, Accuracy: 0.6562\n",
      "Batch number: 144, Training: Loss: 0.6011, Accuracy: 0.5000\n",
      "Batch number: 145, Training: Loss: 0.5193, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5207, Accuracy: 0.5625\n",
      "Batch number: 147, Training: Loss: 0.5167, Accuracy: 0.7188\n",
      "Batch number: 148, Training: Loss: 0.3646, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 0.5120, Accuracy: 0.7812\n",
      "Batch number: 150, Training: Loss: 0.8861, Accuracy: 0.7500\n",
      "Batch number: 151, Training: Loss: 0.5787, Accuracy: 0.6875\n",
      "Batch number: 152, Training: Loss: 0.5782, Accuracy: 0.5625\n",
      "Batch number: 153, Training: Loss: 0.5278, Accuracy: 0.7188\n",
      "Batch number: 154, Training: Loss: 0.5360, Accuracy: 0.4688\n",
      "Batch number: 155, Training: Loss: 0.5359, Accuracy: 0.6562\n",
      "Validation Batch number: 000, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 001, Validation: Loss: 0.6012, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5364, Accuracy: 0.5000\n",
      "Validation Batch number: 005, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.7249, Accuracy: 0.2500\n",
      "Validation Batch number: 007, Validation: Loss: 0.3495, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6825, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 011, Validation: Loss: 0.4725, Accuracy: 0.7500\n",
      "Validation Batch number: 012, Validation: Loss: 0.4041, Accuracy: 0.5000\n",
      "Validation Batch number: 013, Validation: Loss: 0.4643, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 016, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 018, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 019, Validation: Loss: 0.1588, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 0.4763, Accuracy: 1.0000\n",
      "Validation Batch number: 021, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.4763, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.4725, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.5065, Accuracy: 0.7500\n",
      "Validation Batch number: 025, Validation: Loss: 0.1887, Accuracy: 0.7500\n",
      "Validation Batch number: 026, Validation: Loss: 0.4584, Accuracy: 1.0000\n",
      "Validation Batch number: 027, Validation: Loss: 0.0017, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.4725, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.6350, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 031, Validation: Loss: 0.3807, Accuracy: 0.5000\n",
      "Validation Batch number: 032, Validation: Loss: 0.5661, Accuracy: 0.2500\n",
      "Validation Batch number: 033, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 034, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 0.4763, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3175, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3774, Accuracy: 0.5000\n",
      "Validation Batch number: 040, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 042, Validation: Loss: 0.5086, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6144, Accuracy: 0.5000\n",
      "Validation Batch number: 044, Validation: Loss: 0.7249, Accuracy: 0.2500\n",
      "Validation Batch number: 045, Validation: Loss: 0.7249, Accuracy: 0.2500\n",
      "Validation Batch number: 046, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3732, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 050, Validation: Loss: 0.4763, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.3175, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.4725, Accuracy: 0.7500\n",
      "Validation Batch number: 053, Validation: Loss: 0.7249, Accuracy: 0.2500\n",
      "Validation Batch number: 054, Validation: Loss: 0.5293, Accuracy: 0.5000\n",
      "Validation Batch number: 055, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 056, Validation: Loss: 0.3475, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 057, Validation: Loss: 0.5024, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 059, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.4219, Accuracy: 0.5000\n",
      "Validation Batch number: 062, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 063, Validation: Loss: 0.7549, Accuracy: 0.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 065, Validation: Loss: 0.5974, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 068, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 070, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 074, Validation: Loss: 0.4725, Accuracy: 0.7500\n",
      "Validation Batch number: 075, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.6718, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 078, Validation: Loss: 0.6583, Accuracy: 0.5000\n",
      "Validation Batch number: 079, Validation: Loss: 0.3774, Accuracy: 0.5000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 081, Validation: Loss: 0.5807, Accuracy: 0.5000\n",
      "Validation Batch number: 082, Validation: Loss: 0.1894, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.4763, Accuracy: 1.0000\n",
      "Validation Batch number: 085, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.3175, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 0.2224, Accuracy: 1.0000\n",
      "Validation Batch number: 088, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 089, Validation: Loss: 0.4763, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.4180, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3890, Accuracy: 0.5000\n",
      "Validation Batch number: 092, Validation: Loss: 0.7249, Accuracy: 0.2500\n",
      "Validation Batch number: 093, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 094, Validation: Loss: 0.2837, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3175, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3712, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 099, Validation: Loss: 0.1290, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.4763, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.4532, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.3774, Accuracy: 0.5000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3774, Accuracy: 0.5000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3175, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.1680, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.3175, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.3500, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.3930, Accuracy: 1.0000\n",
      "Validation Batch number: 110, Validation: Loss: 0.7549, Accuracy: 0.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 112, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 113, Validation: Loss: 0.7249, Accuracy: 0.2500\n",
      "Validation Batch number: 114, Validation: Loss: 0.4081, Accuracy: 1.0000\n",
      "Validation Batch number: 115, Validation: Loss: 0.3775, Accuracy: 0.5000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3775, Accuracy: 0.5000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5902, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.1896, Accuracy: 0.7500\n",
      "Validation Batch number: 121, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.3479, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5068, Accuracy: 0.7500\n",
      "Validation Batch number: 125, Validation: Loss: 0.3175, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 127, Validation: Loss: 0.5661, Accuracy: 0.2500\n",
      "Validation Batch number: 128, Validation: Loss: 0.3177, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.6312, Accuracy: 0.7500\n",
      "Validation Batch number: 130, Validation: Loss: 0.7249, Accuracy: 0.2500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5362, Accuracy: 0.5000\n",
      "Validation Batch number: 132, Validation: Loss: 0.5062, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.1588, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.6650, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5067, Accuracy: 0.7500\n",
      "Validation Batch number: 136, Validation: Loss: 0.7249, Accuracy: 0.2500\n",
      "Validation Batch number: 137, Validation: Loss: 0.5860, Accuracy: 0.2500\n",
      "Validation Batch number: 138, Validation: Loss: 0.3734, Accuracy: 0.7500\n",
      "Epoch : 001, Training: Loss : 0.7702, Accuracy: 72.8783%\n",
      "Validation : Loss : 0.4867, Accuracy: 68.8849%, Time: 50.7404s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 3/25\n",
      "Batch number: 000, Training: Loss: 3.5680, Accuracy: 0.6562\n",
      "Batch number: 001, Training: Loss: 0.5518, Accuracy: 0.5312\n",
      "Batch number: 002, Training: Loss: 0.6313, Accuracy: 0.6562\n",
      "Batch number: 003, Training: Loss: 0.3922, Accuracy: 0.8125\n",
      "Batch number: 004, Training: Loss: 0.4302, Accuracy: 0.5938\n",
      "Batch number: 005, Training: Loss: 0.5616, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.2565, Accuracy: 0.9688\n",
      "Batch number: 007, Training: Loss: 0.3805, Accuracy: 0.7500\n",
      "Batch number: 008, Training: Loss: 1.2082, Accuracy: 0.6875\n",
      "Batch number: 009, Training: Loss: 0.7132, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 0.4165, Accuracy: 0.6250\n",
      "Batch number: 011, Training: Loss: 0.6003, Accuracy: 0.7812\n",
      "Batch number: 012, Training: Loss: 0.4116, Accuracy: 0.8125\n",
      "Batch number: 013, Training: Loss: 0.3000, Accuracy: 0.8125\n",
      "Batch number: 014, Training: Loss: 0.9561, Accuracy: 0.7500\n",
      "Batch number: 015, Training: Loss: 0.3467, Accuracy: 0.8125\n",
      "Batch number: 016, Training: Loss: 0.9605, Accuracy: 0.6562\n",
      "Batch number: 017, Training: Loss: 0.3347, Accuracy: 0.9062\n",
      "Batch number: 018, Training: Loss: 0.4276, Accuracy: 0.6875\n",
      "Batch number: 019, Training: Loss: 0.8022, Accuracy: 0.5312\n",
      "Batch number: 020, Training: Loss: 0.5083, Accuracy: 0.7812\n",
      "Batch number: 021, Training: Loss: 0.5032, Accuracy: 0.5938\n",
      "Batch number: 022, Training: Loss: 0.3437, Accuracy: 0.8438\n",
      "Batch number: 023, Training: Loss: 0.3613, Accuracy: 0.8438\n",
      "Batch number: 024, Training: Loss: 0.4519, Accuracy: 0.7500\n",
      "Batch number: 025, Training: Loss: 0.3704, Accuracy: 0.7188\n",
      "Batch number: 026, Training: Loss: 2.3748, Accuracy: 0.7812\n",
      "Batch number: 027, Training: Loss: 0.3903, Accuracy: 0.7188\n",
      "Batch number: 028, Training: Loss: 1.0659, Accuracy: 0.6562\n",
      "Batch number: 029, Training: Loss: 0.4439, Accuracy: 0.6250\n",
      "Batch number: 030, Training: Loss: 0.3380, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.4093, Accuracy: 0.8750\n",
      "Batch number: 032, Training: Loss: 0.9528, Accuracy: 0.7812\n",
      "Batch number: 033, Training: Loss: 0.5503, Accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 034, Training: Loss: 0.4360, Accuracy: 0.7500\n",
      "Batch number: 035, Training: Loss: 0.5810, Accuracy: 0.7500\n",
      "Batch number: 036, Training: Loss: 0.4916, Accuracy: 0.8438\n",
      "Batch number: 037, Training: Loss: 0.3769, Accuracy: 0.8125\n",
      "Batch number: 038, Training: Loss: 0.5521, Accuracy: 0.7812\n",
      "Batch number: 039, Training: Loss: 0.4432, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.3906, Accuracy: 0.7812\n",
      "Batch number: 041, Training: Loss: 0.3779, Accuracy: 0.8438\n",
      "Batch number: 042, Training: Loss: 0.5576, Accuracy: 0.7188\n",
      "Batch number: 043, Training: Loss: 0.5125, Accuracy: 0.6562\n",
      "Batch number: 044, Training: Loss: 0.3694, Accuracy: 0.8125\n",
      "Batch number: 045, Training: Loss: 2.3760, Accuracy: 0.6875\n",
      "Batch number: 046, Training: Loss: 1.0825, Accuracy: 0.6875\n",
      "Batch number: 047, Training: Loss: 0.4670, Accuracy: 0.8438\n",
      "Batch number: 048, Training: Loss: 0.3921, Accuracy: 0.8125\n",
      "Batch number: 049, Training: Loss: 0.5242, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 0.4853, Accuracy: 0.7188\n",
      "Batch number: 051, Training: Loss: 0.4234, Accuracy: 0.7500\n",
      "Batch number: 052, Training: Loss: 0.4012, Accuracy: 0.8438\n",
      "Batch number: 053, Training: Loss: 0.5210, Accuracy: 0.6875\n",
      "Batch number: 054, Training: Loss: 0.4455, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 1.5103, Accuracy: 0.8438\n",
      "Batch number: 056, Training: Loss: 0.4242, Accuracy: 0.7188\n",
      "Batch number: 057, Training: Loss: 0.4824, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 0.4995, Accuracy: 0.6562\n",
      "Batch number: 059, Training: Loss: 0.4433, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.5178, Accuracy: 0.5625\n",
      "Batch number: 061, Training: Loss: 0.4407, Accuracy: 0.7500\n",
      "Batch number: 062, Training: Loss: 0.4885, Accuracy: 0.5938\n",
      "Batch number: 063, Training: Loss: 0.4366, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 0.6056, Accuracy: 0.7812\n",
      "Batch number: 065, Training: Loss: 0.4441, Accuracy: 0.7500\n",
      "Batch number: 066, Training: Loss: 0.4733, Accuracy: 0.7500\n",
      "Batch number: 067, Training: Loss: 0.3960, Accuracy: 0.7500\n",
      "Batch number: 068, Training: Loss: 0.4189, Accuracy: 0.8125\n",
      "Batch number: 069, Training: Loss: 0.3900, Accuracy: 0.8750\n",
      "Batch number: 070, Training: Loss: 0.4968, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.3708, Accuracy: 0.8750\n",
      "Batch number: 072, Training: Loss: 0.4505, Accuracy: 0.7500\n",
      "Batch number: 073, Training: Loss: 1.0454, Accuracy: 0.7188\n",
      "Batch number: 074, Training: Loss: 1.0898, Accuracy: 0.5625\n",
      "Batch number: 075, Training: Loss: 0.4921, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 0.3331, Accuracy: 0.8125\n",
      "Batch number: 077, Training: Loss: 0.3665, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 0.6388, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 0.5872, Accuracy: 0.5625\n",
      "Batch number: 080, Training: Loss: 0.4597, Accuracy: 0.5938\n",
      "Batch number: 081, Training: Loss: 0.4943, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 0.4357, Accuracy: 0.6562\n",
      "Batch number: 083, Training: Loss: 0.4213, Accuracy: 0.7188\n",
      "Batch number: 084, Training: Loss: 0.3152, Accuracy: 0.8125\n",
      "Batch number: 085, Training: Loss: 0.9997, Accuracy: 0.5938\n",
      "Batch number: 086, Training: Loss: 0.3332, Accuracy: 0.8438\n",
      "Batch number: 087, Training: Loss: 0.3802, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 0.3539, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 2.4330, Accuracy: 0.7812\n",
      "Batch number: 090, Training: Loss: 0.4318, Accuracy: 0.7188\n",
      "Batch number: 091, Training: Loss: 0.4069, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.4007, Accuracy: 0.7500\n",
      "Batch number: 093, Training: Loss: 0.8033, Accuracy: 0.6875\n",
      "Batch number: 094, Training: Loss: 0.4596, Accuracy: 0.7812\n",
      "Batch number: 095, Training: Loss: 0.4094, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.4161, Accuracy: 0.7188\n",
      "Batch number: 097, Training: Loss: 0.3856, Accuracy: 0.8438\n",
      "Batch number: 098, Training: Loss: 0.3806, Accuracy: 0.8125\n",
      "Batch number: 099, Training: Loss: 0.4256, Accuracy: 0.7188\n",
      "Batch number: 100, Training: Loss: 0.4180, Accuracy: 0.7188\n",
      "Batch number: 101, Training: Loss: 0.4214, Accuracy: 0.7188\n",
      "Batch number: 102, Training: Loss: 0.4348, Accuracy: 0.7812\n",
      "Batch number: 103, Training: Loss: 0.3749, Accuracy: 0.7500\n",
      "Batch number: 104, Training: Loss: 0.4617, Accuracy: 0.7500\n",
      "Batch number: 105, Training: Loss: 0.3876, Accuracy: 0.8750\n",
      "Batch number: 106, Training: Loss: 0.4951, Accuracy: 0.5312\n",
      "Batch number: 107, Training: Loss: 1.2878, Accuracy: 0.6875\n",
      "Batch number: 108, Training: Loss: 0.4212, Accuracy: 0.6562\n",
      "Batch number: 109, Training: Loss: 0.2748, Accuracy: 0.9375\n",
      "Batch number: 110, Training: Loss: 0.4274, Accuracy: 0.6875\n",
      "Batch number: 111, Training: Loss: 0.7684, Accuracy: 0.6875\n",
      "Batch number: 112, Training: Loss: 0.4568, Accuracy: 0.7188\n",
      "Batch number: 113, Training: Loss: 0.4505, Accuracy: 0.7188\n",
      "Batch number: 114, Training: Loss: 0.4656, Accuracy: 0.7188\n",
      "Batch number: 115, Training: Loss: 0.6627, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.6542, Accuracy: 0.6875\n",
      "Batch number: 117, Training: Loss: 0.5926, Accuracy: 0.5938\n",
      "Batch number: 118, Training: Loss: 0.4661, Accuracy: 0.6562\n",
      "Batch number: 119, Training: Loss: 0.4110, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.4271, Accuracy: 0.7812\n",
      "Batch number: 121, Training: Loss: 0.4756, Accuracy: 0.6562\n",
      "Batch number: 122, Training: Loss: 0.3611, Accuracy: 0.8438\n",
      "Batch number: 123, Training: Loss: 0.3932, Accuracy: 0.7500\n",
      "Batch number: 124, Training: Loss: 0.7180, Accuracy: 0.6875\n",
      "Batch number: 125, Training: Loss: 0.2635, Accuracy: 0.9375\n",
      "Batch number: 126, Training: Loss: 0.4721, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.5001, Accuracy: 0.6875\n",
      "Batch number: 128, Training: Loss: 0.4412, Accuracy: 0.8125\n",
      "Batch number: 129, Training: Loss: 0.4718, Accuracy: 0.6875\n",
      "Batch number: 130, Training: Loss: 0.5035, Accuracy: 0.5625\n",
      "Batch number: 131, Training: Loss: 0.6930, Accuracy: 0.8125\n",
      "Batch number: 132, Training: Loss: 0.4433, Accuracy: 0.7812\n",
      "Batch number: 133, Training: Loss: 0.3842, Accuracy: 0.7500\n",
      "Batch number: 134, Training: Loss: 0.3746, Accuracy: 0.8438\n",
      "Batch number: 135, Training: Loss: 0.4872, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.4485, Accuracy: 0.7188\n",
      "Batch number: 137, Training: Loss: 0.3718, Accuracy: 0.8125\n",
      "Batch number: 138, Training: Loss: 0.3378, Accuracy: 0.8438\n",
      "Batch number: 139, Training: Loss: 0.4319, Accuracy: 0.7188\n",
      "Batch number: 140, Training: Loss: 0.4402, Accuracy: 0.8438\n",
      "Batch number: 141, Training: Loss: 0.4320, Accuracy: 0.7188\n",
      "Batch number: 142, Training: Loss: 0.5008, Accuracy: 0.5938\n",
      "Batch number: 143, Training: Loss: 0.3651, Accuracy: 0.8125\n",
      "Batch number: 144, Training: Loss: 0.5269, Accuracy: 0.5938\n",
      "Batch number: 145, Training: Loss: 0.4929, Accuracy: 0.6875\n",
      "Batch number: 146, Training: Loss: 0.3458, Accuracy: 0.8125\n",
      "Batch number: 147, Training: Loss: 0.4116, Accuracy: 0.7812\n",
      "Batch number: 148, Training: Loss: 0.3597, Accuracy: 0.8750\n",
      "Batch number: 149, Training: Loss: 0.4113, Accuracy: 0.7812\n",
      "Batch number: 150, Training: Loss: 1.1652, Accuracy: 0.8750\n",
      "Batch number: 151, Training: Loss: 0.3653, Accuracy: 0.8125\n",
      "Batch number: 152, Training: Loss: 0.3824, Accuracy: 0.7812\n",
      "Batch number: 153, Training: Loss: 1.6938, Accuracy: 0.8438\n",
      "Batch number: 154, Training: Loss: 0.4000, Accuracy: 0.6875\n",
      "Batch number: 155, Training: Loss: 0.4212, Accuracy: 0.7812\n",
      "Validation Batch number: 000, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 001, Validation: Loss: 0.5186, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 005, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.5821, Accuracy: 0.5000\n",
      "Validation Batch number: 007, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 011, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 012, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 013, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 016, Validation: Loss: 0.6151, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 017, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 018, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 019, Validation: Loss: 0.1296, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 021, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 025, Validation: Loss: 0.2262, Accuracy: 0.7500\n",
      "Validation Batch number: 026, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.3655, Accuracy: 1.0000\n",
      "Validation Batch number: 029, Validation: Loss: 0.5186, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.3593, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.2266, Accuracy: 0.7500\n",
      "Validation Batch number: 032, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 034, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 037, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 040, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 042, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.5996, Accuracy: 0.5000\n",
      "Validation Batch number: 044, Validation: Loss: 0.8082, Accuracy: 0.2500\n",
      "Validation Batch number: 045, Validation: Loss: 0.8082, Accuracy: 0.2500\n",
      "Validation Batch number: 046, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 050, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 053, Validation: Loss: 0.8082, Accuracy: 0.2500\n",
      "Validation Batch number: 054, Validation: Loss: 0.3592, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.4259, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 059, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 062, Validation: Loss: 0.3895, Accuracy: 1.0000\n",
      "Validation Batch number: 063, Validation: Loss: 0.5544, Accuracy: 0.5000\n",
      "Validation Batch number: 064, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 068, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 070, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.2703, Accuracy: 1.0000\n",
      "Validation Batch number: 074, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 075, Validation: Loss: 0.1296, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 078, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 079, Validation: Loss: 0.2264, Accuracy: 0.7500\n",
      "Validation Batch number: 080, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 082, Validation: Loss: 0.2262, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 085, Validation: Loss: 0.3609, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1296, Accuracy: 1.0000\n",
      "Validation Batch number: 088, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 092, Validation: Loss: 0.8082, Accuracy: 0.2500\n",
      "Validation Batch number: 093, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 094, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.2263, Accuracy: 0.7500\n",
      "Validation Batch number: 100, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 103, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 104, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.1296, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.3577, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.2599, Accuracy: 1.0000\n",
      "Validation Batch number: 110, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 112, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 113, Validation: Loss: 0.8082, Accuracy: 0.2500\n",
      "Validation Batch number: 114, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 115, Validation: Loss: 0.6786, Accuracy: 0.2500\n",
      "Validation Batch number: 116, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 117, Validation: Loss: 0.1297, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.3889, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.0124, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 124, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 125, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 127, Validation: Loss: 0.4524, Accuracy: 0.5000\n",
      "Validation Batch number: 128, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 130, Validation: Loss: 0.8082, Accuracy: 0.2500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 132, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.1296, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 134, Validation: Loss: 0.6151, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 136, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 137, Validation: Loss: 0.6786, Accuracy: 0.2500\n",
      "Validation Batch number: 138, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Epoch : 002, Training: Loss : 0.5672, Accuracy: 73.9592%\n",
      "Validation : Loss : 0.4428, Accuracy: 73.3813%, Time: 50.9432s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 4/25\n",
      "Batch number: 000, Training: Loss: 0.3218, Accuracy: 0.8125\n",
      "Batch number: 001, Training: Loss: 0.5007, Accuracy: 0.6250\n",
      "Batch number: 002, Training: Loss: 0.4522, Accuracy: 0.7188\n",
      "Batch number: 003, Training: Loss: 0.4172, Accuracy: 0.7500\n",
      "Batch number: 004, Training: Loss: 0.9574, Accuracy: 0.5312\n",
      "Batch number: 005, Training: Loss: 0.3751, Accuracy: 0.7188\n",
      "Batch number: 006, Training: Loss: 0.4392, Accuracy: 0.7500\n",
      "Batch number: 007, Training: Loss: 0.3630, Accuracy: 0.7812\n",
      "Batch number: 008, Training: Loss: 0.9815, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.2912, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 0.3972, Accuracy: 0.6875\n",
      "Batch number: 011, Training: Loss: 0.3101, Accuracy: 0.9062\n",
      "Batch number: 012, Training: Loss: 0.5244, Accuracy: 0.8125\n",
      "Batch number: 013, Training: Loss: 0.3400, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 0.4605, Accuracy: 0.8438\n",
      "Batch number: 015, Training: Loss: 0.3486, Accuracy: 0.8125\n",
      "Batch number: 016, Training: Loss: 3.4114, Accuracy: 0.8750\n",
      "Batch number: 017, Training: Loss: 0.3750, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.4870, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 0.5418, Accuracy: 0.5625\n",
      "Batch number: 020, Training: Loss: 0.4449, Accuracy: 0.8125\n",
      "Batch number: 021, Training: Loss: 0.5439, Accuracy: 0.5938\n",
      "Batch number: 022, Training: Loss: 0.4797, Accuracy: 0.7188\n",
      "Batch number: 023, Training: Loss: 0.4429, Accuracy: 0.7812\n",
      "Batch number: 024, Training: Loss: 0.5269, Accuracy: 0.6562\n",
      "Batch number: 025, Training: Loss: 0.3960, Accuracy: 0.7500\n",
      "Batch number: 026, Training: Loss: 0.4723, Accuracy: 0.6875\n",
      "Batch number: 027, Training: Loss: 0.5372, Accuracy: 0.6250\n",
      "Batch number: 028, Training: Loss: 0.4529, Accuracy: 0.7500\n",
      "Batch number: 029, Training: Loss: 0.5085, Accuracy: 0.5938\n",
      "Batch number: 030, Training: Loss: 0.5544, Accuracy: 0.5938\n",
      "Batch number: 031, Training: Loss: 0.4250, Accuracy: 0.7812\n",
      "Batch number: 032, Training: Loss: 0.4619, Accuracy: 0.8125\n",
      "Batch number: 033, Training: Loss: 0.3918, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 0.3974, Accuracy: 0.8438\n",
      "Batch number: 035, Training: Loss: 0.5088, Accuracy: 0.6875\n",
      "Batch number: 036, Training: Loss: 0.6835, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.5253, Accuracy: 0.7812\n",
      "Batch number: 038, Training: Loss: 0.8574, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 0.3305, Accuracy: 0.8125\n",
      "Batch number: 040, Training: Loss: 0.2949, Accuracy: 0.8750\n",
      "Batch number: 041, Training: Loss: 0.4182, Accuracy: 0.7812\n",
      "Batch number: 042, Training: Loss: 1.0600, Accuracy: 0.7812\n",
      "Batch number: 043, Training: Loss: 0.8084, Accuracy: 0.6250\n",
      "Batch number: 044, Training: Loss: 1.1376, Accuracy: 0.7500\n",
      "Batch number: 045, Training: Loss: 0.4314, Accuracy: 0.6875\n",
      "Batch number: 046, Training: Loss: 1.4157, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.3283, Accuracy: 0.9062\n",
      "Batch number: 048, Training: Loss: 0.5552, Accuracy: 0.7188\n",
      "Batch number: 049, Training: Loss: 0.6204, Accuracy: 0.5938\n",
      "Batch number: 050, Training: Loss: 0.4811, Accuracy: 0.7188\n",
      "Batch number: 051, Training: Loss: 0.4208, Accuracy: 0.7500\n",
      "Batch number: 052, Training: Loss: 0.4963, Accuracy: 0.6875\n",
      "Batch number: 053, Training: Loss: 0.5742, Accuracy: 0.6250\n",
      "Batch number: 054, Training: Loss: 0.4989, Accuracy: 0.6562\n",
      "Batch number: 055, Training: Loss: 0.3968, Accuracy: 0.8438\n",
      "Batch number: 056, Training: Loss: 1.1015, Accuracy: 0.5312\n",
      "Batch number: 057, Training: Loss: 0.6118, Accuracy: 0.5312\n",
      "Batch number: 058, Training: Loss: 0.4800, Accuracy: 0.6875\n",
      "Batch number: 059, Training: Loss: 0.4966, Accuracy: 0.7188\n",
      "Batch number: 060, Training: Loss: 0.5728, Accuracy: 0.5312\n",
      "Batch number: 061, Training: Loss: 0.4501, Accuracy: 0.7188\n",
      "Batch number: 062, Training: Loss: 0.5626, Accuracy: 0.5312\n",
      "Batch number: 063, Training: Loss: 0.4721, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 0.4116, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.4377, Accuracy: 0.7500\n",
      "Batch number: 066, Training: Loss: 0.4250, Accuracy: 0.7812\n",
      "Batch number: 067, Training: Loss: 0.4447, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.4436, Accuracy: 0.7812\n",
      "Batch number: 069, Training: Loss: 0.4082, Accuracy: 0.7500\n",
      "Batch number: 070, Training: Loss: 0.4939, Accuracy: 0.6875\n",
      "Batch number: 071, Training: Loss: 0.5170, Accuracy: 0.6875\n",
      "Batch number: 072, Training: Loss: 0.3708, Accuracy: 0.8438\n",
      "Batch number: 073, Training: Loss: 0.3987, Accuracy: 0.8438\n",
      "Batch number: 074, Training: Loss: 0.4320, Accuracy: 0.6250\n",
      "Batch number: 075, Training: Loss: 0.4059, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 1.1266, Accuracy: 0.7500\n",
      "Batch number: 077, Training: Loss: 0.3652, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 0.4053, Accuracy: 0.7500\n",
      "Batch number: 079, Training: Loss: 0.6456, Accuracy: 0.6875\n",
      "Batch number: 080, Training: Loss: 0.4362, Accuracy: 0.6562\n",
      "Batch number: 081, Training: Loss: 0.7391, Accuracy: 0.8438\n",
      "Batch number: 082, Training: Loss: 0.3121, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 0.4432, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.3877, Accuracy: 0.7500\n",
      "Batch number: 085, Training: Loss: 0.3598, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.4430, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.4153, Accuracy: 0.7188\n",
      "Batch number: 088, Training: Loss: 0.4928, Accuracy: 0.5938\n",
      "Batch number: 089, Training: Loss: 0.4594, Accuracy: 0.6875\n",
      "Batch number: 090, Training: Loss: 0.5420, Accuracy: 0.5938\n",
      "Batch number: 091, Training: Loss: 0.4331, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 0.3917, Accuracy: 0.7500\n",
      "Batch number: 093, Training: Loss: 0.3788, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.4325, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 0.3981, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.4148, Accuracy: 0.7188\n",
      "Batch number: 097, Training: Loss: 0.4439, Accuracy: 0.7812\n",
      "Batch number: 098, Training: Loss: 0.3828, Accuracy: 0.8125\n",
      "Batch number: 099, Training: Loss: 0.4317, Accuracy: 0.7188\n",
      "Batch number: 100, Training: Loss: 0.4042, Accuracy: 0.7500\n",
      "Batch number: 101, Training: Loss: 0.4425, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 1.5931, Accuracy: 0.7188\n",
      "Batch number: 103, Training: Loss: 0.3766, Accuracy: 0.7812\n",
      "Batch number: 104, Training: Loss: 0.4541, Accuracy: 0.7500\n",
      "Batch number: 105, Training: Loss: 0.4489, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 0.5316, Accuracy: 0.5312\n",
      "Batch number: 107, Training: Loss: 0.4596, Accuracy: 0.7812\n",
      "Batch number: 108, Training: Loss: 0.4829, Accuracy: 0.6250\n",
      "Batch number: 109, Training: Loss: 0.4928, Accuracy: 0.7812\n",
      "Batch number: 110, Training: Loss: 0.3654, Accuracy: 0.8125\n",
      "Batch number: 111, Training: Loss: 0.5041, Accuracy: 0.6562\n",
      "Batch number: 112, Training: Loss: 0.4208, Accuracy: 0.7500\n",
      "Batch number: 113, Training: Loss: 0.5319, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5874, Accuracy: 0.5625\n",
      "Batch number: 115, Training: Loss: 0.4763, Accuracy: 0.6875\n",
      "Batch number: 116, Training: Loss: 0.4540, Accuracy: 0.7500\n",
      "Batch number: 117, Training: Loss: 0.5036, Accuracy: 0.5625\n",
      "Batch number: 118, Training: Loss: 0.4811, Accuracy: 0.6250\n",
      "Batch number: 119, Training: Loss: 0.4424, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.4309, Accuracy: 0.7812\n",
      "Batch number: 121, Training: Loss: 0.4281, Accuracy: 0.6875\n",
      "Batch number: 122, Training: Loss: 0.4226, Accuracy: 0.7500\n",
      "Batch number: 123, Training: Loss: 0.4683, Accuracy: 0.6562\n",
      "Batch number: 124, Training: Loss: 0.4358, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 0.4491, Accuracy: 0.7188\n",
      "Batch number: 126, Training: Loss: 0.4506, Accuracy: 0.6562\n",
      "Batch number: 127, Training: Loss: 0.3855, Accuracy: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 128, Training: Loss: 4.8398, Accuracy: 0.7500\n",
      "Batch number: 129, Training: Loss: 0.4309, Accuracy: 0.7188\n",
      "Batch number: 130, Training: Loss: 0.4950, Accuracy: 0.5625\n",
      "Batch number: 131, Training: Loss: 0.4308, Accuracy: 0.7188\n",
      "Batch number: 132, Training: Loss: 0.4215, Accuracy: 0.8125\n",
      "Batch number: 133, Training: Loss: 0.4994, Accuracy: 0.6250\n",
      "Batch number: 134, Training: Loss: 0.4037, Accuracy: 0.8125\n",
      "Batch number: 135, Training: Loss: 0.4387, Accuracy: 0.7188\n",
      "Batch number: 136, Training: Loss: 0.3953, Accuracy: 0.7812\n",
      "Batch number: 137, Training: Loss: 0.3492, Accuracy: 0.8438\n",
      "Batch number: 138, Training: Loss: 0.4102, Accuracy: 0.7812\n",
      "Batch number: 139, Training: Loss: 0.5812, Accuracy: 0.6875\n",
      "Batch number: 140, Training: Loss: 0.3498, Accuracy: 0.8438\n",
      "Batch number: 141, Training: Loss: 0.7130, Accuracy: 0.6875\n",
      "Batch number: 142, Training: Loss: 0.4845, Accuracy: 0.5938\n",
      "Batch number: 143, Training: Loss: 0.3421, Accuracy: 0.8438\n",
      "Batch number: 144, Training: Loss: 0.5194, Accuracy: 0.5938\n",
      "Batch number: 145, Training: Loss: 0.4921, Accuracy: 0.6875\n",
      "Batch number: 146, Training: Loss: 0.5191, Accuracy: 0.5938\n",
      "Batch number: 147, Training: Loss: 0.4150, Accuracy: 0.7812\n",
      "Batch number: 148, Training: Loss: 0.4744, Accuracy: 0.7500\n",
      "Batch number: 149, Training: Loss: 0.4664, Accuracy: 0.7188\n",
      "Batch number: 150, Training: Loss: 0.5624, Accuracy: 0.6250\n",
      "Batch number: 151, Training: Loss: 0.4699, Accuracy: 0.6875\n",
      "Batch number: 152, Training: Loss: 0.5360, Accuracy: 0.5938\n",
      "Batch number: 153, Training: Loss: 0.5010, Accuracy: 0.7188\n",
      "Batch number: 154, Training: Loss: 0.5724, Accuracy: 0.5312\n",
      "Batch number: 155, Training: Loss: 0.4751, Accuracy: 0.7500\n",
      "Validation Batch number: 000, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 001, Validation: Loss: 0.5678, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.7021, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 005, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 007, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.7021, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 011, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 012, Validation: Loss: 0.4182, Accuracy: 0.5000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 014, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 016, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 017, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 018, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 019, Validation: Loss: 0.1420, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 021, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 025, Validation: Loss: 0.2091, Accuracy: 0.7500\n",
      "Validation Batch number: 026, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.5678, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 031, Validation: Loss: 0.4199, Accuracy: 0.5000\n",
      "Validation Batch number: 032, Validation: Loss: 0.6273, Accuracy: 0.2500\n",
      "Validation Batch number: 033, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 034, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 037, Validation: Loss: 0.2839, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.7021, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.4182, Accuracy: 0.5000\n",
      "Validation Batch number: 040, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 042, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 043, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 044, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 045, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 046, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 050, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.2839, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 053, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 054, Validation: Loss: 0.3519, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 056, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 058, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 059, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.4182, Accuracy: 0.5000\n",
      "Validation Batch number: 062, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 063, Validation: Loss: 0.8364, Accuracy: 0.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.3597, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 068, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 070, Validation: Loss: 0.7021, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.7021, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 074, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 075, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 078, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 079, Validation: Loss: 0.4182, Accuracy: 0.5000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 081, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 082, Validation: Loss: 0.2091, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 085, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.2839, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1420, Accuracy: 1.0000\n",
      "Validation Batch number: 088, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 089, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.2839, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 091, Validation: Loss: 0.6273, Accuracy: 0.2500\n",
      "Validation Batch number: 092, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 093, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 094, Validation: Loss: 0.2839, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.2839, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.7021, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 099, Validation: Loss: 0.6273, Accuracy: 0.2500\n",
      "Validation Batch number: 100, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.4182, Accuracy: 0.5000\n",
      "Validation Batch number: 103, Validation: Loss: 0.4182, Accuracy: 0.5000\n",
      "Validation Batch number: 104, Validation: Loss: 0.2839, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 108, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 109, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 110, Validation: Loss: 0.8364, Accuracy: 0.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 112, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 113, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 114, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.6273, Accuracy: 0.2500\n",
      "Validation Batch number: 116, Validation: Loss: 0.4182, Accuracy: 0.5000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.4259, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.4182, Accuracy: 0.5000\n",
      "Validation Batch number: 121, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.3511, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 124, Validation: Loss: 0.7021, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.2839, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 127, Validation: Loss: 0.6273, Accuracy: 0.2500\n",
      "Validation Batch number: 128, Validation: Loss: 0.2839, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 130, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Validation Batch number: 132, Validation: Loss: 0.4930, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.1420, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.6350, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 0.7021, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.7693, Accuracy: 0.2500\n",
      "Validation Batch number: 137, Validation: Loss: 0.8364, Accuracy: 0.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5602, Accuracy: 0.5000\n",
      "Epoch : 003, Training: Loss : 0.5438, Accuracy: 72.4980%\n",
      "Validation : Loss : 0.5045, Accuracy: 65.8273%, Time: 50.9229s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 5/25\n",
      "Batch number: 000, Training: Loss: 0.6892, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5170, Accuracy: 0.5938\n",
      "Batch number: 002, Training: Loss: 0.4482, Accuracy: 0.7188\n",
      "Batch number: 003, Training: Loss: 0.6634, Accuracy: 0.6562\n",
      "Batch number: 004, Training: Loss: 1.5708, Accuracy: 0.5000\n",
      "Batch number: 005, Training: Loss: 2.1740, Accuracy: 0.5938\n",
      "Batch number: 006, Training: Loss: 1.8858, Accuracy: 0.7188\n",
      "Batch number: 007, Training: Loss: 0.2957, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 0.6937, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 1.0924, Accuracy: 0.8750\n",
      "Batch number: 010, Training: Loss: 0.3276, Accuracy: 0.6875\n",
      "Batch number: 011, Training: Loss: 1.8415, Accuracy: 0.8438\n",
      "Batch number: 012, Training: Loss: 1.6458, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 8.7856, Accuracy: 0.6250\n",
      "Batch number: 014, Training: Loss: 16.3553, Accuracy: 0.8125\n",
      "Batch number: 015, Training: Loss: 0.3782, Accuracy: 0.7500\n",
      "Batch number: 016, Training: Loss: 0.4408, Accuracy: 0.7812\n",
      "Batch number: 017, Training: Loss: 0.4846, Accuracy: 0.7500\n",
      "Batch number: 018, Training: Loss: 0.5108, Accuracy: 0.5625\n",
      "Batch number: 019, Training: Loss: 0.5820, Accuracy: 0.4688\n",
      "Batch number: 020, Training: Loss: 0.5487, Accuracy: 0.7188\n",
      "Batch number: 021, Training: Loss: 0.6438, Accuracy: 0.4375\n",
      "Batch number: 022, Training: Loss: 0.6119, Accuracy: 0.5312\n",
      "Batch number: 023, Training: Loss: 0.5919, Accuracy: 0.5312\n",
      "Batch number: 024, Training: Loss: 0.6813, Accuracy: 0.4375\n",
      "Batch number: 025, Training: Loss: 0.6899, Accuracy: 0.3438\n",
      "Batch number: 026, Training: Loss: 0.6591, Accuracy: 0.4375\n",
      "Batch number: 027, Training: Loss: 0.6603, Accuracy: 0.4062\n",
      "Batch number: 028, Training: Loss: 0.6137, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.6600, Accuracy: 0.3438\n",
      "Batch number: 030, Training: Loss: 0.6730, Accuracy: 0.2812\n",
      "Batch number: 031, Training: Loss: 0.6728, Accuracy: 0.4688\n",
      "Batch number: 032, Training: Loss: 0.6076, Accuracy: 0.6250\n",
      "Batch number: 033, Training: Loss: 0.4981, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.5657, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5443, Accuracy: 0.5312\n",
      "Batch number: 036, Training: Loss: 0.6678, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.4558, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.5042, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.5373, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.4127, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.5013, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 2.5083, Accuracy: 0.4375\n",
      "Batch number: 043, Training: Loss: 0.5865, Accuracy: 0.5938\n",
      "Batch number: 044, Training: Loss: 0.5255, Accuracy: 0.5312\n",
      "Batch number: 045, Training: Loss: 0.4113, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 1.5991, Accuracy: 0.5312\n",
      "Batch number: 047, Training: Loss: 0.3989, Accuracy: 0.5312\n",
      "Batch number: 048, Training: Loss: 0.5002, Accuracy: 0.5938\n",
      "Batch number: 049, Training: Loss: 0.5623, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 0.6066, Accuracy: 0.5625\n",
      "Batch number: 051, Training: Loss: 0.5204, Accuracy: 0.6250\n",
      "Batch number: 052, Training: Loss: 0.5641, Accuracy: 0.5312\n",
      "Batch number: 053, Training: Loss: 0.6280, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 0.6062, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.5849, Accuracy: 0.5000\n",
      "Batch number: 056, Training: Loss: 0.6523, Accuracy: 0.3750\n",
      "Batch number: 057, Training: Loss: 0.6286, Accuracy: 0.4375\n",
      "Batch number: 058, Training: Loss: 0.6066, Accuracy: 0.5000\n",
      "Batch number: 059, Training: Loss: 0.6282, Accuracy: 0.5312\n",
      "Batch number: 060, Training: Loss: 0.6060, Accuracy: 0.7500\n",
      "Batch number: 061, Training: Loss: 0.6061, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.6731, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.5647, Accuracy: 0.5312\n",
      "Batch number: 064, Training: Loss: 0.5994, Accuracy: 0.5625\n",
      "Batch number: 065, Training: Loss: 0.5610, Accuracy: 0.6250\n",
      "Batch number: 066, Training: Loss: 0.6067, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.5800, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.5442, Accuracy: 0.5625\n",
      "Batch number: 069, Training: Loss: 0.4786, Accuracy: 0.6250\n",
      "Batch number: 070, Training: Loss: 1.1347, Accuracy: 0.6250\n",
      "Batch number: 071, Training: Loss: 0.5449, Accuracy: 0.5625\n",
      "Batch number: 072, Training: Loss: 0.5453, Accuracy: 0.5938\n",
      "Batch number: 073, Training: Loss: 0.4876, Accuracy: 0.5625\n",
      "Batch number: 074, Training: Loss: 0.8899, Accuracy: 0.7812\n",
      "Batch number: 075, Training: Loss: 0.5220, Accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 076, Training: Loss: 0.5134, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.4739, Accuracy: 0.5938\n",
      "Batch number: 078, Training: Loss: 1.3278, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 3.4236, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.3864, Accuracy: 0.7812\n",
      "Batch number: 081, Training: Loss: 0.8592, Accuracy: 0.4688\n",
      "Batch number: 082, Training: Loss: 0.4091, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 0.4425, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.5018, Accuracy: 0.6562\n",
      "Batch number: 085, Training: Loss: 0.5494, Accuracy: 0.5312\n",
      "Batch number: 086, Training: Loss: 0.5367, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.4957, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.5535, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.6007, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.6115, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.5798, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.6146, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.5273, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.5140, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.5569, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.5529, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.5434, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.5866, Accuracy: 0.6875\n",
      "Batch number: 100, Training: Loss: 0.5621, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5157, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.5546, Accuracy: 0.5938\n",
      "Batch number: 103, Training: Loss: 0.5180, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.5553, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.5740, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.6034, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.6077, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.4696, Accuracy: 0.7500\n",
      "Batch number: 109, Training: Loss: 0.5768, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.5412, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5613, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.5493, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.4791, Accuracy: 0.6562\n",
      "Batch number: 114, Training: Loss: 0.5614, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.5947, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.8174, Accuracy: 0.5938\n",
      "Batch number: 117, Training: Loss: 0.7320, Accuracy: 0.7500\n",
      "Batch number: 118, Training: Loss: 0.6770, Accuracy: 0.6875\n",
      "Batch number: 119, Training: Loss: 0.5053, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5140, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.5281, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5226, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5556, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.6139, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.5835, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.5510, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.5645, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.6115, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5574, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5455, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.5569, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5733, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5134, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5695, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5369, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5023, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5614, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5413, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.4978, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.5011, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.4784, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.4841, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.5044, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5515, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.6043, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.4738, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5268, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.6387, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5460, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 1.0300, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5217, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5089, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5933, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5310, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.6359, Accuracy: 0.4688\n",
      "Validation Batch number: 000, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7542, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.5064, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.4767, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1886, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5657, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5657, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5377, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.3178, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.6371, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0115, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7542, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.4767, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.4784, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5657, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3779, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3178, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.5064, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3574, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5361, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5657, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 051, Validation: Loss: 0.3771, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3480, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3179, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6356, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3483, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.6684, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3178, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5064, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1589, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.3480, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5657, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3771, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.5176, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.4767, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3771, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3771, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.5064, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.4767, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5657, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5657, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3203, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3178, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3771, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.4804, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.6356, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5373, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.3374, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3178, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5668, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.3178, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3772, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5066, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4767, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5360, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1886, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7246, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.6949, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6652, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6356, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5063, Accuracy: 0.7500\n",
      "Epoch : 004, Training: Loss : 0.8057, Accuracy: 61.5693%\n",
      "Validation : Loss : 0.5262, Accuracy: 61.5108%, Time: 50.7093s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 6/25\n",
      "Batch number: 000, Training: Loss: 0.4310, Accuracy: 0.7188\n",
      "Batch number: 001, Training: Loss: 0.5338, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5410, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5447, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.5234, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4940, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.5609, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.4688, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.9257, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.4981, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5072, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.4651, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5578, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.5131, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 1.0210, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.4262, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.4932, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.4041, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.4171, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.4113, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 1.5981, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 3.3565, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.4592, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.5415, Accuracy: 0.7500\n",
      "Batch number: 024, Training: Loss: 0.4813, Accuracy: 0.7188\n",
      "Batch number: 025, Training: Loss: 2.0175, Accuracy: 0.5938\n",
      "Batch number: 026, Training: Loss: 0.4977, Accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 027, Training: Loss: 0.5440, Accuracy: 0.5625\n",
      "Batch number: 028, Training: Loss: 0.5392, Accuracy: 0.6562\n",
      "Batch number: 029, Training: Loss: 0.5290, Accuracy: 0.5312\n",
      "Batch number: 030, Training: Loss: 0.5764, Accuracy: 0.4062\n",
      "Batch number: 031, Training: Loss: 0.5843, Accuracy: 0.5938\n",
      "Batch number: 032, Training: Loss: 0.5551, Accuracy: 0.7188\n",
      "Batch number: 033, Training: Loss: 0.5226, Accuracy: 0.5938\n",
      "Batch number: 034, Training: Loss: 0.5432, Accuracy: 0.5938\n",
      "Batch number: 035, Training: Loss: 0.5592, Accuracy: 0.6562\n",
      "Batch number: 036, Training: Loss: 0.5979, Accuracy: 0.6875\n",
      "Batch number: 037, Training: Loss: 0.5686, Accuracy: 0.5312\n",
      "Batch number: 038, Training: Loss: 0.4902, Accuracy: 0.7500\n",
      "Batch number: 039, Training: Loss: 0.5280, Accuracy: 0.5625\n",
      "Batch number: 040, Training: Loss: 0.4598, Accuracy: 0.6562\n",
      "Batch number: 041, Training: Loss: 0.5179, Accuracy: 0.6562\n",
      "Batch number: 042, Training: Loss: 0.5447, Accuracy: 0.7500\n",
      "Batch number: 043, Training: Loss: 0.5179, Accuracy: 0.6562\n",
      "Batch number: 044, Training: Loss: 0.5122, Accuracy: 0.7188\n",
      "Batch number: 045, Training: Loss: 0.4600, Accuracy: 0.6250\n",
      "Batch number: 046, Training: Loss: 0.6550, Accuracy: 0.6875\n",
      "Batch number: 047, Training: Loss: 0.4882, Accuracy: 0.7500\n",
      "Batch number: 048, Training: Loss: 0.4709, Accuracy: 0.7188\n",
      "Batch number: 049, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 050, Training: Loss: 0.4268, Accuracy: 0.8125\n",
      "Batch number: 051, Training: Loss: 0.4031, Accuracy: 0.7812\n",
      "Batch number: 052, Training: Loss: 0.5924, Accuracy: 0.8438\n",
      "Batch number: 053, Training: Loss: 0.3969, Accuracy: 0.8438\n",
      "Batch number: 054, Training: Loss: 0.3868, Accuracy: 0.7500\n",
      "Batch number: 055, Training: Loss: 0.4055, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.3407, Accuracy: 0.8125\n",
      "Batch number: 057, Training: Loss: 0.3320, Accuracy: 0.8438\n",
      "Batch number: 058, Training: Loss: 0.3992, Accuracy: 0.7812\n",
      "Batch number: 059, Training: Loss: 9.7852, Accuracy: 0.9062\n",
      "Batch number: 060, Training: Loss: 0.4146, Accuracy: 0.6875\n",
      "Batch number: 061, Training: Loss: 0.4245, Accuracy: 0.7500\n",
      "Batch number: 062, Training: Loss: 0.3245, Accuracy: 0.7812\n",
      "Batch number: 063, Training: Loss: 0.3993, Accuracy: 0.8438\n",
      "Batch number: 064, Training: Loss: 1.8114, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.3981, Accuracy: 0.8125\n",
      "Batch number: 066, Training: Loss: 0.4951, Accuracy: 0.7188\n",
      "Batch number: 067, Training: Loss: 0.4679, Accuracy: 0.6562\n",
      "Batch number: 068, Training: Loss: 0.4470, Accuracy: 0.7812\n",
      "Batch number: 069, Training: Loss: 0.4160, Accuracy: 0.7812\n",
      "Batch number: 070, Training: Loss: 0.5395, Accuracy: 0.5938\n",
      "Batch number: 071, Training: Loss: 0.4657, Accuracy: 0.7500\n",
      "Batch number: 072, Training: Loss: 0.4572, Accuracy: 0.7500\n",
      "Batch number: 073, Training: Loss: 0.4122, Accuracy: 0.8438\n",
      "Batch number: 074, Training: Loss: 0.5052, Accuracy: 0.5625\n",
      "Batch number: 075, Training: Loss: 0.5619, Accuracy: 0.6562\n",
      "Batch number: 076, Training: Loss: 0.5427, Accuracy: 0.5938\n",
      "Batch number: 077, Training: Loss: 0.5257, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 0.5228, Accuracy: 0.6250\n",
      "Batch number: 079, Training: Loss: 0.6268, Accuracy: 0.6875\n",
      "Batch number: 080, Training: Loss: 0.6831, Accuracy: 0.5625\n",
      "Batch number: 081, Training: Loss: 0.4889, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 0.4482, Accuracy: 0.6250\n",
      "Batch number: 083, Training: Loss: 0.4990, Accuracy: 0.6250\n",
      "Batch number: 084, Training: Loss: 0.5253, Accuracy: 0.5938\n",
      "Batch number: 085, Training: Loss: 0.4897, Accuracy: 0.7500\n",
      "Batch number: 086, Training: Loss: 0.3908, Accuracy: 0.7500\n",
      "Batch number: 087, Training: Loss: 0.4420, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.5027, Accuracy: 0.5938\n",
      "Batch number: 089, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.7726, Accuracy: 0.5938\n",
      "Batch number: 091, Training: Loss: 0.5239, Accuracy: 0.7188\n",
      "Batch number: 092, Training: Loss: 0.4446, Accuracy: 0.7188\n",
      "Batch number: 093, Training: Loss: 0.5095, Accuracy: 0.7500\n",
      "Batch number: 094, Training: Loss: 0.4769, Accuracy: 0.7812\n",
      "Batch number: 095, Training: Loss: 0.9306, Accuracy: 0.4688\n",
      "Batch number: 096, Training: Loss: 0.3605, Accuracy: 0.7812\n",
      "Batch number: 097, Training: Loss: 0.6458, Accuracy: 0.6875\n",
      "Batch number: 098, Training: Loss: 0.4134, Accuracy: 0.7812\n",
      "Batch number: 099, Training: Loss: 0.5086, Accuracy: 0.6250\n",
      "Batch number: 100, Training: Loss: 0.4787, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5167, Accuracy: 0.5938\n",
      "Batch number: 102, Training: Loss: 0.5100, Accuracy: 0.6875\n",
      "Batch number: 103, Training: Loss: 0.4530, Accuracy: 0.6875\n",
      "Batch number: 104, Training: Loss: 1.2908, Accuracy: 0.5938\n",
      "Batch number: 105, Training: Loss: 0.5643, Accuracy: 0.6875\n",
      "Batch number: 106, Training: Loss: 0.6009, Accuracy: 0.4375\n",
      "Batch number: 107, Training: Loss: 0.5843, Accuracy: 0.6250\n",
      "Batch number: 108, Training: Loss: 0.6236, Accuracy: 0.4375\n",
      "Batch number: 109, Training: Loss: 0.5184, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 0.5440, Accuracy: 0.5938\n",
      "Batch number: 111, Training: Loss: 0.5746, Accuracy: 0.5625\n",
      "Batch number: 112, Training: Loss: 0.5994, Accuracy: 0.5312\n",
      "Batch number: 113, Training: Loss: 0.5438, Accuracy: 0.5625\n",
      "Batch number: 114, Training: Loss: 0.5318, Accuracy: 0.5938\n",
      "Batch number: 115, Training: Loss: 0.6075, Accuracy: 0.5625\n",
      "Batch number: 116, Training: Loss: 0.5664, Accuracy: 0.5938\n",
      "Batch number: 117, Training: Loss: 0.5040, Accuracy: 0.5000\n",
      "Batch number: 118, Training: Loss: 0.8660, Accuracy: 0.4062\n",
      "Batch number: 119, Training: Loss: 0.4972, Accuracy: 0.5938\n",
      "Batch number: 120, Training: Loss: 0.4899, Accuracy: 0.7188\n",
      "Batch number: 121, Training: Loss: 0.4866, Accuracy: 0.5625\n",
      "Batch number: 122, Training: Loss: 0.4278, Accuracy: 0.7500\n",
      "Batch number: 123, Training: Loss: 0.3647, Accuracy: 0.7812\n",
      "Batch number: 124, Training: Loss: 9.4537, Accuracy: 0.5938\n",
      "Batch number: 125, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 126, Training: Loss: 0.4359, Accuracy: 0.6562\n",
      "Batch number: 127, Training: Loss: 0.5121, Accuracy: 0.6562\n",
      "Batch number: 128, Training: Loss: 1.7027, Accuracy: 0.6562\n",
      "Batch number: 129, Training: Loss: 0.4769, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5790, Accuracy: 0.4688\n",
      "Batch number: 131, Training: Loss: 0.4561, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5150, Accuracy: 0.7188\n",
      "Batch number: 133, Training: Loss: 0.4784, Accuracy: 0.6250\n",
      "Batch number: 134, Training: Loss: 0.5039, Accuracy: 0.7188\n",
      "Batch number: 135, Training: Loss: 0.5215, Accuracy: 0.5938\n",
      "Batch number: 136, Training: Loss: 0.4977, Accuracy: 0.6562\n",
      "Batch number: 137, Training: Loss: 0.5200, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5149, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.4769, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.4793, Accuracy: 0.6562\n",
      "Batch number: 141, Training: Loss: 0.4399, Accuracy: 0.6875\n",
      "Batch number: 142, Training: Loss: 0.4797, Accuracy: 0.5625\n",
      "Batch number: 143, Training: Loss: 0.7092, Accuracy: 0.6875\n",
      "Batch number: 144, Training: Loss: 0.4755, Accuracy: 0.6250\n",
      "Batch number: 145, Training: Loss: 0.5191, Accuracy: 0.6562\n",
      "Batch number: 146, Training: Loss: 0.4342, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.4531, Accuracy: 0.7500\n",
      "Batch number: 148, Training: Loss: 0.4304, Accuracy: 0.8125\n",
      "Batch number: 149, Training: Loss: 1.2339, Accuracy: 0.7500\n",
      "Batch number: 150, Training: Loss: 1.3945, Accuracy: 0.6250\n",
      "Batch number: 151, Training: Loss: 1.1160, Accuracy: 0.7188\n",
      "Batch number: 152, Training: Loss: 0.4343, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5584, Accuracy: 0.7188\n",
      "Batch number: 154, Training: Loss: 0.5027, Accuracy: 0.5312\n",
      "Batch number: 155, Training: Loss: 0.5096, Accuracy: 0.6875\n",
      "Validation Batch number: 000, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 001, Validation: Loss: 0.6747, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.5991, Accuracy: 0.7500\n",
      "Validation Batch number: 003, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 005, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.7026, Accuracy: 0.2500\n",
      "Validation Batch number: 007, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.5247, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5153, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 011, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 012, Validation: Loss: 0.2515, Accuracy: 0.7500\n",
      "Validation Batch number: 013, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 014, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 016, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 018, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 019, Validation: Loss: 0.1687, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 0.5060, Accuracy: 1.0000\n",
      "Validation Batch number: 021, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5060, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 025, Validation: Loss: 0.1780, Accuracy: 0.7500\n",
      "Validation Batch number: 026, Validation: Loss: 0.4076, Accuracy: 1.0000\n",
      "Validation Batch number: 027, Validation: Loss: 0.0396, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.6747, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.4710, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.3562, Accuracy: 0.5000\n",
      "Validation Batch number: 032, Validation: Loss: 0.5124, Accuracy: 0.5000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 034, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 0.5060, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3176, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6933, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3560, Accuracy: 0.5000\n",
      "Validation Batch number: 040, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 042, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 043, Validation: Loss: 0.6976, Accuracy: 0.5000\n",
      "Validation Batch number: 044, Validation: Loss: 0.7026, Accuracy: 0.2500\n",
      "Validation Batch number: 045, Validation: Loss: 0.7026, Accuracy: 0.2500\n",
      "Validation Batch number: 046, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 050, Validation: Loss: 0.5060, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.3374, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 053, Validation: Loss: 0.7026, Accuracy: 0.2500\n",
      "Validation Batch number: 054, Validation: Loss: 0.3896, Accuracy: 1.0000\n",
      "Validation Batch number: 055, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 056, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.4618, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 059, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3560, Accuracy: 0.5000\n",
      "Validation Batch number: 062, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 063, Validation: Loss: 0.7119, Accuracy: 0.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 065, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 068, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 070, Validation: Loss: 0.6933, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6933, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 074, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 075, Validation: Loss: 0.2443, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.6480, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.5818, Accuracy: 0.7500\n",
      "Validation Batch number: 078, Validation: Loss: 0.7026, Accuracy: 0.2500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3560, Accuracy: 0.5000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 081, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 082, Validation: Loss: 0.1780, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5060, Accuracy: 1.0000\n",
      "Validation Batch number: 085, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 086, Validation: Loss: 0.3374, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1687, Accuracy: 1.0000\n",
      "Validation Batch number: 088, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 089, Validation: Loss: 0.4215, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.3374, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.4853, Accuracy: 0.5000\n",
      "Validation Batch number: 092, Validation: Loss: 0.7026, Accuracy: 0.2500\n",
      "Validation Batch number: 093, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3374, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3374, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6933, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.5060, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.5340, Accuracy: 0.2500\n",
      "Validation Batch number: 100, Validation: Loss: 0.4234, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.4799, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.3560, Accuracy: 0.5000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3560, Accuracy: 0.5000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3374, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.1687, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.3374, Accuracy: 1.0000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 109, Validation: Loss: 0.5255, Accuracy: 1.0000\n",
      "Validation Batch number: 110, Validation: Loss: 0.7119, Accuracy: 0.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 112, Validation: Loss: 0.3919, Accuracy: 0.7500\n",
      "Validation Batch number: 113, Validation: Loss: 0.7026, Accuracy: 0.2500\n",
      "Validation Batch number: 114, Validation: Loss: 0.3999, Accuracy: 1.0000\n",
      "Validation Batch number: 115, Validation: Loss: 0.3560, Accuracy: 0.5000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3560, Accuracy: 0.5000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.3423, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.1780, Accuracy: 0.7500\n",
      "Validation Batch number: 121, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.3467, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.5252, Accuracy: 0.7500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5592, Accuracy: 0.7500\n",
      "Validation Batch number: 125, Validation: Loss: 0.3374, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 127, Validation: Loss: 0.5340, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 128, Validation: Loss: 0.3751, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 130, Validation: Loss: 0.7026, Accuracy: 0.2500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5246, Accuracy: 0.5000\n",
      "Validation Batch number: 132, Validation: Loss: 0.5153, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.1687, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.6840, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 0.6933, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.7026, Accuracy: 0.2500\n",
      "Validation Batch number: 137, Validation: Loss: 0.5340, Accuracy: 0.2500\n",
      "Validation Batch number: 138, Validation: Loss: 0.4023, Accuracy: 0.7500\n",
      "Epoch : 005, Training: Loss : 0.6968, Accuracy: 66.3131%\n",
      "Validation : Loss : 0.4861, Accuracy: 69.0647%, Time: 50.5909s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 7/25\n",
      "Batch number: 000, Training: Loss: 0.3277, Accuracy: 0.7500\n",
      "Batch number: 001, Training: Loss: 0.4592, Accuracy: 0.5625\n",
      "Batch number: 002, Training: Loss: 0.4776, Accuracy: 0.6562\n",
      "Batch number: 003, Training: Loss: 0.4446, Accuracy: 0.7812\n",
      "Batch number: 004, Training: Loss: 2.6474, Accuracy: 0.5625\n",
      "Batch number: 005, Training: Loss: 0.4633, Accuracy: 0.6250\n",
      "Batch number: 006, Training: Loss: 1.1619, Accuracy: 0.7188\n",
      "Batch number: 007, Training: Loss: 0.3936, Accuracy: 0.7812\n",
      "Batch number: 008, Training: Loss: 0.4395, Accuracy: 0.7500\n",
      "Batch number: 009, Training: Loss: 2.9976, Accuracy: 0.5625\n",
      "Batch number: 010, Training: Loss: 0.5176, Accuracy: 0.5312\n",
      "Batch number: 011, Training: Loss: 0.4216, Accuracy: 0.7812\n",
      "Batch number: 012, Training: Loss: 0.4957, Accuracy: 0.7812\n",
      "Batch number: 013, Training: Loss: 0.6947, Accuracy: 0.6250\n",
      "Batch number: 014, Training: Loss: 0.8749, Accuracy: 0.6875\n",
      "Batch number: 015, Training: Loss: 0.4556, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5157, Accuracy: 0.6875\n",
      "Batch number: 017, Training: Loss: 0.3903, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.5671, Accuracy: 0.4688\n",
      "Batch number: 019, Training: Loss: 0.4616, Accuracy: 0.5625\n",
      "Batch number: 020, Training: Loss: 0.8132, Accuracy: 0.8438\n",
      "Batch number: 021, Training: Loss: 0.5094, Accuracy: 0.5938\n",
      "Batch number: 022, Training: Loss: 0.4259, Accuracy: 0.7812\n",
      "Batch number: 023, Training: Loss: 0.5113, Accuracy: 0.7500\n",
      "Batch number: 024, Training: Loss: 1.0754, Accuracy: 0.6562\n",
      "Batch number: 025, Training: Loss: 0.4973, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.6345, Accuracy: 0.6875\n",
      "Batch number: 027, Training: Loss: 0.5033, Accuracy: 0.6250\n",
      "Batch number: 028, Training: Loss: 0.5245, Accuracy: 0.7188\n",
      "Batch number: 029, Training: Loss: 0.4374, Accuracy: 0.6562\n",
      "Batch number: 030, Training: Loss: 0.5054, Accuracy: 0.5625\n",
      "Batch number: 031, Training: Loss: 2.8052, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 0.8879, Accuracy: 0.6250\n",
      "Batch number: 033, Training: Loss: 0.4789, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.4744, Accuracy: 0.6875\n",
      "Batch number: 035, Training: Loss: 0.5347, Accuracy: 0.6875\n",
      "Batch number: 036, Training: Loss: 0.5243, Accuracy: 0.7812\n",
      "Batch number: 037, Training: Loss: 0.5821, Accuracy: 0.5938\n",
      "Batch number: 038, Training: Loss: 0.4937, Accuracy: 0.7500\n",
      "Batch number: 039, Training: Loss: 0.5550, Accuracy: 0.5312\n",
      "Batch number: 040, Training: Loss: 0.4587, Accuracy: 0.6562\n",
      "Batch number: 041, Training: Loss: 0.3734, Accuracy: 0.8438\n",
      "Batch number: 042, Training: Loss: 0.4668, Accuracy: 0.8438\n",
      "Batch number: 043, Training: Loss: 0.5227, Accuracy: 0.6562\n",
      "Batch number: 044, Training: Loss: 0.4581, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 0.4728, Accuracy: 0.6250\n",
      "Batch number: 046, Training: Loss: 0.5064, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.4195, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 0.4427, Accuracy: 0.7500\n",
      "Batch number: 049, Training: Loss: 0.5238, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 1.4612, Accuracy: 0.7812\n",
      "Batch number: 051, Training: Loss: 0.4303, Accuracy: 0.7188\n",
      "Batch number: 052, Training: Loss: 0.4771, Accuracy: 0.7188\n",
      "Batch number: 053, Training: Loss: 0.5384, Accuracy: 0.6562\n",
      "Batch number: 054, Training: Loss: 0.6853, Accuracy: 0.5938\n",
      "Batch number: 055, Training: Loss: 1.2799, Accuracy: 0.9062\n",
      "Batch number: 056, Training: Loss: 0.5464, Accuracy: 0.6562\n",
      "Batch number: 057, Training: Loss: 0.4820, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 0.4493, Accuracy: 0.7188\n",
      "Batch number: 059, Training: Loss: 0.4502, Accuracy: 0.7812\n",
      "Batch number: 060, Training: Loss: 0.5286, Accuracy: 0.5625\n",
      "Batch number: 061, Training: Loss: 0.4742, Accuracy: 0.6875\n",
      "Batch number: 062, Training: Loss: 0.4556, Accuracy: 0.6250\n",
      "Batch number: 063, Training: Loss: 0.4760, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 0.4760, Accuracy: 0.7500\n",
      "Batch number: 065, Training: Loss: 0.4143, Accuracy: 0.7812\n",
      "Batch number: 066, Training: Loss: 0.4460, Accuracy: 0.8125\n",
      "Batch number: 067, Training: Loss: 0.5205, Accuracy: 0.5938\n",
      "Batch number: 068, Training: Loss: 0.5016, Accuracy: 0.7188\n",
      "Batch number: 069, Training: Loss: 0.4139, Accuracy: 0.7812\n",
      "Batch number: 070, Training: Loss: 0.4398, Accuracy: 0.7188\n",
      "Batch number: 071, Training: Loss: 0.3956, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 0.4263, Accuracy: 0.7812\n",
      "Batch number: 073, Training: Loss: 1.2134, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 0.4071, Accuracy: 0.6562\n",
      "Batch number: 075, Training: Loss: 8.1258, Accuracy: 0.6562\n",
      "Batch number: 076, Training: Loss: 0.7148, Accuracy: 0.6875\n",
      "Batch number: 077, Training: Loss: 0.4748, Accuracy: 0.6875\n",
      "Batch number: 078, Training: Loss: 0.4574, Accuracy: 0.6875\n",
      "Batch number: 079, Training: Loss: 0.4655, Accuracy: 0.7188\n",
      "Batch number: 080, Training: Loss: 0.5173, Accuracy: 0.5312\n",
      "Batch number: 081, Training: Loss: 0.4556, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 0.3344, Accuracy: 0.7500\n",
      "Batch number: 083, Training: Loss: 0.3867, Accuracy: 0.7500\n",
      "Batch number: 084, Training: Loss: 0.3868, Accuracy: 0.7500\n",
      "Batch number: 085, Training: Loss: 0.4213, Accuracy: 0.8125\n",
      "Batch number: 086, Training: Loss: 0.3602, Accuracy: 0.7812\n",
      "Batch number: 087, Training: Loss: 0.3602, Accuracy: 0.7812\n",
      "Batch number: 088, Training: Loss: 0.4576, Accuracy: 0.6250\n",
      "Batch number: 089, Training: Loss: 0.3248, Accuracy: 0.8438\n",
      "Batch number: 090, Training: Loss: 0.3778, Accuracy: 0.7812\n",
      "Batch number: 091, Training: Loss: 15.2173, Accuracy: 0.9062\n",
      "Batch number: 092, Training: Loss: 0.3511, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 1.6171, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.4199, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 0.3966, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.4950, Accuracy: 0.6250\n",
      "Batch number: 097, Training: Loss: 0.5162, Accuracy: 0.6875\n",
      "Batch number: 098, Training: Loss: 0.5468, Accuracy: 0.6250\n",
      "Batch number: 099, Training: Loss: 0.5675, Accuracy: 0.5625\n",
      "Batch number: 100, Training: Loss: 0.4586, Accuracy: 0.6875\n",
      "Batch number: 101, Training: Loss: 0.6052, Accuracy: 0.5000\n",
      "Batch number: 102, Training: Loss: 0.5834, Accuracy: 0.5938\n",
      "Batch number: 103, Training: Loss: 0.5669, Accuracy: 0.5625\n",
      "Batch number: 104, Training: Loss: 0.5635, Accuracy: 0.6250\n",
      "Batch number: 105, Training: Loss: 0.4988, Accuracy: 0.7500\n",
      "Batch number: 106, Training: Loss: 0.6312, Accuracy: 0.4062\n",
      "Batch number: 107, Training: Loss: 0.6233, Accuracy: 0.6250\n",
      "Batch number: 108, Training: Loss: 0.5823, Accuracy: 0.5000\n",
      "Batch number: 109, Training: Loss: 0.5268, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 0.5797, Accuracy: 0.5625\n",
      "Batch number: 111, Training: Loss: 0.5791, Accuracy: 0.5625\n",
      "Batch number: 112, Training: Loss: 0.5523, Accuracy: 0.5938\n",
      "Batch number: 113, Training: Loss: 0.5775, Accuracy: 0.5625\n",
      "Batch number: 114, Training: Loss: 0.5767, Accuracy: 0.5625\n",
      "Batch number: 115, Training: Loss: 0.6020, Accuracy: 0.5312\n",
      "Batch number: 116, Training: Loss: 0.5612, Accuracy: 0.6250\n",
      "Batch number: 117, Training: Loss: 0.5877, Accuracy: 0.4375\n",
      "Batch number: 118, Training: Loss: 0.6228, Accuracy: 0.4062\n",
      "Batch number: 119, Training: Loss: 0.6084, Accuracy: 0.4688\n",
      "Batch number: 120, Training: Loss: 0.9320, Accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 121, Training: Loss: 1.2680, Accuracy: 0.5312\n",
      "Batch number: 122, Training: Loss: 0.8582, Accuracy: 0.6562\n",
      "Batch number: 123, Training: Loss: 0.5056, Accuracy: 0.5938\n",
      "Batch number: 124, Training: Loss: 4.2992, Accuracy: 0.5938\n",
      "Batch number: 125, Training: Loss: 0.4911, Accuracy: 0.5625\n",
      "Batch number: 126, Training: Loss: 1.9335, Accuracy: 0.5312\n",
      "Batch number: 127, Training: Loss: 0.5826, Accuracy: 0.5625\n",
      "Batch number: 128, Training: Loss: 0.6053, Accuracy: 0.5938\n",
      "Batch number: 129, Training: Loss: 0.5877, Accuracy: 0.5000\n",
      "Batch number: 130, Training: Loss: 0.5892, Accuracy: 0.3750\n",
      "Batch number: 131, Training: Loss: 0.5853, Accuracy: 0.5000\n",
      "Batch number: 132, Training: Loss: 0.5639, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5823, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.6067, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5603, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5823, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5409, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.7161, Accuracy: 0.5938\n",
      "Batch number: 139, Training: Loss: 0.5177, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.4807, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.4576, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.4866, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.5015, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5330, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.4867, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.4934, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.4873, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.4795, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.4484, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5110, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.4431, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.4188, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.4338, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.4037, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.3902, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7175, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7409, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.1857, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.5443, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.1620, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7139, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.3740, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.3850, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7409, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.0346, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7175, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.3705, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 2.6064, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 3.6293, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.2051, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7175, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.5581, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.3705, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.3471, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 086, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 2.4533, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.5557, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.2173, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.3704, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.5556, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5323, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.4857, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Epoch : 006, Training: Loss : 0.7661, Accuracy: 66.3131%\n",
      "Validation : Loss : 0.4550, Accuracy: 61.5108%, Time: 50.4100s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 8/25\n",
      "Batch number: 000, Training: Loss: 0.3529, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.4337, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.3764, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.3983, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.3453, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 1.9657, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.3497, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.3703, Accuracy: 0.5938\n",
      "Batch number: 008, Training: Loss: 0.4188, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 0.3899, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.4596, Accuracy: 0.5938\n",
      "Batch number: 011, Training: Loss: 0.4060, Accuracy: 0.8438\n",
      "Batch number: 012, Training: Loss: 0.4444, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.7492, Accuracy: 0.6875\n",
      "Batch number: 014, Training: Loss: 0.7473, Accuracy: 0.7188\n",
      "Batch number: 015, Training: Loss: 0.4406, Accuracy: 0.7500\n",
      "Batch number: 016, Training: Loss: 0.4205, Accuracy: 0.8125\n",
      "Batch number: 017, Training: Loss: 0.3678, Accuracy: 0.9062\n",
      "Batch number: 018, Training: Loss: 0.4631, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 0.4438, Accuracy: 0.6562\n",
      "Batch number: 020, Training: Loss: 0.3990, Accuracy: 0.9062\n",
      "Batch number: 021, Training: Loss: 0.4547, Accuracy: 0.6875\n",
      "Batch number: 022, Training: Loss: 0.4429, Accuracy: 0.7500\n",
      "Batch number: 023, Training: Loss: 0.3673, Accuracy: 0.8438\n",
      "Batch number: 024, Training: Loss: 0.4670, Accuracy: 0.7188\n",
      "Batch number: 025, Training: Loss: 0.4373, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.4227, Accuracy: 0.7500\n",
      "Batch number: 027, Training: Loss: 0.4818, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.4841, Accuracy: 0.7188\n",
      "Batch number: 029, Training: Loss: 0.4472, Accuracy: 0.6562\n",
      "Batch number: 030, Training: Loss: 0.4117, Accuracy: 0.6562\n",
      "Batch number: 031, Training: Loss: 0.4253, Accuracy: 0.7500\n",
      "Batch number: 032, Training: Loss: 0.4578, Accuracy: 0.8125\n",
      "Batch number: 033, Training: Loss: 0.4042, Accuracy: 0.7500\n",
      "Batch number: 034, Training: Loss: 0.4067, Accuracy: 0.7500\n",
      "Batch number: 035, Training: Loss: 0.4037, Accuracy: 0.8125\n",
      "Batch number: 036, Training: Loss: 0.4816, Accuracy: 0.8750\n",
      "Batch number: 037, Training: Loss: 0.4429, Accuracy: 0.6875\n",
      "Batch number: 038, Training: Loss: 0.3925, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 0.4140, Accuracy: 0.7188\n",
      "Batch number: 040, Training: Loss: 0.3533, Accuracy: 0.8125\n",
      "Batch number: 041, Training: Loss: 0.4380, Accuracy: 0.7500\n",
      "Batch number: 042, Training: Loss: 0.3962, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.4652, Accuracy: 0.7188\n",
      "Batch number: 044, Training: Loss: 0.3606, Accuracy: 0.8750\n",
      "Batch number: 045, Training: Loss: 0.3988, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 0.6773, Accuracy: 0.7812\n",
      "Batch number: 047, Training: Loss: 0.3577, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 0.3523, Accuracy: 0.8438\n",
      "Batch number: 049, Training: Loss: 0.4206, Accuracy: 0.7500\n",
      "Batch number: 050, Training: Loss: 3.1796, Accuracy: 0.7812\n",
      "Batch number: 051, Training: Loss: 0.3919, Accuracy: 0.7812\n",
      "Batch number: 052, Training: Loss: 0.4233, Accuracy: 0.7812\n",
      "Batch number: 053, Training: Loss: 0.4229, Accuracy: 0.7812\n",
      "Batch number: 054, Training: Loss: 1.5695, Accuracy: 0.7188\n",
      "Batch number: 055, Training: Loss: 0.3649, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.3902, Accuracy: 0.7500\n",
      "Batch number: 057, Training: Loss: 0.4026, Accuracy: 0.7500\n",
      "Batch number: 058, Training: Loss: 0.4508, Accuracy: 0.7188\n",
      "Batch number: 059, Training: Loss: 0.4764, Accuracy: 0.7500\n",
      "Batch number: 060, Training: Loss: 0.4813, Accuracy: 0.6250\n",
      "Batch number: 061, Training: Loss: 0.4705, Accuracy: 0.6875\n",
      "Batch number: 062, Training: Loss: 0.3793, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 1.1235, Accuracy: 0.6562\n",
      "Batch number: 064, Training: Loss: 0.8803, Accuracy: 0.7812\n",
      "Batch number: 065, Training: Loss: 0.8827, Accuracy: 0.6562\n",
      "Batch number: 066, Training: Loss: 0.5974, Accuracy: 0.6562\n",
      "Batch number: 067, Training: Loss: 0.5605, Accuracy: 0.6250\n",
      "Batch number: 068, Training: Loss: 0.4206, Accuracy: 0.7812\n",
      "Batch number: 069, Training: Loss: 0.4361, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 070, Training: Loss: 0.6075, Accuracy: 0.5312\n",
      "Batch number: 071, Training: Loss: 0.5418, Accuracy: 0.6562\n",
      "Batch number: 072, Training: Loss: 0.5115, Accuracy: 0.6875\n",
      "Batch number: 073, Training: Loss: 0.5112, Accuracy: 0.7188\n",
      "Batch number: 074, Training: Loss: 0.6314, Accuracy: 0.4375\n",
      "Batch number: 075, Training: Loss: 0.4808, Accuracy: 0.6875\n",
      "Batch number: 076, Training: Loss: 0.4948, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.5391, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.5815, Accuracy: 0.5625\n",
      "Batch number: 079, Training: Loss: 0.5238, Accuracy: 0.6562\n",
      "Batch number: 080, Training: Loss: 0.4576, Accuracy: 0.6250\n",
      "Batch number: 081, Training: Loss: 0.4428, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 0.4830, Accuracy: 0.5938\n",
      "Batch number: 083, Training: Loss: 0.4449, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.4852, Accuracy: 0.6562\n",
      "Batch number: 085, Training: Loss: 0.4702, Accuracy: 0.7500\n",
      "Batch number: 086, Training: Loss: 0.4706, Accuracy: 0.6562\n",
      "Batch number: 087, Training: Loss: 0.4973, Accuracy: 0.6250\n",
      "Batch number: 088, Training: Loss: 0.4898, Accuracy: 0.5938\n",
      "Batch number: 089, Training: Loss: 0.5123, Accuracy: 0.6250\n",
      "Batch number: 090, Training: Loss: 1.6454, Accuracy: 0.6875\n",
      "Batch number: 091, Training: Loss: 0.5188, Accuracy: 0.7188\n",
      "Batch number: 092, Training: Loss: 0.4835, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.4137, Accuracy: 0.8438\n",
      "Batch number: 094, Training: Loss: 0.5014, Accuracy: 0.7188\n",
      "Batch number: 095, Training: Loss: 0.4730, Accuracy: 0.6250\n",
      "Batch number: 096, Training: Loss: 0.4385, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.4763, Accuracy: 0.7500\n",
      "Batch number: 098, Training: Loss: 0.4405, Accuracy: 0.7500\n",
      "Batch number: 099, Training: Loss: 0.4379, Accuracy: 0.6875\n",
      "Batch number: 100, Training: Loss: 0.3789, Accuracy: 0.7812\n",
      "Batch number: 101, Training: Loss: 0.4633, Accuracy: 0.6562\n",
      "Batch number: 102, Training: Loss: 0.4847, Accuracy: 0.7188\n",
      "Batch number: 103, Training: Loss: 1.2788, Accuracy: 0.6875\n",
      "Batch number: 104, Training: Loss: 0.4849, Accuracy: 0.7188\n",
      "Batch number: 105, Training: Loss: 0.8787, Accuracy: 0.7812\n",
      "Batch number: 106, Training: Loss: 0.4848, Accuracy: 0.5625\n",
      "Batch number: 107, Training: Loss: 0.4959, Accuracy: 0.7500\n",
      "Batch number: 108, Training: Loss: 0.5212, Accuracy: 0.5625\n",
      "Batch number: 109, Training: Loss: 0.4817, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.3974, Accuracy: 0.7812\n",
      "Batch number: 111, Training: Loss: 0.5250, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.5250, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.5250, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.4995, Accuracy: 0.6562\n",
      "Batch number: 115, Training: Loss: 0.5249, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.5868, Accuracy: 0.5938\n",
      "Batch number: 117, Training: Loss: 0.5604, Accuracy: 0.4688\n",
      "Batch number: 118, Training: Loss: 0.5706, Accuracy: 0.5000\n",
      "Batch number: 119, Training: Loss: 0.5104, Accuracy: 0.5938\n",
      "Batch number: 120, Training: Loss: 0.6111, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.6472, Accuracy: 0.3750\n",
      "Batch number: 122, Training: Loss: 0.4811, Accuracy: 0.6562\n",
      "Batch number: 123, Training: Loss: 1.2183, Accuracy: 0.5625\n",
      "Batch number: 124, Training: Loss: 0.5310, Accuracy: 0.6875\n",
      "Batch number: 125, Training: Loss: 0.5456, Accuracy: 0.5938\n",
      "Batch number: 126, Training: Loss: 0.5096, Accuracy: 0.5625\n",
      "Batch number: 127, Training: Loss: 0.4940, Accuracy: 0.6875\n",
      "Batch number: 128, Training: Loss: 0.5812, Accuracy: 0.6250\n",
      "Batch number: 129, Training: Loss: 0.7635, Accuracy: 0.5938\n",
      "Batch number: 130, Training: Loss: 0.5562, Accuracy: 0.4375\n",
      "Batch number: 131, Training: Loss: 0.5225, Accuracy: 0.5938\n",
      "Batch number: 132, Training: Loss: 0.5825, Accuracy: 0.6250\n",
      "Batch number: 133, Training: Loss: 0.5229, Accuracy: 0.5625\n",
      "Batch number: 134, Training: Loss: 0.5248, Accuracy: 0.6875\n",
      "Batch number: 135, Training: Loss: 0.5430, Accuracy: 0.5625\n",
      "Batch number: 136, Training: Loss: 0.4760, Accuracy: 0.6875\n",
      "Batch number: 137, Training: Loss: 0.5635, Accuracy: 0.5625\n",
      "Batch number: 138, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5197, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.4984, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5835, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.4956, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.4565, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5808, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5005, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5169, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5089, Accuracy: 0.6250\n",
      "Batch number: 148, Training: Loss: 0.5261, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.4836, Accuracy: 0.6250\n",
      "Batch number: 150, Training: Loss: 1.7585, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5203, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5573, Accuracy: 0.6875\n",
      "Batch number: 153, Training: Loss: 0.5680, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.4888, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.4656, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7303, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6938, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6938, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1826, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1643, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.1643, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7303, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.4930, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.4930, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3651, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6938, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6756, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 045, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3651, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6573, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6938, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6938, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.1826, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.6938, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1643, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.3479, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3651, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1826, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3651, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3651, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6938, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.4757, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.4930, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3651, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.6938, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.4930, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4930, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5477, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3469, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.6903, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3651, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.3287, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5295, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1826, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7120, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.6938, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6756, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6573, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5112, Accuracy: 0.7500\n",
      "Epoch : 007, Training: Loss : 0.5503, Accuracy: 68.6349%\n",
      "Validation : Loss : 0.5125, Accuracy: 61.5108%, Time: 50.5416s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 9/25\n",
      "Batch number: 000, Training: Loss: 0.4747, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5159, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5762, Accuracy: 0.5938\n",
      "Batch number: 003, Training: Loss: 0.4999, Accuracy: 0.6250\n",
      "Batch number: 004, Training: Loss: 0.4060, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4951, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.5000, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.4848, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5485, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.5131, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5516, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.5868, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5536, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.5387, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.5760, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.4952, Accuracy: 0.6875\n",
      "Batch number: 016, Training: Loss: 0.4836, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.5264, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.5147, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.5150, Accuracy: 0.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 020, Training: Loss: 0.5074, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.5186, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.5003, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.4367, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.4997, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.4979, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5624, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.4989, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5195, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.5412, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.5453, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.5637, Accuracy: 0.4688\n",
      "Batch number: 033, Training: Loss: 0.5088, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5197, Accuracy: 0.7188\n",
      "Batch number: 036, Training: Loss: 0.5408, Accuracy: 0.7812\n",
      "Batch number: 037, Training: Loss: 0.5202, Accuracy: 0.5938\n",
      "Batch number: 038, Training: Loss: 0.4965, Accuracy: 0.7500\n",
      "Batch number: 039, Training: Loss: 0.4773, Accuracy: 0.6250\n",
      "Batch number: 040, Training: Loss: 0.4112, Accuracy: 0.7188\n",
      "Batch number: 041, Training: Loss: 0.4968, Accuracy: 0.6875\n",
      "Batch number: 042, Training: Loss: 0.5142, Accuracy: 0.7812\n",
      "Batch number: 043, Training: Loss: 0.4963, Accuracy: 0.6875\n",
      "Batch number: 044, Training: Loss: 0.5377, Accuracy: 0.6875\n",
      "Batch number: 045, Training: Loss: 0.4581, Accuracy: 0.6250\n",
      "Batch number: 046, Training: Loss: 0.5140, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.5134, Accuracy: 0.7188\n",
      "Batch number: 048, Training: Loss: 0.5881, Accuracy: 0.5625\n",
      "Batch number: 049, Training: Loss: 0.4514, Accuracy: 0.7188\n",
      "Batch number: 050, Training: Loss: 0.4683, Accuracy: 0.7500\n",
      "Batch number: 051, Training: Loss: 0.4742, Accuracy: 0.6875\n",
      "Batch number: 052, Training: Loss: 0.4871, Accuracy: 0.7500\n",
      "Batch number: 053, Training: Loss: 0.5373, Accuracy: 0.6562\n",
      "Batch number: 054, Training: Loss: 0.4345, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.4569, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 0.3629, Accuracy: 0.7812\n",
      "Batch number: 057, Training: Loss: 0.5027, Accuracy: 0.6250\n",
      "Batch number: 058, Training: Loss: 0.4495, Accuracy: 0.7188\n",
      "Batch number: 059, Training: Loss: 0.4825, Accuracy: 0.7500\n",
      "Batch number: 060, Training: Loss: 0.4464, Accuracy: 0.6250\n",
      "Batch number: 061, Training: Loss: 0.4245, Accuracy: 0.7500\n",
      "Batch number: 062, Training: Loss: 0.5707, Accuracy: 0.4688\n",
      "Batch number: 063, Training: Loss: 0.4810, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 0.5056, Accuracy: 0.7188\n",
      "Batch number: 065, Training: Loss: 0.5173, Accuracy: 0.6562\n",
      "Batch number: 066, Training: Loss: 0.5113, Accuracy: 0.6875\n",
      "Batch number: 067, Training: Loss: 0.4509, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.4550, Accuracy: 0.7812\n",
      "Batch number: 069, Training: Loss: 0.4674, Accuracy: 0.7188\n",
      "Batch number: 070, Training: Loss: 0.4802, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.4105, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 0.3849, Accuracy: 0.8438\n",
      "Batch number: 073, Training: Loss: 0.4716, Accuracy: 0.7812\n",
      "Batch number: 074, Training: Loss: 0.4657, Accuracy: 0.5625\n",
      "Batch number: 075, Training: Loss: 0.4622, Accuracy: 0.6875\n",
      "Batch number: 076, Training: Loss: 0.4046, Accuracy: 0.7500\n",
      "Batch number: 077, Training: Loss: 0.3716, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 0.4815, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 0.4664, Accuracy: 0.7188\n",
      "Batch number: 080, Training: Loss: 0.4610, Accuracy: 0.5938\n",
      "Batch number: 081, Training: Loss: 0.4614, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 0.4365, Accuracy: 0.6250\n",
      "Batch number: 083, Training: Loss: 0.3865, Accuracy: 0.7500\n",
      "Batch number: 084, Training: Loss: 0.4122, Accuracy: 0.7188\n",
      "Batch number: 085, Training: Loss: 1.1820, Accuracy: 0.7188\n",
      "Batch number: 086, Training: Loss: 0.3942, Accuracy: 0.7188\n",
      "Batch number: 087, Training: Loss: 2.2998, Accuracy: 0.7500\n",
      "Batch number: 088, Training: Loss: 0.4370, Accuracy: 0.6562\n",
      "Batch number: 089, Training: Loss: 0.4045, Accuracy: 0.7500\n",
      "Batch number: 090, Training: Loss: 0.4122, Accuracy: 0.7188\n",
      "Batch number: 091, Training: Loss: 0.4179, Accuracy: 0.8438\n",
      "Batch number: 092, Training: Loss: 0.3275, Accuracy: 0.8438\n",
      "Batch number: 093, Training: Loss: 0.3145, Accuracy: 0.9688\n",
      "Batch number: 094, Training: Loss: 0.6192, Accuracy: 0.8438\n",
      "Batch number: 095, Training: Loss: 0.3427, Accuracy: 0.7812\n",
      "Batch number: 096, Training: Loss: 0.2820, Accuracy: 0.8750\n",
      "Batch number: 097, Training: Loss: 9.0751, Accuracy: 0.8438\n",
      "Batch number: 098, Training: Loss: 0.3596, Accuracy: 0.8438\n",
      "Batch number: 099, Training: Loss: 0.6176, Accuracy: 0.6875\n",
      "Batch number: 100, Training: Loss: 0.4094, Accuracy: 0.7500\n",
      "Batch number: 101, Training: Loss: 0.4141, Accuracy: 0.7188\n",
      "Batch number: 102, Training: Loss: 0.5366, Accuracy: 0.6562\n",
      "Batch number: 103, Training: Loss: 0.5407, Accuracy: 0.5938\n",
      "Batch number: 104, Training: Loss: 0.5915, Accuracy: 0.5938\n",
      "Batch number: 105, Training: Loss: 0.5050, Accuracy: 0.7500\n",
      "Batch number: 106, Training: Loss: 0.6119, Accuracy: 0.4375\n",
      "Batch number: 107, Training: Loss: 0.5979, Accuracy: 0.6250\n",
      "Batch number: 108, Training: Loss: 0.6722, Accuracy: 0.4062\n",
      "Batch number: 109, Training: Loss: 0.5223, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 0.6390, Accuracy: 0.5000\n",
      "Batch number: 111, Training: Loss: 0.6111, Accuracy: 0.5312\n",
      "Batch number: 112, Training: Loss: 0.7181, Accuracy: 0.4062\n",
      "Batch number: 113, Training: Loss: 0.6090, Accuracy: 0.5312\n",
      "Batch number: 114, Training: Loss: 0.6077, Accuracy: 0.5312\n",
      "Batch number: 115, Training: Loss: 0.6854, Accuracy: 0.4375\n",
      "Batch number: 116, Training: Loss: 0.6925, Accuracy: 0.4688\n",
      "Batch number: 117, Training: Loss: 0.6941, Accuracy: 0.3125\n",
      "Batch number: 118, Training: Loss: 0.6995, Accuracy: 0.3438\n",
      "Batch number: 119, Training: Loss: 0.6374, Accuracy: 0.4375\n",
      "Batch number: 120, Training: Loss: 0.6103, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.6644, Accuracy: 0.3438\n",
      "Batch number: 122, Training: Loss: 0.6173, Accuracy: 0.5000\n",
      "Batch number: 123, Training: Loss: 0.6224, Accuracy: 0.4375\n",
      "Batch number: 124, Training: Loss: 0.6480, Accuracy: 0.5625\n",
      "Batch number: 125, Training: Loss: 0.6796, Accuracy: 0.4062\n",
      "Batch number: 126, Training: Loss: 0.6365, Accuracy: 0.3750\n",
      "Batch number: 127, Training: Loss: 0.6229, Accuracy: 0.5000\n",
      "Batch number: 128, Training: Loss: 0.6498, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.6475, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 1.1512, Accuracy: 0.6562\n",
      "Batch number: 131, Training: Loss: 0.6631, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.6507, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5943, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.6280, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.6339, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5991, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.6177, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.6171, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.6106, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.6557, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.6444, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.6010, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 1.4219, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5983, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5975, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.7561, Accuracy: 0.5625\n",
      "Batch number: 147, Training: Loss: 0.5910, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.6633, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5921, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.6367, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5981, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.6199, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.6324, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5574, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.6167, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.8966, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.4862, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 005, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3552, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3931, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5423, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.3931, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.1310, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.8966, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.3931, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.3931, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.6725, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.2621, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.4866, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.6725, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3552, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3931, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.5241, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3552, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3552, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.3552, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.2621, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1310, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.6725, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.4483, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3931, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.4483, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.4483, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.5241, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.6725, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.6725, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3931, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.5241, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5795, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.5241, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5794, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.5241, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.5241, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3552, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3552, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.2621, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5794, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 122, Validation: Loss: 0.3552, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.4483, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.3934, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.2242, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.8035, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.7104, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6173, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.5241, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.4862, Accuracy: 0.7500\n",
      "Epoch : 008, Training: Loss : 0.6091, Accuracy: 64.5316%\n",
      "Validation : Loss : 0.5812, Accuracy: 61.5108%, Time: 50.7151s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 10/25\n",
      "Batch number: 000, Training: Loss: 0.5265, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5586, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5815, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5938, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.4742, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.5743, Accuracy: 0.7188\n",
      "Batch number: 006, Training: Loss: 1.8026, Accuracy: 0.6875\n",
      "Batch number: 007, Training: Loss: 0.8149, Accuracy: 0.5938\n",
      "Batch number: 008, Training: Loss: 0.6346, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 1.0193, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5181, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.6065, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.6101, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.5215, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.6852, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.5539, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5878, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.9022, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.5409, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.5502, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.6130, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.6340, Accuracy: 0.6250\n",
      "Batch number: 022, Training: Loss: 0.9488, Accuracy: 0.5625\n",
      "Batch number: 023, Training: Loss: 0.6240, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5709, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.5631, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.5976, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5538, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.5953, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5388, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.5786, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.6127, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.6168, Accuracy: 0.4688\n",
      "Batch number: 033, Training: Loss: 0.6270, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.5979, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.6124, Accuracy: 0.5000\n",
      "Batch number: 036, Training: Loss: 0.6035, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.6466, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.5626, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.5668, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.5858, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.5859, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 0.6125, Accuracy: 0.5000\n",
      "Batch number: 043, Training: Loss: 0.5884, Accuracy: 0.6250\n",
      "Batch number: 044, Training: Loss: 0.6947, Accuracy: 0.5312\n",
      "Batch number: 045, Training: Loss: 0.5583, Accuracy: 0.7812\n",
      "Batch number: 046, Training: Loss: 0.5697, Accuracy: 0.5938\n",
      "Batch number: 047, Training: Loss: 0.7285, Accuracy: 0.5312\n",
      "Batch number: 048, Training: Loss: 0.6552, Accuracy: 0.5938\n",
      "Batch number: 049, Training: Loss: 0.5799, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 0.6844, Accuracy: 0.5625\n",
      "Batch number: 051, Training: Loss: 0.5800, Accuracy: 0.6250\n",
      "Batch number: 052, Training: Loss: 0.6336, Accuracy: 0.5312\n",
      "Batch number: 053, Training: Loss: 0.6092, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 0.5899, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.5796, Accuracy: 0.5000\n",
      "Batch number: 056, Training: Loss: 0.5905, Accuracy: 0.6875\n",
      "Batch number: 057, Training: Loss: 0.6147, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 0.5803, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.6328, Accuracy: 0.5312\n",
      "Batch number: 060, Training: Loss: 0.5624, Accuracy: 0.7500\n",
      "Batch number: 061, Training: Loss: 0.5997, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.6395, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.5946, Accuracy: 0.5312\n",
      "Batch number: 064, Training: Loss: 0.6236, Accuracy: 0.5312\n",
      "Batch number: 065, Training: Loss: 0.5850, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 0.5901, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.5507, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.5760, Accuracy: 0.5312\n",
      "Batch number: 069, Training: Loss: 0.5658, Accuracy: 0.5938\n",
      "Batch number: 070, Training: Loss: 0.5750, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.5515, Accuracy: 0.5625\n",
      "Batch number: 072, Training: Loss: 0.6482, Accuracy: 0.5625\n",
      "Batch number: 073, Training: Loss: 0.5802, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 0.5522, Accuracy: 0.8125\n",
      "Batch number: 075, Training: Loss: 0.5025, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 0.5757, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.6192, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.6146, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 0.5848, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.5374, Accuracy: 0.7812\n",
      "Batch number: 081, Training: Loss: 0.6235, Accuracy: 0.4688\n",
      "Batch number: 082, Training: Loss: 0.5388, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 0.5123, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.5121, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.5957, Accuracy: 0.5312\n",
      "Batch number: 086, Training: Loss: 0.4928, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.5118, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.4436, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 1.2318, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.5930, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.5843, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.5173, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.6411, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.5408, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.6004, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.6062, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.5804, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.6045, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.6309, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.5930, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5682, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.6104, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.6114, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.6294, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.6291, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.5867, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.6045, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.5798, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.6174, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.5610, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5610, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.6359, Accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 113, Training: Loss: 0.5984, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5984, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.6171, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.6295, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.6046, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.5978, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.5951, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5932, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.5701, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5796, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5284, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.6284, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.6157, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.5919, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.5700, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.6045, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5893, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5203, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.5954, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.6059, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5449, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5801, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5711, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5274, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5450, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5274, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5185, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.4924, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5184, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5280, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.4560, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5275, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.4993, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5277, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5700, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.5679, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.4975, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5586, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5067, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.4441, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5258, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.4733, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5229, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7898, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6974, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6974, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1974, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5923, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5923, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1513, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5923, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7898, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5923, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3949, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6974, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5923, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3949, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.3518, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6050, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6974, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6974, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.1974, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.6974, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5000, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 082, Validation: Loss: 0.1513, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5923, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3949, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1974, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5923, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.3949, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3949, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3949, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6974, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.4538, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5923, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5923, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.6974, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.4538, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4538, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.3025, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3487, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3949, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4538, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1974, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7436, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5461, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6512, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6050, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5000, Accuracy: 0.7500\n",
      "Epoch : 009, Training: Loss : 0.5983, Accuracy: 62.1898%\n",
      "Validation : Loss : 0.5129, Accuracy: 61.5108%, Time: 50.4671s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 11/25\n",
      "Batch number: 000, Training: Loss: 0.4737, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5043, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.8373, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5079, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.4703, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4744, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.5021, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5495, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5332, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.4704, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.4871, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.4475, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5366, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.8348, Accuracy: 0.6250\n",
      "Batch number: 014, Training: Loss: 0.5336, Accuracy: 0.4688\n",
      "Batch number: 015, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5449, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.4651, Accuracy: 0.5938\n",
      "Batch number: 018, Training: Loss: 0.4936, Accuracy: 0.7500\n",
      "Batch number: 019, Training: Loss: 8.9051, Accuracy: 0.7500\n",
      "Batch number: 020, Training: Loss: 2.7595, Accuracy: 0.4375\n",
      "Batch number: 021, Training: Loss: 0.4978, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.4781, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.4777, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5203, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.4548, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.4983, Accuracy: 0.6250\n",
      "Batch number: 028, Training: Loss: 0.5065, Accuracy: 0.6875\n",
      "Batch number: 029, Training: Loss: 0.5210, Accuracy: 0.5312\n",
      "Batch number: 030, Training: Loss: 0.4565, Accuracy: 0.5625\n",
      "Batch number: 031, Training: Loss: 0.5189, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 0.5831, Accuracy: 0.6875\n",
      "Batch number: 033, Training: Loss: 0.4546, Accuracy: 0.6875\n",
      "Batch number: 034, Training: Loss: 0.5054, Accuracy: 0.6562\n",
      "Batch number: 035, Training: Loss: 0.5616, Accuracy: 0.6562\n",
      "Batch number: 036, Training: Loss: 0.5143, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 0.5434, Accuracy: 0.5625\n",
      "Batch number: 038, Training: Loss: 0.5160, Accuracy: 0.7188\n",
      "Batch number: 039, Training: Loss: 0.4334, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.4335, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.4957, Accuracy: 0.6875\n",
      "Batch number: 042, Training: Loss: 0.5782, Accuracy: 0.6875\n",
      "Batch number: 043, Training: Loss: 0.4953, Accuracy: 0.6875\n",
      "Batch number: 044, Training: Loss: 0.5844, Accuracy: 0.6250\n",
      "Batch number: 045, Training: Loss: 0.4595, Accuracy: 0.6250\n",
      "Batch number: 046, Training: Loss: 0.5121, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.4420, Accuracy: 0.8125\n",
      "Batch number: 048, Training: Loss: 0.5176, Accuracy: 0.6562\n",
      "Batch number: 049, Training: Loss: 0.5445, Accuracy: 0.5938\n",
      "Batch number: 050, Training: Loss: 0.5373, Accuracy: 0.6562\n",
      "Batch number: 051, Training: Loss: 0.4503, Accuracy: 0.7188\n",
      "Batch number: 052, Training: Loss: 0.5805, Accuracy: 0.6250\n",
      "Batch number: 053, Training: Loss: 0.5608, Accuracy: 0.6250\n",
      "Batch number: 054, Training: Loss: 0.5305, Accuracy: 0.5625\n",
      "Batch number: 055, Training: Loss: 0.4800, Accuracy: 0.7812\n",
      "Batch number: 056, Training: Loss: 0.4831, Accuracy: 0.6250\n",
      "Batch number: 057, Training: Loss: 0.5027, Accuracy: 0.6250\n",
      "Batch number: 058, Training: Loss: 0.4979, Accuracy: 0.6562\n",
      "Batch number: 059, Training: Loss: 0.4833, Accuracy: 0.7500\n",
      "Batch number: 060, Training: Loss: 0.5178, Accuracy: 0.5312\n",
      "Batch number: 061, Training: Loss: 0.6233, Accuracy: 0.4688\n",
      "Batch number: 062, Training: Loss: 0.6144, Accuracy: 0.4062\n",
      "Batch number: 063, Training: Loss: 0.5802, Accuracy: 0.6250\n",
      "Batch number: 064, Training: Loss: 0.8465, Accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 065, Training: Loss: 0.5652, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 0.5994, Accuracy: 0.5938\n",
      "Batch number: 067, Training: Loss: 0.6016, Accuracy: 0.4688\n",
      "Batch number: 068, Training: Loss: 0.4855, Accuracy: 0.7500\n",
      "Batch number: 069, Training: Loss: 0.5648, Accuracy: 0.5938\n",
      "Batch number: 070, Training: Loss: 0.5488, Accuracy: 0.5625\n",
      "Batch number: 071, Training: Loss: 0.5137, Accuracy: 0.6875\n",
      "Batch number: 072, Training: Loss: 0.5609, Accuracy: 0.6250\n",
      "Batch number: 073, Training: Loss: 0.5539, Accuracy: 0.6875\n",
      "Batch number: 074, Training: Loss: 0.5661, Accuracy: 0.4062\n",
      "Batch number: 075, Training: Loss: 0.5677, Accuracy: 0.5625\n",
      "Batch number: 076, Training: Loss: 0.5938, Accuracy: 0.5000\n",
      "Batch number: 077, Training: Loss: 0.6133, Accuracy: 0.5000\n",
      "Batch number: 078, Training: Loss: 0.5462, Accuracy: 0.5625\n",
      "Batch number: 079, Training: Loss: 0.5409, Accuracy: 0.6250\n",
      "Batch number: 080, Training: Loss: 0.5310, Accuracy: 0.4688\n",
      "Batch number: 081, Training: Loss: 0.5337, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 0.5955, Accuracy: 0.3750\n",
      "Batch number: 083, Training: Loss: 0.5226, Accuracy: 0.5625\n",
      "Batch number: 084, Training: Loss: 0.4997, Accuracy: 0.5938\n",
      "Batch number: 085, Training: Loss: 0.6279, Accuracy: 0.5625\n",
      "Batch number: 086, Training: Loss: 0.5859, Accuracy: 0.4688\n",
      "Batch number: 087, Training: Loss: 0.4549, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.5399, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.5405, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.5189, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.6515, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.5809, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.5298, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.5680, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.4930, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.5974, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.5488, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.5436, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.5584, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.5178, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5349, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.5674, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.5176, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.5877, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.5779, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.5243, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.5946, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.5108, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.5614, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.5209, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5811, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.5611, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.5409, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 2.3034, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.5611, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.5266, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.5468, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.5524, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.5147, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5673, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.4678, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5410, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5541, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.5353, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.5410, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.4904, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.5646, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.5720, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5175, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5624, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.5571, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5923, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.4740, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5491, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5618, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5017, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5608, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5608, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5765, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.4821, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5174, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5247, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.4626, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5328, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.4867, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5327, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5455, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.4953, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.4865, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5296, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5015, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5715, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.4643, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5304, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7423, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3491, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3231, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1856, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1616, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7423, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.3232, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.3231, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3712, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3231, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 040, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3712, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3231, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6463, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.2233, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5336, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3231, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1616, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3712, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1856, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.3712, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.4847, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3712, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3712, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.3231, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.4847, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3231, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3712, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.4847, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4847, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3231, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5567, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.3231, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3712, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4145, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5327, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1856, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7183, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6703, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6463, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5087, Accuracy: 0.7500\n",
      "Epoch : 010, Training: Loss : 0.6142, Accuracy: 62.3899%\n",
      "Validation : Loss : 0.5110, Accuracy: 61.5108%, Time: 50.4349s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 12/25\n",
      "Batch number: 000, Training: Loss: 0.4541, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5550, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5003, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5028, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.4467, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4951, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.4385, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5045, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.4651, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.4571, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.4299, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.4193, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.7304, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.4767, Accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 014, Training: Loss: 0.5039, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.3909, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.4768, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.3879, Accuracy: 0.9062\n",
      "Batch number: 018, Training: Loss: 0.4562, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 0.4569, Accuracy: 0.6250\n",
      "Batch number: 020, Training: Loss: 0.4673, Accuracy: 0.8438\n",
      "Batch number: 021, Training: Loss: 0.8982, Accuracy: 0.6250\n",
      "Batch number: 022, Training: Loss: 0.4494, Accuracy: 0.7500\n",
      "Batch number: 023, Training: Loss: 0.3802, Accuracy: 0.8438\n",
      "Batch number: 024, Training: Loss: 0.4711, Accuracy: 0.7188\n",
      "Batch number: 025, Training: Loss: 0.3872, Accuracy: 0.7500\n",
      "Batch number: 026, Training: Loss: 0.4977, Accuracy: 0.6562\n",
      "Batch number: 027, Training: Loss: 0.4543, Accuracy: 0.6875\n",
      "Batch number: 028, Training: Loss: 0.5127, Accuracy: 0.6875\n",
      "Batch number: 029, Training: Loss: 0.4642, Accuracy: 0.6250\n",
      "Batch number: 030, Training: Loss: 0.4748, Accuracy: 0.5625\n",
      "Batch number: 031, Training: Loss: 0.4390, Accuracy: 0.7812\n",
      "Batch number: 032, Training: Loss: 0.5448, Accuracy: 0.7188\n",
      "Batch number: 033, Training: Loss: 0.5283, Accuracy: 0.5938\n",
      "Batch number: 034, Training: Loss: 0.5011, Accuracy: 0.6562\n",
      "Batch number: 035, Training: Loss: 0.5058, Accuracy: 0.7188\n",
      "Batch number: 036, Training: Loss: 0.4633, Accuracy: 0.8438\n",
      "Batch number: 037, Training: Loss: 0.5295, Accuracy: 0.5938\n",
      "Batch number: 038, Training: Loss: 0.5048, Accuracy: 0.7188\n",
      "Batch number: 039, Training: Loss: 0.4618, Accuracy: 0.6562\n",
      "Batch number: 040, Training: Loss: 0.3613, Accuracy: 0.7812\n",
      "Batch number: 041, Training: Loss: 0.5171, Accuracy: 0.6562\n",
      "Batch number: 042, Training: Loss: 0.4650, Accuracy: 0.8125\n",
      "Batch number: 043, Training: Loss: 0.4668, Accuracy: 0.7188\n",
      "Batch number: 044, Training: Loss: 0.4778, Accuracy: 0.7500\n",
      "Batch number: 045, Training: Loss: 0.4706, Accuracy: 0.6250\n",
      "Batch number: 046, Training: Loss: 3.4319, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.5680, Accuracy: 0.7500\n",
      "Batch number: 048, Training: Loss: 0.4405, Accuracy: 0.7500\n",
      "Batch number: 049, Training: Loss: 0.5522, Accuracy: 0.5938\n",
      "Batch number: 050, Training: Loss: 0.5099, Accuracy: 0.6875\n",
      "Batch number: 051, Training: Loss: 2.6012, Accuracy: 0.6250\n",
      "Batch number: 052, Training: Loss: 0.5012, Accuracy: 0.7188\n",
      "Batch number: 053, Training: Loss: 0.6148, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 0.4393, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.4923, Accuracy: 0.7500\n",
      "Batch number: 056, Training: Loss: 0.5185, Accuracy: 0.5938\n",
      "Batch number: 057, Training: Loss: 0.5371, Accuracy: 0.5938\n",
      "Batch number: 058, Training: Loss: 0.5271, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.5799, Accuracy: 0.6250\n",
      "Batch number: 060, Training: Loss: 0.5086, Accuracy: 0.5625\n",
      "Batch number: 061, Training: Loss: 0.5266, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.6893, Accuracy: 0.3438\n",
      "Batch number: 063, Training: Loss: 0.5539, Accuracy: 0.6562\n",
      "Batch number: 064, Training: Loss: 0.6053, Accuracy: 0.5938\n",
      "Batch number: 065, Training: Loss: 0.5432, Accuracy: 0.6250\n",
      "Batch number: 066, Training: Loss: 0.5591, Accuracy: 0.6250\n",
      "Batch number: 067, Training: Loss: 0.5631, Accuracy: 0.5312\n",
      "Batch number: 068, Training: Loss: 0.4794, Accuracy: 0.7500\n",
      "Batch number: 069, Training: Loss: 0.5174, Accuracy: 0.6562\n",
      "Batch number: 070, Training: Loss: 0.5294, Accuracy: 0.5938\n",
      "Batch number: 071, Training: Loss: 0.5362, Accuracy: 0.6562\n",
      "Batch number: 072, Training: Loss: 0.4871, Accuracy: 0.7188\n",
      "Batch number: 073, Training: Loss: 0.5254, Accuracy: 0.7188\n",
      "Batch number: 074, Training: Loss: 0.4571, Accuracy: 0.5625\n",
      "Batch number: 075, Training: Loss: 0.4737, Accuracy: 0.6875\n",
      "Batch number: 076, Training: Loss: 0.4788, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.4254, Accuracy: 0.7500\n",
      "Batch number: 078, Training: Loss: 0.5509, Accuracy: 0.5625\n",
      "Batch number: 079, Training: Loss: 0.4932, Accuracy: 0.6875\n",
      "Batch number: 080, Training: Loss: 0.5180, Accuracy: 0.5000\n",
      "Batch number: 081, Training: Loss: 0.4526, Accuracy: 0.8438\n",
      "Batch number: 082, Training: Loss: 0.3995, Accuracy: 0.6562\n",
      "Batch number: 083, Training: Loss: 0.3633, Accuracy: 0.7812\n",
      "Batch number: 084, Training: Loss: 0.3634, Accuracy: 0.7812\n",
      "Batch number: 085, Training: Loss: 0.3915, Accuracy: 0.8750\n",
      "Batch number: 086, Training: Loss: 0.3876, Accuracy: 0.7500\n",
      "Batch number: 087, Training: Loss: 0.3397, Accuracy: 0.8125\n",
      "Batch number: 088, Training: Loss: 0.3751, Accuracy: 0.7188\n",
      "Batch number: 089, Training: Loss: 6.3995, Accuracy: 0.7188\n",
      "Batch number: 090, Training: Loss: 0.3592, Accuracy: 0.8125\n",
      "Batch number: 091, Training: Loss: 0.4099, Accuracy: 0.8750\n",
      "Batch number: 092, Training: Loss: 0.4065, Accuracy: 0.7500\n",
      "Batch number: 093, Training: Loss: 0.4075, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.4351, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 0.3918, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.4111, Accuracy: 0.7188\n",
      "Batch number: 097, Training: Loss: 0.4813, Accuracy: 0.7500\n",
      "Batch number: 098, Training: Loss: 0.3758, Accuracy: 0.8438\n",
      "Batch number: 099, Training: Loss: 0.3802, Accuracy: 0.7812\n",
      "Batch number: 100, Training: Loss: 2.0510, Accuracy: 0.6875\n",
      "Batch number: 101, Training: Loss: 0.4874, Accuracy: 0.6250\n",
      "Batch number: 102, Training: Loss: 24.0812, Accuracy: 0.6250\n",
      "Batch number: 103, Training: Loss: 0.4047, Accuracy: 0.7500\n",
      "Batch number: 104, Training: Loss: 0.5359, Accuracy: 0.6562\n",
      "Batch number: 105, Training: Loss: 0.4878, Accuracy: 0.7812\n",
      "Batch number: 106, Training: Loss: 0.5899, Accuracy: 0.4375\n",
      "Batch number: 107, Training: Loss: 0.5719, Accuracy: 0.6562\n",
      "Batch number: 108, Training: Loss: 0.6009, Accuracy: 0.4688\n",
      "Batch number: 109, Training: Loss: 0.5305, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 0.5515, Accuracy: 0.5938\n",
      "Batch number: 111, Training: Loss: 0.6288, Accuracy: 0.5000\n",
      "Batch number: 112, Training: Loss: 0.6800, Accuracy: 0.4375\n",
      "Batch number: 113, Training: Loss: 0.6022, Accuracy: 0.5312\n",
      "Batch number: 114, Training: Loss: 0.7035, Accuracy: 0.4062\n",
      "Batch number: 115, Training: Loss: 0.6512, Accuracy: 0.4688\n",
      "Batch number: 116, Training: Loss: 0.6868, Accuracy: 0.4688\n",
      "Batch number: 117, Training: Loss: 0.7407, Accuracy: 0.2812\n",
      "Batch number: 118, Training: Loss: 0.6876, Accuracy: 0.3438\n",
      "Batch number: 119, Training: Loss: 0.5566, Accuracy: 0.5312\n",
      "Batch number: 120, Training: Loss: 0.6087, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.6546, Accuracy: 0.3438\n",
      "Batch number: 122, Training: Loss: 0.6138, Accuracy: 0.5000\n",
      "Batch number: 123, Training: Loss: 0.6398, Accuracy: 0.4062\n",
      "Batch number: 124, Training: Loss: 0.6034, Accuracy: 0.6250\n",
      "Batch number: 125, Training: Loss: 0.6794, Accuracy: 0.4375\n",
      "Batch number: 126, Training: Loss: 0.6529, Accuracy: 0.3438\n",
      "Batch number: 127, Training: Loss: 0.6279, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.6499, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.6446, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5933, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.6606, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.6999, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.6126, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.6477, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.6136, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.6179, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.6738, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.6540, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.6094, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.5877, Accuracy: 0.6562\n",
      "Batch number: 141, Training: Loss: 0.6078, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.7559, Accuracy: 0.7188\n",
      "Batch number: 143, Training: Loss: 0.6327, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5964, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.6074, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5944, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.6084, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.6473, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.6258, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.6703, Accuracy: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 151, Training: Loss: 0.6040, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.6195, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.6337, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5882, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.6507, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.9046, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3890, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.3890, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.9046, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.3890, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.6785, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3890, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.2593, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.4523, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.4523, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.6785, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.6785, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.6152, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 112, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3558, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.2769, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.5820, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.8081, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.7117, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6152, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.5187, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.4855, Accuracy: 0.7500\n",
      "Epoch : 011, Training: Loss : 0.7604, Accuracy: 64.5516%\n",
      "Validation : Loss : 0.6198, Accuracy: 61.5108%, Time: 50.4996s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 13/25\n",
      "Batch number: 000, Training: Loss: 0.5745, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5767, Accuracy: 0.7188\n",
      "Batch number: 002, Training: Loss: 0.6148, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.6154, Accuracy: 0.6250\n",
      "Batch number: 004, Training: Loss: 0.5365, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.5897, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.7842, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.6251, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.6700, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 1.1406, Accuracy: 0.6562\n",
      "Batch number: 010, Training: Loss: 0.5628, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.6574, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.6661, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.5705, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.6945, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.6021, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.6726, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.5894, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.5936, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.5457, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.6273, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.5865, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.6102, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.5770, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5599, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.6258, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.6146, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5536, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.5852, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5498, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.5297, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.6180, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.6860, Accuracy: 0.4375\n",
      "Batch number: 033, Training: Loss: 0.6219, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.5976, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5922, Accuracy: 0.5312\n",
      "Batch number: 036, Training: Loss: 0.6373, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.5883, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.5894, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.5979, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.5807, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.6059, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 0.6561, Accuracy: 0.4688\n",
      "Batch number: 043, Training: Loss: 0.6056, Accuracy: 0.5938\n",
      "Batch number: 044, Training: Loss: 0.6095, Accuracy: 0.5625\n",
      "Batch number: 045, Training: Loss: 0.5755, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 0.5650, Accuracy: 0.5312\n",
      "Batch number: 047, Training: Loss: 0.6008, Accuracy: 0.5312\n",
      "Batch number: 048, Training: Loss: 0.5579, Accuracy: 0.6562\n",
      "Batch number: 049, Training: Loss: 0.8597, Accuracy: 0.6875\n",
      "Batch number: 050, Training: Loss: 0.5925, Accuracy: 0.5625\n",
      "Batch number: 051, Training: Loss: 0.5797, Accuracy: 0.6250\n",
      "Batch number: 052, Training: Loss: 0.5980, Accuracy: 0.5312\n",
      "Batch number: 053, Training: Loss: 0.6293, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 0.5683, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.5643, Accuracy: 0.5000\n",
      "Batch number: 056, Training: Loss: 0.5500, Accuracy: 0.6875\n",
      "Batch number: 057, Training: Loss: 0.5555, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 0.5416, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.6336, Accuracy: 0.5312\n",
      "Batch number: 060, Training: Loss: 0.5798, Accuracy: 0.7500\n",
      "Batch number: 061, Training: Loss: 0.5608, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.5994, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.6452, Accuracy: 0.5312\n",
      "Batch number: 064, Training: Loss: 0.6721, Accuracy: 0.5312\n",
      "Batch number: 065, Training: Loss: 0.6428, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 0.5708, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.6086, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.5759, Accuracy: 0.5312\n",
      "Batch number: 069, Training: Loss: 0.5658, Accuracy: 0.5938\n",
      "Batch number: 070, Training: Loss: 0.5745, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.5900, Accuracy: 0.5625\n",
      "Batch number: 072, Training: Loss: 0.5707, Accuracy: 0.5625\n",
      "Batch number: 073, Training: Loss: 1.3119, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 0.5806, Accuracy: 0.8125\n",
      "Batch number: 075, Training: Loss: 0.5802, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 0.6529, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.6188, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.5559, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 0.5658, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.4969, Accuracy: 0.7812\n",
      "Batch number: 081, Training: Loss: 0.6253, Accuracy: 0.4688\n",
      "Batch number: 082, Training: Loss: 0.5535, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 0.5118, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.5117, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.5781, Accuracy: 0.5312\n",
      "Batch number: 086, Training: Loss: 0.5681, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.5302, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.4991, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.4987, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.5546, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.5686, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.5545, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.5690, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.5234, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.5046, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.4924, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 2.1748, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.5294, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.5173, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.5173, Accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 101, Training: Loss: 0.5498, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.5907, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.4981, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.5540, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.5858, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.4983, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.5792, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.5085, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.5472, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.5214, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5212, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.5410, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.4808, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5006, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.5206, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.5869, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.5085, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.5135, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.5159, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5248, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.4498, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.4997, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5366, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.5290, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.5203, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.4935, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.4809, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.5881, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5469, Accuracy: 0.6250\n",
      "Batch number: 130, Training: Loss: 0.4905, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.4353, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5463, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5167, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5444, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5201, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5828, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.4574, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5395, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.4991, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.4977, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5144, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.4991, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5590, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.4799, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5378, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5218, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.5044, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5218, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5441, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.4573, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.4767, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5448, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5146, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.4760, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7076, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6932, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6932, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1769, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1697, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7076, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.3475, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6932, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6789, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6932, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6932, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 073, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.1769, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1697, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1769, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6932, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.6789, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.5092, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3395, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5307, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.1697, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3466, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.5092, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.3538, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1769, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7004, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5235, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6861, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6789, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5164, Accuracy: 0.7500\n",
      "Epoch : 012, Training: Loss : 0.5824, Accuracy: 62.1097%\n",
      "Validation : Loss : 0.4909, Accuracy: 61.5108%, Time: 50.9364s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 14/25\n",
      "Batch number: 000, Training: Loss: 0.5606, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5183, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5624, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5207, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.4522, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4970, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.4563, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5430, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5801, Accuracy: 0.5000\n",
      "Batch number: 009, Training: Loss: 0.4962, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5166, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.4793, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5457, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.4979, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.6308, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.4552, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5208, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.4562, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.5199, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.4985, Accuracy: 0.5625\n",
      "Batch number: 020, Training: Loss: 0.4970, Accuracy: 0.8125\n",
      "Batch number: 021, Training: Loss: 0.5640, Accuracy: 0.5312\n",
      "Batch number: 022, Training: Loss: 0.4755, Accuracy: 0.7188\n",
      "Batch number: 023, Training: Loss: 0.4312, Accuracy: 0.7812\n",
      "Batch number: 024, Training: Loss: 0.5190, Accuracy: 0.6562\n",
      "Batch number: 025, Training: Loss: 0.5222, Accuracy: 0.5625\n",
      "Batch number: 026, Training: Loss: 0.5423, Accuracy: 0.5938\n",
      "Batch number: 027, Training: Loss: 0.4990, Accuracy: 0.6250\n",
      "Batch number: 028, Training: Loss: 0.5620, Accuracy: 0.6250\n",
      "Batch number: 029, Training: Loss: 0.4797, Accuracy: 0.5938\n",
      "Batch number: 030, Training: Loss: 0.4829, Accuracy: 0.5312\n",
      "Batch number: 031, Training: Loss: 0.4953, Accuracy: 0.7188\n",
      "Batch number: 032, Training: Loss: 0.5352, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.4992, Accuracy: 0.6250\n",
      "Batch number: 034, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 035, Training: Loss: 0.4927, Accuracy: 0.7500\n",
      "Batch number: 036, Training: Loss: 0.5569, Accuracy: 0.7500\n",
      "Batch number: 037, Training: Loss: 0.5222, Accuracy: 0.5938\n",
      "Batch number: 038, Training: Loss: 0.4912, Accuracy: 0.7500\n",
      "Batch number: 039, Training: Loss: 0.5021, Accuracy: 0.5938\n",
      "Batch number: 040, Training: Loss: 0.4566, Accuracy: 0.6562\n",
      "Batch number: 041, Training: Loss: 0.4950, Accuracy: 0.6875\n",
      "Batch number: 042, Training: Loss: 0.5303, Accuracy: 0.7500\n",
      "Batch number: 043, Training: Loss: 0.6103, Accuracy: 0.5312\n",
      "Batch number: 044, Training: Loss: 0.4654, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 0.5303, Accuracy: 0.5312\n",
      "Batch number: 046, Training: Loss: 0.5578, Accuracy: 0.6562\n",
      "Batch number: 047, Training: Loss: 0.3938, Accuracy: 0.8750\n",
      "Batch number: 048, Training: Loss: 0.5410, Accuracy: 0.6250\n",
      "Batch number: 049, Training: Loss: 0.5448, Accuracy: 0.5938\n",
      "Batch number: 050, Training: Loss: 0.5142, Accuracy: 0.6875\n",
      "Batch number: 051, Training: Loss: 0.4264, Accuracy: 0.7500\n",
      "Batch number: 052, Training: Loss: 0.4851, Accuracy: 0.7500\n",
      "Batch number: 053, Training: Loss: 0.5129, Accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 054, Training: Loss: 0.4590, Accuracy: 0.6562\n",
      "Batch number: 055, Training: Loss: 0.5512, Accuracy: 0.6875\n",
      "Batch number: 056, Training: Loss: 0.4837, Accuracy: 0.6250\n",
      "Batch number: 057, Training: Loss: 0.5274, Accuracy: 0.5938\n",
      "Batch number: 058, Training: Loss: 0.4981, Accuracy: 0.6562\n",
      "Batch number: 059, Training: Loss: 0.4337, Accuracy: 0.8125\n",
      "Batch number: 060, Training: Loss: 0.4462, Accuracy: 0.6250\n",
      "Batch number: 061, Training: Loss: 0.4982, Accuracy: 0.6562\n",
      "Batch number: 062, Training: Loss: 0.4220, Accuracy: 0.6562\n",
      "Batch number: 063, Training: Loss: 0.4819, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 0.5063, Accuracy: 0.7188\n",
      "Batch number: 065, Training: Loss: 0.4682, Accuracy: 0.7188\n",
      "Batch number: 066, Training: Loss: 0.5116, Accuracy: 0.6875\n",
      "Batch number: 067, Training: Loss: 0.4112, Accuracy: 0.7188\n",
      "Batch number: 068, Training: Loss: 0.4809, Accuracy: 0.7500\n",
      "Batch number: 069, Training: Loss: 0.4925, Accuracy: 0.6875\n",
      "Batch number: 070, Training: Loss: 0.5543, Accuracy: 0.5625\n",
      "Batch number: 071, Training: Loss: 0.5112, Accuracy: 0.6875\n",
      "Batch number: 072, Training: Loss: 0.4365, Accuracy: 0.7812\n",
      "Batch number: 073, Training: Loss: 0.5484, Accuracy: 0.6875\n",
      "Batch number: 074, Training: Loss: 0.4371, Accuracy: 0.5938\n",
      "Batch number: 075, Training: Loss: 0.5238, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 0.4802, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.4988, Accuracy: 0.6562\n",
      "Batch number: 078, Training: Loss: 0.5552, Accuracy: 0.5625\n",
      "Batch number: 079, Training: Loss: 0.4924, Accuracy: 0.6875\n",
      "Batch number: 080, Training: Loss: 0.4799, Accuracy: 0.5625\n",
      "Batch number: 081, Training: Loss: 0.4682, Accuracy: 0.8125\n",
      "Batch number: 082, Training: Loss: 0.5038, Accuracy: 0.5312\n",
      "Batch number: 083, Training: Loss: 0.5101, Accuracy: 0.5938\n",
      "Batch number: 084, Training: Loss: 0.4604, Accuracy: 0.6562\n",
      "Batch number: 085, Training: Loss: 0.5308, Accuracy: 0.6875\n",
      "Batch number: 086, Training: Loss: 0.4110, Accuracy: 0.7188\n",
      "Batch number: 087, Training: Loss: 0.3866, Accuracy: 0.7500\n",
      "Batch number: 088, Training: Loss: 0.5182, Accuracy: 0.5312\n",
      "Batch number: 089, Training: Loss: 0.5028, Accuracy: 0.6250\n",
      "Batch number: 090, Training: Loss: 0.4544, Accuracy: 0.6875\n",
      "Batch number: 091, Training: Loss: 0.4800, Accuracy: 0.7812\n",
      "Batch number: 092, Training: Loss: 0.3826, Accuracy: 0.7812\n",
      "Batch number: 093, Training: Loss: 0.4329, Accuracy: 0.8438\n",
      "Batch number: 094, Training: Loss: 0.4848, Accuracy: 0.7500\n",
      "Batch number: 095, Training: Loss: 0.4871, Accuracy: 0.5938\n",
      "Batch number: 096, Training: Loss: 0.4287, Accuracy: 0.7188\n",
      "Batch number: 097, Training: Loss: 0.4844, Accuracy: 0.7500\n",
      "Batch number: 098, Training: Loss: 0.4693, Accuracy: 0.7188\n",
      "Batch number: 099, Training: Loss: 0.4303, Accuracy: 0.7188\n",
      "Batch number: 100, Training: Loss: 0.4062, Accuracy: 0.7500\n",
      "Batch number: 101, Training: Loss: 0.4592, Accuracy: 0.6562\n",
      "Batch number: 102, Training: Loss: 0.4883, Accuracy: 0.7188\n",
      "Batch number: 103, Training: Loss: 0.4060, Accuracy: 0.7500\n",
      "Batch number: 104, Training: Loss: 5.3499, Accuracy: 0.6875\n",
      "Batch number: 105, Training: Loss: 0.4724, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 0.4026, Accuracy: 0.6562\n",
      "Batch number: 107, Training: Loss: 20.1849, Accuracy: 0.7500\n",
      "Batch number: 108, Training: Loss: 0.4908, Accuracy: 0.5938\n",
      "Batch number: 109, Training: Loss: 0.5129, Accuracy: 0.7812\n",
      "Batch number: 110, Training: Loss: 0.4997, Accuracy: 0.6562\n",
      "Batch number: 111, Training: Loss: 0.5236, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.5737, Accuracy: 0.5625\n",
      "Batch number: 113, Training: Loss: 0.5739, Accuracy: 0.5625\n",
      "Batch number: 114, Training: Loss: 0.5739, Accuracy: 0.5625\n",
      "Batch number: 115, Training: Loss: 0.6239, Accuracy: 0.5000\n",
      "Batch number: 116, Training: Loss: 0.6110, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.6293, Accuracy: 0.3750\n",
      "Batch number: 118, Training: Loss: 0.6401, Accuracy: 0.4062\n",
      "Batch number: 119, Training: Loss: 0.6277, Accuracy: 0.4688\n",
      "Batch number: 120, Training: Loss: 0.5365, Accuracy: 0.6562\n",
      "Batch number: 121, Training: Loss: 0.6133, Accuracy: 0.4062\n",
      "Batch number: 122, Training: Loss: 0.5932, Accuracy: 0.5312\n",
      "Batch number: 123, Training: Loss: 0.5996, Accuracy: 0.4688\n",
      "Batch number: 124, Training: Loss: 0.5312, Accuracy: 0.7188\n",
      "Batch number: 125, Training: Loss: 0.5669, Accuracy: 0.5625\n",
      "Batch number: 126, Training: Loss: 0.5059, Accuracy: 0.5625\n",
      "Batch number: 127, Training: Loss: 0.8784, Accuracy: 0.6562\n",
      "Batch number: 128, Training: Loss: 0.6051, Accuracy: 0.5938\n",
      "Batch number: 129, Training: Loss: 5.2034, Accuracy: 0.3750\n",
      "Batch number: 130, Training: Loss: 0.5692, Accuracy: 0.4062\n",
      "Batch number: 131, Training: Loss: 0.6518, Accuracy: 0.4062\n",
      "Batch number: 132, Training: Loss: 0.6282, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5403, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.6278, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5815, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.4994, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.6643, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5817, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5783, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.5529, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5373, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.6039, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.5217, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5512, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5224, Accuracy: 0.6250\n",
      "Batch number: 146, Training: Loss: 0.5309, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5666, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.8044, Accuracy: 0.5000\n",
      "Batch number: 149, Training: Loss: 0.5296, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.8890, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5236, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.4799, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5360, Accuracy: 0.5625\n",
      "Batch number: 154, Training: Loss: 6.4284, Accuracy: 0.7500\n",
      "Batch number: 155, Training: Loss: 0.5420, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7959, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6979, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6979, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3000, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1990, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1500, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7959, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.3000, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.4500, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.4990, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 034, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6979, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3000, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.3516, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.4500, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6000, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6979, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6979, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.1990, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3000, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1500, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1990, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3000, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6979, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.4500, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3000, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3000, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.6000, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.6005, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4500, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3000, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5969, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.1500, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3490, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.6979, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4500, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.3979, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1990, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7469, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5479, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6490, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6000, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.4990, Accuracy: 0.7500\n",
      "Epoch : 013, Training: Loss : 0.7448, Accuracy: 64.2114%\n",
      "Validation : Loss : 0.5035, Accuracy: 61.5108%, Time: 50.7472s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 15/25\n",
      "Batch number: 000, Training: Loss: 0.4362, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5113, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5609, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5481, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.5013, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.5114, Accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 006, Training: Loss: 0.4668, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5351, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5596, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.4869, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.4625, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.4959, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5688, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.5364, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 3.1548, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.4785, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5109, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.4745, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.5301, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.5109, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.5363, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.5584, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.6369, Accuracy: 0.5625\n",
      "Batch number: 023, Training: Loss: 0.5021, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5636, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.6204, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.5826, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5394, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.5441, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5785, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.5127, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.5233, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.5908, Accuracy: 0.4688\n",
      "Batch number: 033, Training: Loss: 0.6020, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.5410, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5250, Accuracy: 0.5312\n",
      "Batch number: 036, Training: Loss: 0.5923, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.6233, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.5871, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.6015, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.5595, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.5844, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 0.5899, Accuracy: 0.4688\n",
      "Batch number: 043, Training: Loss: 0.5844, Accuracy: 0.5938\n",
      "Batch number: 044, Training: Loss: 0.6290, Accuracy: 0.5312\n",
      "Batch number: 045, Training: Loss: 0.6213, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 0.5871, Accuracy: 0.5312\n",
      "Batch number: 047, Training: Loss: 0.5871, Accuracy: 0.5312\n",
      "Batch number: 048, Training: Loss: 0.5844, Accuracy: 0.5938\n",
      "Batch number: 049, Training: Loss: 0.5620, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 0.5228, Accuracy: 0.5625\n",
      "Batch number: 051, Training: Loss: 0.5410, Accuracy: 0.6250\n",
      "Batch number: 052, Training: Loss: 0.5659, Accuracy: 0.5312\n",
      "Batch number: 053, Training: Loss: 0.6067, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 0.5810, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.5879, Accuracy: 0.5000\n",
      "Batch number: 056, Training: Loss: 0.5600, Accuracy: 0.6875\n",
      "Batch number: 057, Training: Loss: 0.6033, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 0.5832, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.6290, Accuracy: 0.5312\n",
      "Batch number: 060, Training: Loss: 0.5778, Accuracy: 0.7500\n",
      "Batch number: 061, Training: Loss: 0.6038, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.6178, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.5881, Accuracy: 0.5312\n",
      "Batch number: 064, Training: Loss: 0.5886, Accuracy: 0.5312\n",
      "Batch number: 065, Training: Loss: 0.5637, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 0.5867, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.5767, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.5693, Accuracy: 0.5312\n",
      "Batch number: 069, Training: Loss: 0.5640, Accuracy: 0.5938\n",
      "Batch number: 070, Training: Loss: 0.5989, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.5670, Accuracy: 0.5625\n",
      "Batch number: 072, Training: Loss: 0.5469, Accuracy: 0.5625\n",
      "Batch number: 073, Training: Loss: 0.6337, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 0.5838, Accuracy: 0.8125\n",
      "Batch number: 075, Training: Loss: 0.5409, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 0.6182, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.5810, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.5974, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 0.5646, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.4626, Accuracy: 0.7812\n",
      "Batch number: 081, Training: Loss: 0.6007, Accuracy: 0.4688\n",
      "Batch number: 082, Training: Loss: 0.5599, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 0.5913, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.5517, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.6139, Accuracy: 0.5312\n",
      "Batch number: 086, Training: Loss: 0.5508, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.5963, Accuracy: 0.6562\n",
      "Batch number: 088, Training: Loss: 0.5961, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.5929, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.5548, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.6241, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.6107, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.6072, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.6007, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.5950, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.5471, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.6017, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.6234, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.5177, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.5359, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5464, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.6130, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.5540, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.6131, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.5819, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.5420, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.6097, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.5449, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.6057, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.6162, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.6163, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.6164, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.5796, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5796, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.5612, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.6300, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.5653, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.5411, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.5292, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5750, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.5519, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5980, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5287, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.6089, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.5614, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.5394, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.6052, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.6202, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5722, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5423, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.5721, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.6028, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5101, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5593, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5359, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5436, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5977, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5616, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5540, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.5797, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5540, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.6037, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 143, Training: Loss: 0.5255, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5823, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5335, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5820, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5876, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.6036, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5697, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.6314, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5617, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5178, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5845, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5604, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5657, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.8177, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3500, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.4368, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.6133, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.2913, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.2912, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.8177, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.4368, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.5824, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.6133, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.4089, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.2912, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.6133, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.4089, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3500, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.4368, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.5824, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3500, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3500, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.3500, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.2912, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.2912, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.6133, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.4089, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.4368, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.4089, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.4368, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.6133, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.6133, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.4368, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.2912, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 104, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.5824, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.5824, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.5824, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3500, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3500, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.2912, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.4089, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4368, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5545, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.3500, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7589, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.7001, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6412, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.5824, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.4956, Accuracy: 0.7500\n",
      "Epoch : 014, Training: Loss : 0.5842, Accuracy: 62.0096%\n",
      "Validation : Loss : 0.5735, Accuracy: 61.5108%, Time: 51.0758s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 16/25\n",
      "Batch number: 000, Training: Loss: 0.5649, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5468, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5614, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5869, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.5253, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.5286, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.6108, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5947, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5656, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.5757, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5319, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.5401, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5989, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.5541, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.6415, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.5176, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5566, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.5442, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.5232, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.5612, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.5723, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.5362, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.5854, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.4842, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5468, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.5317, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.5802, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5173, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.6087, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5183, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.4810, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.5107, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.6207, Accuracy: 0.4688\n",
      "Batch number: 033, Training: Loss: 0.5174, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.5411, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5125, Accuracy: 0.5312\n",
      "Batch number: 036, Training: Loss: 0.5822, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.4977, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.5095, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.5152, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.4789, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.5655, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 0.5922, Accuracy: 0.4688\n",
      "Batch number: 043, Training: Loss: 0.5635, Accuracy: 0.5938\n",
      "Batch number: 044, Training: Loss: 0.5454, Accuracy: 0.5312\n",
      "Batch number: 045, Training: Loss: 0.4954, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 0.5229, Accuracy: 0.5312\n",
      "Batch number: 047, Training: Loss: 0.4795, Accuracy: 0.5312\n",
      "Batch number: 048, Training: Loss: 0.4560, Accuracy: 0.5938\n",
      "Batch number: 049, Training: Loss: 0.5415, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 0.4977, Accuracy: 0.7188\n",
      "Batch number: 051, Training: Loss: 0.4322, Accuracy: 0.7500\n",
      "Batch number: 052, Training: Loss: 0.5174, Accuracy: 0.7188\n",
      "Batch number: 053, Training: Loss: 0.5621, Accuracy: 0.6250\n",
      "Batch number: 054, Training: Loss: 0.4354, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.4452, Accuracy: 0.8438\n",
      "Batch number: 056, Training: Loss: 0.5247, Accuracy: 0.5625\n",
      "Batch number: 057, Training: Loss: 0.4313, Accuracy: 0.7188\n",
      "Batch number: 058, Training: Loss: 0.5206, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.4887, Accuracy: 0.7500\n",
      "Batch number: 060, Training: Loss: 0.3944, Accuracy: 0.6875\n",
      "Batch number: 061, Training: Loss: 0.5210, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.4885, Accuracy: 0.5625\n",
      "Batch number: 063, Training: Loss: 0.4866, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 0.4625, Accuracy: 0.7812\n",
      "Batch number: 065, Training: Loss: 0.4699, Accuracy: 0.7188\n",
      "Batch number: 066, Training: Loss: 0.4655, Accuracy: 0.7500\n",
      "Batch number: 067, Training: Loss: 0.4587, Accuracy: 0.6562\n",
      "Batch number: 068, Training: Loss: 0.4600, Accuracy: 0.7812\n",
      "Batch number: 069, Training: Loss: 0.4448, Accuracy: 0.7500\n",
      "Batch number: 070, Training: Loss: 0.4788, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.4632, Accuracy: 0.7500\n",
      "Batch number: 072, Training: Loss: 0.4382, Accuracy: 0.7812\n",
      "Batch number: 073, Training: Loss: 0.4754, Accuracy: 0.7812\n",
      "Batch number: 074, Training: Loss: 0.4603, Accuracy: 0.5625\n",
      "Batch number: 075, Training: Loss: 0.4488, Accuracy: 0.7188\n",
      "Batch number: 076, Training: Loss: 0.5051, Accuracy: 0.6250\n",
      "Batch number: 077, Training: Loss: 0.4487, Accuracy: 0.7188\n",
      "Batch number: 078, Training: Loss: 0.5306, Accuracy: 0.5938\n",
      "Batch number: 079, Training: Loss: 0.5174, Accuracy: 0.6562\n",
      "Batch number: 080, Training: Loss: 0.4316, Accuracy: 0.6250\n",
      "Batch number: 081, Training: Loss: 0.5161, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 0.5070, Accuracy: 0.5312\n",
      "Batch number: 083, Training: Loss: 0.4618, Accuracy: 0.6562\n",
      "Batch number: 084, Training: Loss: 0.4303, Accuracy: 0.7188\n",
      "Batch number: 085, Training: Loss: 0.4297, Accuracy: 0.8125\n",
      "Batch number: 086, Training: Loss: 0.4864, Accuracy: 0.6250\n",
      "Batch number: 087, Training: Loss: 0.4114, Accuracy: 0.7188\n",
      "Batch number: 088, Training: Loss: 0.3988, Accuracy: 0.6875\n",
      "Batch number: 089, Training: Loss: 0.4549, Accuracy: 0.6875\n",
      "Batch number: 090, Training: Loss: 0.4797, Accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 091, Training: Loss: 0.4503, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 0.4520, Accuracy: 0.7188\n",
      "Batch number: 093, Training: Loss: 0.3765, Accuracy: 0.9062\n",
      "Batch number: 094, Training: Loss: 0.4067, Accuracy: 0.8438\n",
      "Batch number: 095, Training: Loss: 0.4422, Accuracy: 0.6562\n",
      "Batch number: 096, Training: Loss: 0.4114, Accuracy: 0.7188\n",
      "Batch number: 097, Training: Loss: 2.4696, Accuracy: 0.6562\n",
      "Batch number: 098, Training: Loss: 0.4924, Accuracy: 0.6875\n",
      "Batch number: 099, Training: Loss: 0.6329, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.3548, Accuracy: 0.8125\n",
      "Batch number: 101, Training: Loss: 0.4870, Accuracy: 0.6250\n",
      "Batch number: 102, Training: Loss: 0.4605, Accuracy: 0.7500\n",
      "Batch number: 103, Training: Loss: 0.3797, Accuracy: 0.7812\n",
      "Batch number: 104, Training: Loss: 0.4854, Accuracy: 0.7188\n",
      "Batch number: 105, Training: Loss: 0.4898, Accuracy: 0.7812\n",
      "Batch number: 106, Training: Loss: 0.4839, Accuracy: 0.5625\n",
      "Batch number: 107, Training: Loss: 0.5216, Accuracy: 0.7188\n",
      "Batch number: 108, Training: Loss: 0.4446, Accuracy: 0.6562\n",
      "Batch number: 109, Training: Loss: 0.4813, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.4484, Accuracy: 0.7188\n",
      "Batch number: 111, Training: Loss: 0.5253, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.3969, Accuracy: 0.7812\n",
      "Batch number: 113, Training: Loss: 0.4740, Accuracy: 0.6875\n",
      "Batch number: 114, Training: Loss: 0.4741, Accuracy: 0.6875\n",
      "Batch number: 115, Training: Loss: 0.4224, Accuracy: 0.7500\n",
      "Batch number: 116, Training: Loss: 0.4582, Accuracy: 0.7500\n",
      "Batch number: 117, Training: Loss: 0.5144, Accuracy: 0.5312\n",
      "Batch number: 118, Training: Loss: 0.4723, Accuracy: 0.6250\n",
      "Batch number: 119, Training: Loss: 0.4917, Accuracy: 0.6250\n",
      "Batch number: 120, Training: Loss: 0.4584, Accuracy: 0.7500\n",
      "Batch number: 121, Training: Loss: 0.4535, Accuracy: 0.6250\n",
      "Batch number: 122, Training: Loss: 0.4740, Accuracy: 0.6875\n",
      "Batch number: 123, Training: Loss: 0.4889, Accuracy: 0.6250\n",
      "Batch number: 124, Training: Loss: 0.5468, Accuracy: 0.6875\n",
      "Batch number: 125, Training: Loss: 0.4484, Accuracy: 0.7188\n",
      "Batch number: 126, Training: Loss: 0.4698, Accuracy: 0.6250\n",
      "Batch number: 127, Training: Loss: 0.4669, Accuracy: 0.7188\n",
      "Batch number: 128, Training: Loss: 0.5544, Accuracy: 0.6562\n",
      "Batch number: 129, Training: Loss: 0.4553, Accuracy: 0.6875\n",
      "Batch number: 130, Training: Loss: 0.5574, Accuracy: 0.4688\n",
      "Batch number: 131, Training: Loss: 0.4802, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.4301, Accuracy: 0.8125\n",
      "Batch number: 133, Training: Loss: 0.5107, Accuracy: 0.5938\n",
      "Batch number: 134, Training: Loss: 0.4371, Accuracy: 0.7812\n",
      "Batch number: 135, Training: Loss: 0.4301, Accuracy: 0.7188\n",
      "Batch number: 136, Training: Loss: 0.4244, Accuracy: 0.7500\n",
      "Batch number: 137, Training: Loss: 0.4983, Accuracy: 0.6562\n",
      "Batch number: 138, Training: Loss: 0.5229, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.4792, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.4247, Accuracy: 0.7500\n",
      "Batch number: 141, Training: Loss: 0.4791, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.4950, Accuracy: 0.5625\n",
      "Batch number: 143, Training: Loss: 0.4494, Accuracy: 0.7188\n",
      "Batch number: 144, Training: Loss: 0.5081, Accuracy: 0.5938\n",
      "Batch number: 145, Training: Loss: 0.4689, Accuracy: 0.7188\n",
      "Batch number: 146, Training: Loss: 0.6282, Accuracy: 0.4375\n",
      "Batch number: 147, Training: Loss: 0.4933, Accuracy: 0.6875\n",
      "Batch number: 148, Training: Loss: 0.4607, Accuracy: 0.7812\n",
      "Batch number: 149, Training: Loss: 0.4696, Accuracy: 0.7188\n",
      "Batch number: 150, Training: Loss: 0.4893, Accuracy: 0.7188\n",
      "Batch number: 151, Training: Loss: 0.4977, Accuracy: 0.6562\n",
      "Batch number: 152, Training: Loss: 0.4543, Accuracy: 0.6875\n",
      "Batch number: 153, Training: Loss: 0.5090, Accuracy: 0.7188\n",
      "Batch number: 154, Training: Loss: 0.4711, Accuracy: 0.5625\n",
      "Batch number: 155, Training: Loss: 0.4139, Accuracy: 0.8438\n",
      "Validation Batch number: 000, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 001, Validation: Loss: 0.6286, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6954, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 005, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.7288, Accuracy: 0.2500\n",
      "Validation Batch number: 007, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 008, Validation: Loss: 0.6954, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 011, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 012, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 014, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 016, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 018, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 019, Validation: Loss: 0.1572, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 021, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 025, Validation: Loss: 0.1905, Accuracy: 0.7500\n",
      "Validation Batch number: 026, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.6286, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 031, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 032, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 034, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3143, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6954, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 040, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 042, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 044, Validation: Loss: 0.7288, Accuracy: 0.2500\n",
      "Validation Batch number: 045, Validation: Loss: 0.7288, Accuracy: 0.2500\n",
      "Validation Batch number: 046, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 050, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.3143, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 053, Validation: Loss: 0.7288, Accuracy: 0.2500\n",
      "Validation Batch number: 054, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 055, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 056, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5382, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 059, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 062, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 063, Validation: Loss: 0.7621, Accuracy: 0.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 065, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 068, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 070, Validation: Loss: 0.6954, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6954, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 074, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 075, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.6954, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 078, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 079, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 080, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 082, Validation: Loss: 0.1905, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 085, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3143, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 089, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.3143, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 092, Validation: Loss: 0.7288, Accuracy: 0.2500\n",
      "Validation Batch number: 093, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3143, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3143, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6954, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 100, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3143, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.2831, Accuracy: 1.0000\n",
      "Validation Batch number: 106, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 108, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 109, Validation: Loss: 0.6954, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 112, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 113, Validation: Loss: 0.7288, Accuracy: 0.2500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.5716, Accuracy: 0.2500\n",
      "Validation Batch number: 116, Validation: Loss: 0.3811, Accuracy: 0.5000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.4715, Accuracy: 1.0000\n",
      "Validation Batch number: 120, Validation: Loss: 0.1905, Accuracy: 0.7500\n",
      "Validation Batch number: 121, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.3477, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 125, Validation: Loss: 0.3143, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 127, Validation: Loss: 0.5716, Accuracy: 0.2500\n",
      "Validation Batch number: 128, Validation: Loss: 0.3143, Accuracy: 1.0000\n",
      "Validation Batch number: 129, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 130, Validation: Loss: 0.7288, Accuracy: 0.2500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 132, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.1572, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.6620, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5048, Accuracy: 0.7500\n",
      "Validation Batch number: 136, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Validation Batch number: 137, Validation: Loss: 0.5716, Accuracy: 0.2500\n",
      "Validation Batch number: 138, Validation: Loss: 0.5382, Accuracy: 0.5000\n",
      "Epoch : 015, Training: Loss : 0.5074, Accuracy: 66.5332%\n",
      "Validation : Loss : 0.4868, Accuracy: 67.9856%, Time: 50.6153s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 17/25\n",
      "Batch number: 000, Training: Loss: 0.4346, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5095, Accuracy: 0.5625\n",
      "Batch number: 002, Training: Loss: 0.5453, Accuracy: 0.5938\n",
      "Batch number: 003, Training: Loss: 0.4936, Accuracy: 0.6875\n",
      "Batch number: 004, Training: Loss: 0.4230, Accuracy: 0.6250\n",
      "Batch number: 005, Training: Loss: 0.4582, Accuracy: 0.6562\n",
      "Batch number: 006, Training: Loss: 0.4537, Accuracy: 0.7188\n",
      "Batch number: 007, Training: Loss: 0.4429, Accuracy: 0.7812\n",
      "Batch number: 008, Training: Loss: 0.4864, Accuracy: 0.7500\n",
      "Batch number: 009, Training: Loss: 0.5090, Accuracy: 0.5625\n",
      "Batch number: 010, Training: Loss: 0.5127, Accuracy: 0.5312\n",
      "Batch number: 011, Training: Loss: 0.4432, Accuracy: 0.7812\n",
      "Batch number: 012, Training: Loss: 0.5501, Accuracy: 0.7188\n",
      "Batch number: 013, Training: Loss: 0.5012, Accuracy: 0.6250\n",
      "Batch number: 014, Training: Loss: 0.5463, Accuracy: 0.7500\n",
      "Batch number: 015, Training: Loss: 0.4542, Accuracy: 0.6875\n",
      "Batch number: 016, Training: Loss: 0.4900, Accuracy: 0.7188\n",
      "Batch number: 017, Training: Loss: 0.4148, Accuracy: 0.8438\n",
      "Batch number: 018, Training: Loss: 0.4625, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 0.4866, Accuracy: 0.5938\n",
      "Batch number: 020, Training: Loss: 0.5274, Accuracy: 0.7812\n",
      "Batch number: 021, Training: Loss: 0.5023, Accuracy: 0.6250\n",
      "Batch number: 022, Training: Loss: 0.4692, Accuracy: 0.7188\n",
      "Batch number: 023, Training: Loss: 0.4691, Accuracy: 0.7188\n",
      "Batch number: 024, Training: Loss: 0.4689, Accuracy: 0.7188\n",
      "Batch number: 025, Training: Loss: 0.4352, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.4737, Accuracy: 0.6875\n",
      "Batch number: 027, Training: Loss: 0.4546, Accuracy: 0.6875\n",
      "Batch number: 028, Training: Loss: 0.5364, Accuracy: 0.6562\n",
      "Batch number: 029, Training: Loss: 0.4655, Accuracy: 0.6250\n",
      "Batch number: 030, Training: Loss: 0.4764, Accuracy: 0.5625\n",
      "Batch number: 031, Training: Loss: 0.4874, Accuracy: 0.7188\n",
      "Batch number: 032, Training: Loss: 0.5203, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.5281, Accuracy: 0.5938\n",
      "Batch number: 034, Training: Loss: 0.4492, Accuracy: 0.7188\n",
      "Batch number: 035, Training: Loss: 0.4574, Accuracy: 0.7812\n",
      "Batch number: 036, Training: Loss: 0.5143, Accuracy: 0.7812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 037, Training: Loss: 0.5040, Accuracy: 0.6250\n",
      "Batch number: 038, Training: Loss: 0.4070, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 0.4361, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.4859, Accuracy: 0.6250\n",
      "Batch number: 041, Training: Loss: 0.3928, Accuracy: 0.8125\n",
      "Batch number: 042, Training: Loss: 2.0599, Accuracy: 0.6875\n",
      "Batch number: 043, Training: Loss: 0.5174, Accuracy: 0.6562\n",
      "Batch number: 044, Training: Loss: 0.4537, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 0.4945, Accuracy: 0.5938\n",
      "Batch number: 046, Training: Loss: 0.5035, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.4015, Accuracy: 0.8438\n",
      "Batch number: 048, Training: Loss: 0.4410, Accuracy: 0.7500\n",
      "Batch number: 049, Training: Loss: 0.4740, Accuracy: 0.6875\n",
      "Batch number: 050, Training: Loss: 0.5101, Accuracy: 0.6875\n",
      "Batch number: 051, Training: Loss: 0.4223, Accuracy: 0.7500\n",
      "Batch number: 052, Training: Loss: 0.5278, Accuracy: 0.6875\n",
      "Batch number: 053, Training: Loss: 0.5359, Accuracy: 0.6562\n",
      "Batch number: 054, Training: Loss: 0.4913, Accuracy: 0.6250\n",
      "Batch number: 055, Training: Loss: 0.4300, Accuracy: 0.8438\n",
      "Batch number: 056, Training: Loss: 0.5183, Accuracy: 0.5938\n",
      "Batch number: 057, Training: Loss: 0.4834, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 0.4746, Accuracy: 0.6875\n",
      "Batch number: 059, Training: Loss: 0.4480, Accuracy: 0.7812\n",
      "Batch number: 060, Training: Loss: 0.4839, Accuracy: 0.5938\n",
      "Batch number: 061, Training: Loss: 0.5275, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.5628, Accuracy: 0.5000\n",
      "Batch number: 063, Training: Loss: 0.5010, Accuracy: 0.7188\n",
      "Batch number: 064, Training: Loss: 0.4487, Accuracy: 0.7812\n",
      "Batch number: 065, Training: Loss: 0.5183, Accuracy: 0.6562\n",
      "Batch number: 066, Training: Loss: 0.5098, Accuracy: 0.6875\n",
      "Batch number: 067, Training: Loss: 0.5172, Accuracy: 0.5938\n",
      "Batch number: 068, Training: Loss: 0.4494, Accuracy: 0.7812\n",
      "Batch number: 069, Training: Loss: 0.4139, Accuracy: 0.7812\n",
      "Batch number: 070, Training: Loss: 0.5346, Accuracy: 0.5938\n",
      "Batch number: 071, Training: Loss: 0.4579, Accuracy: 0.7500\n",
      "Batch number: 072, Training: Loss: 0.4839, Accuracy: 0.7188\n",
      "Batch number: 073, Training: Loss: 0.5196, Accuracy: 0.7188\n",
      "Batch number: 074, Training: Loss: 0.5234, Accuracy: 0.5000\n",
      "Batch number: 075, Training: Loss: 0.4482, Accuracy: 0.7188\n",
      "Batch number: 076, Training: Loss: 0.5868, Accuracy: 0.5938\n",
      "Batch number: 077, Training: Loss: 0.3965, Accuracy: 0.7812\n",
      "Batch number: 078, Training: Loss: 0.5336, Accuracy: 0.5938\n",
      "Batch number: 079, Training: Loss: 0.5178, Accuracy: 0.6562\n",
      "Batch number: 080, Training: Loss: 0.5120, Accuracy: 0.5312\n",
      "Batch number: 081, Training: Loss: 0.5137, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 0.4847, Accuracy: 0.5625\n",
      "Batch number: 083, Training: Loss: 0.4625, Accuracy: 0.6562\n",
      "Batch number: 084, Training: Loss: 0.4874, Accuracy: 0.6250\n",
      "Batch number: 085, Training: Loss: 0.5043, Accuracy: 0.7188\n",
      "Batch number: 086, Training: Loss: 0.4364, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.4114, Accuracy: 0.7188\n",
      "Batch number: 088, Training: Loss: 0.3984, Accuracy: 0.6875\n",
      "Batch number: 089, Training: Loss: 0.4548, Accuracy: 0.6875\n",
      "Batch number: 090, Training: Loss: 0.4301, Accuracy: 0.7188\n",
      "Batch number: 091, Training: Loss: 0.4766, Accuracy: 0.7812\n",
      "Batch number: 092, Training: Loss: 0.5279, Accuracy: 0.5938\n",
      "Batch number: 093, Training: Loss: 0.4529, Accuracy: 0.8125\n",
      "Batch number: 094, Training: Loss: 0.4338, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 0.5138, Accuracy: 0.5625\n",
      "Batch number: 096, Training: Loss: 0.4354, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.4825, Accuracy: 0.7500\n",
      "Batch number: 098, Training: Loss: 0.4929, Accuracy: 0.6875\n",
      "Batch number: 099, Training: Loss: 0.5034, Accuracy: 0.6250\n",
      "Batch number: 100, Training: Loss: 0.4058, Accuracy: 0.7500\n",
      "Batch number: 101, Training: Loss: 0.4598, Accuracy: 0.6562\n",
      "Batch number: 102, Training: Loss: 0.4632, Accuracy: 0.7500\n",
      "Batch number: 103, Training: Loss: 0.4302, Accuracy: 0.7188\n",
      "Batch number: 104, Training: Loss: 0.5120, Accuracy: 0.6875\n",
      "Batch number: 105, Training: Loss: 0.4959, Accuracy: 0.7812\n",
      "Batch number: 106, Training: Loss: 0.5011, Accuracy: 0.5312\n",
      "Batch number: 107, Training: Loss: 0.5008, Accuracy: 0.7500\n",
      "Batch number: 108, Training: Loss: 0.5149, Accuracy: 0.5625\n",
      "Batch number: 109, Training: Loss: 0.5386, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 0.4737, Accuracy: 0.6875\n",
      "Batch number: 111, Training: Loss: 0.5230, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.5230, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.4737, Accuracy: 0.6875\n",
      "Batch number: 114, Training: Loss: 0.4737, Accuracy: 0.6875\n",
      "Batch number: 115, Training: Loss: 0.4984, Accuracy: 0.6562\n",
      "Batch number: 116, Training: Loss: 0.5609, Accuracy: 0.6250\n",
      "Batch number: 117, Training: Loss: 0.5270, Accuracy: 0.5000\n",
      "Batch number: 118, Training: Loss: 0.5643, Accuracy: 0.5000\n",
      "Batch number: 119, Training: Loss: 0.4111, Accuracy: 0.7188\n",
      "Batch number: 120, Training: Loss: 0.4390, Accuracy: 0.7812\n",
      "Batch number: 121, Training: Loss: 0.4211, Accuracy: 0.6562\n",
      "Batch number: 122, Training: Loss: 0.5226, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.4591, Accuracy: 0.6562\n",
      "Batch number: 124, Training: Loss: 0.4555, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 0.4978, Accuracy: 0.6562\n",
      "Batch number: 126, Training: Loss: 0.4392, Accuracy: 0.6562\n",
      "Batch number: 127, Training: Loss: 0.4695, Accuracy: 0.7188\n",
      "Batch number: 128, Training: Loss: 0.5327, Accuracy: 0.6875\n",
      "Batch number: 129, Training: Loss: 0.4066, Accuracy: 0.7500\n",
      "Batch number: 130, Training: Loss: 0.5193, Accuracy: 0.5000\n",
      "Batch number: 131, Training: Loss: 0.4780, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5094, Accuracy: 0.7188\n",
      "Batch number: 133, Training: Loss: 0.4345, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.4899, Accuracy: 0.7188\n",
      "Batch number: 135, Training: Loss: 0.5015, Accuracy: 0.6250\n",
      "Batch number: 136, Training: Loss: 0.4031, Accuracy: 0.7812\n",
      "Batch number: 137, Training: Loss: 0.5213, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.4504, Accuracy: 0.7188\n",
      "Batch number: 139, Training: Loss: 0.4542, Accuracy: 0.6875\n",
      "Batch number: 140, Training: Loss: 0.4031, Accuracy: 0.7812\n",
      "Batch number: 141, Training: Loss: 0.4542, Accuracy: 0.6875\n",
      "Batch number: 142, Training: Loss: 0.5606, Accuracy: 0.4688\n",
      "Batch number: 143, Training: Loss: 0.4740, Accuracy: 0.6875\n",
      "Batch number: 144, Training: Loss: 0.5051, Accuracy: 0.5938\n",
      "Batch number: 145, Training: Loss: 0.4704, Accuracy: 0.7188\n",
      "Batch number: 146, Training: Loss: 0.4342, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.4471, Accuracy: 0.7500\n",
      "Batch number: 148, Training: Loss: 0.4168, Accuracy: 0.8438\n",
      "Batch number: 149, Training: Loss: 0.4705, Accuracy: 0.7188\n",
      "Batch number: 150, Training: Loss: 0.5138, Accuracy: 0.6875\n",
      "Batch number: 151, Training: Loss: 0.4268, Accuracy: 0.7500\n",
      "Batch number: 152, Training: Loss: 0.4305, Accuracy: 0.7188\n",
      "Batch number: 153, Training: Loss: 0.5331, Accuracy: 0.6875\n",
      "Batch number: 154, Training: Loss: 0.4233, Accuracy: 0.6250\n",
      "Batch number: 155, Training: Loss: 0.4611, Accuracy: 0.7812\n",
      "Validation Batch number: 000, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 001, Validation: Loss: 0.6255, Accuracy: 1.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6956, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 005, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.7307, Accuracy: 0.2500\n",
      "Validation Batch number: 007, Validation: Loss: 0.5393, Accuracy: 0.5000\n",
      "Validation Batch number: 008, Validation: Loss: 0.6956, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 011, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 012, Validation: Loss: 0.3829, Accuracy: 0.5000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 014, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 015, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 016, Validation: Loss: 0.6605, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 017, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 018, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 019, Validation: Loss: 0.1564, Accuracy: 1.0000\n",
      "Validation Batch number: 020, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 021, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 022, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 023, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 025, Validation: Loss: 0.1914, Accuracy: 0.7500\n",
      "Validation Batch number: 026, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 029, Validation: Loss: 0.6255, Accuracy: 1.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 031, Validation: Loss: 0.3829, Accuracy: 0.5000\n",
      "Validation Batch number: 032, Validation: Loss: 0.5743, Accuracy: 0.2500\n",
      "Validation Batch number: 033, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 034, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 035, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 036, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6956, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3829, Accuracy: 0.5000\n",
      "Validation Batch number: 040, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 042, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.7307, Accuracy: 0.2500\n",
      "Validation Batch number: 044, Validation: Loss: 0.7307, Accuracy: 0.2500\n",
      "Validation Batch number: 045, Validation: Loss: 0.7307, Accuracy: 0.2500\n",
      "Validation Batch number: 046, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 049, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 050, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 051, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 053, Validation: Loss: 0.7307, Accuracy: 0.2500\n",
      "Validation Batch number: 054, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 055, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 056, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 059, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3829, Accuracy: 0.5000\n",
      "Validation Batch number: 062, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 063, Validation: Loss: 0.7657, Accuracy: 0.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 065, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 068, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 070, Validation: Loss: 0.6956, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6956, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 074, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 075, Validation: Loss: 0.3078, Accuracy: 1.0000\n",
      "Validation Batch number: 076, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 077, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 078, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 079, Validation: Loss: 0.3829, Accuracy: 0.5000\n",
      "Validation Batch number: 080, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 082, Validation: Loss: 0.1914, Accuracy: 0.7500\n",
      "Validation Batch number: 083, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 085, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1564, Accuracy: 1.0000\n",
      "Validation Batch number: 088, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 089, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 090, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3829, Accuracy: 0.5000\n",
      "Validation Batch number: 092, Validation: Loss: 0.7307, Accuracy: 0.2500\n",
      "Validation Batch number: 093, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6956, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 099, Validation: Loss: 0.5743, Accuracy: 0.2500\n",
      "Validation Batch number: 100, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 101, Validation: Loss: 0.4691, Accuracy: 1.0000\n",
      "Validation Batch number: 102, Validation: Loss: 0.3829, Accuracy: 0.5000\n",
      "Validation Batch number: 103, Validation: Loss: 0.1914, Accuracy: 0.7500\n",
      "Validation Batch number: 104, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 105, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 107, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 108, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 110, Validation: Loss: 0.5743, Accuracy: 0.2500\n",
      "Validation Batch number: 111, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 112, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 113, Validation: Loss: 0.7307, Accuracy: 0.2500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 115, Validation: Loss: 0.5743, Accuracy: 0.2500\n",
      "Validation Batch number: 116, Validation: Loss: 0.3829, Accuracy: 0.5000\n",
      "Validation Batch number: 117, Validation: Loss: 0.1564, Accuracy: 1.0000\n",
      "Validation Batch number: 118, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 120, Validation: Loss: 0.1914, Accuracy: 0.7500\n",
      "Validation Batch number: 121, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 122, Validation: Loss: 0.3478, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 125, Validation: Loss: 0.3127, Accuracy: 1.0000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 127, Validation: Loss: 0.3829, Accuracy: 0.5000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 129, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 130, Validation: Loss: 0.7307, Accuracy: 0.2500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5392, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 132, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 133, Validation: Loss: 0.1564, Accuracy: 1.0000\n",
      "Validation Batch number: 134, Validation: Loss: 0.6605, Accuracy: 0.7500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5042, Accuracy: 0.7500\n",
      "Validation Batch number: 136, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Validation Batch number: 137, Validation: Loss: 0.5743, Accuracy: 0.2500\n",
      "Validation Batch number: 138, Validation: Loss: 0.5392, Accuracy: 0.5000\n",
      "Epoch : 016, Training: Loss : 0.4887, Accuracy: 68.3747%\n",
      "Validation : Loss : 0.4828, Accuracy: 68.5252%, Time: 50.5540s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 18/25\n",
      "Batch number: 000, Training: Loss: 0.3869, Accuracy: 0.7500\n",
      "Batch number: 001, Training: Loss: 0.5068, Accuracy: 0.5938\n",
      "Batch number: 002, Training: Loss: 0.5219, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.4932, Accuracy: 0.6875\n",
      "Batch number: 004, Training: Loss: 0.4008, Accuracy: 0.6562\n",
      "Batch number: 005, Training: Loss: 0.4831, Accuracy: 0.6250\n",
      "Batch number: 006, Training: Loss: 0.4978, Accuracy: 0.6562\n",
      "Batch number: 007, Training: Loss: 0.4887, Accuracy: 0.7188\n",
      "Batch number: 008, Training: Loss: 0.4602, Accuracy: 0.7812\n",
      "Batch number: 009, Training: Loss: 0.4635, Accuracy: 0.6250\n",
      "Batch number: 010, Training: Loss: 0.5643, Accuracy: 0.4688\n",
      "Batch number: 011, Training: Loss: 0.4649, Accuracy: 0.7500\n",
      "Batch number: 012, Training: Loss: 0.4757, Accuracy: 0.8125\n",
      "Batch number: 013, Training: Loss: 0.5023, Accuracy: 0.6250\n",
      "Batch number: 014, Training: Loss: 0.5668, Accuracy: 0.7188\n",
      "Batch number: 015, Training: Loss: 0.5024, Accuracy: 0.6250\n",
      "Batch number: 016, Training: Loss: 0.4645, Accuracy: 0.7500\n",
      "Batch number: 017, Training: Loss: 0.3872, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 0.4643, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 0.4889, Accuracy: 0.5938\n",
      "Batch number: 020, Training: Loss: 0.4728, Accuracy: 0.8125\n",
      "Batch number: 021, Training: Loss: 0.4789, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.4439, Accuracy: 0.7500\n",
      "Batch number: 023, Training: Loss: 0.4437, Accuracy: 0.7500\n",
      "Batch number: 024, Training: Loss: 0.4927, Accuracy: 0.6875\n",
      "Batch number: 025, Training: Loss: 0.4852, Accuracy: 0.6250\n",
      "Batch number: 026, Training: Loss: 0.5479, Accuracy: 0.5938\n",
      "Batch number: 027, Training: Loss: 0.5291, Accuracy: 0.5938\n",
      "Batch number: 028, Training: Loss: 0.4867, Accuracy: 0.7188\n",
      "Batch number: 029, Training: Loss: 0.5162, Accuracy: 0.5625\n",
      "Batch number: 030, Training: Loss: 0.4536, Accuracy: 0.5938\n",
      "Batch number: 031, Training: Loss: 0.5362, Accuracy: 0.6562\n",
      "Batch number: 032, Training: Loss: 0.5194, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.5039, Accuracy: 0.6250\n",
      "Batch number: 034, Training: Loss: 0.4492, Accuracy: 0.7188\n",
      "Batch number: 035, Training: Loss: 0.4572, Accuracy: 0.7812\n",
      "Batch number: 036, Training: Loss: 0.5142, Accuracy: 0.7812\n",
      "Batch number: 037, Training: Loss: 0.5039, Accuracy: 0.6250\n",
      "Batch number: 038, Training: Loss: 0.4566, Accuracy: 0.7812\n",
      "Batch number: 039, Training: Loss: 0.4359, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.4113, Accuracy: 0.7188\n",
      "Batch number: 041, Training: Loss: 0.4925, Accuracy: 0.6875\n",
      "Batch number: 042, Training: Loss: 0.5178, Accuracy: 0.7500\n",
      "Batch number: 043, Training: Loss: 0.4924, Accuracy: 0.6875\n",
      "Batch number: 044, Training: Loss: 0.4796, Accuracy: 0.7500\n",
      "Batch number: 045, Training: Loss: 0.4935, Accuracy: 0.5938\n",
      "Batch number: 046, Training: Loss: 0.5041, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.4535, Accuracy: 0.7812\n",
      "Batch number: 048, Training: Loss: 0.4162, Accuracy: 0.7812\n",
      "Batch number: 049, Training: Loss: 0.4993, Accuracy: 0.6562\n",
      "Batch number: 050, Training: Loss: 0.4593, Accuracy: 0.7500\n",
      "Batch number: 051, Training: Loss: 0.4484, Accuracy: 0.7188\n",
      "Batch number: 052, Training: Loss: 0.4511, Accuracy: 0.7812\n",
      "Batch number: 053, Training: Loss: 0.5359, Accuracy: 0.6562\n",
      "Batch number: 054, Training: Loss: 0.3942, Accuracy: 0.7500\n",
      "Batch number: 055, Training: Loss: 0.3895, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.4650, Accuracy: 0.6562\n",
      "Batch number: 057, Training: Loss: 0.4568, Accuracy: 0.6875\n",
      "Batch number: 058, Training: Loss: 0.4482, Accuracy: 0.7188\n",
      "Batch number: 059, Training: Loss: 0.5272, Accuracy: 0.6875\n",
      "Batch number: 060, Training: Loss: 0.4312, Accuracy: 0.6562\n",
      "Batch number: 061, Training: Loss: 0.5012, Accuracy: 0.6562\n",
      "Batch number: 062, Training: Loss: 0.3784, Accuracy: 0.7188\n",
      "Batch number: 063, Training: Loss: 0.4210, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 0.4739, Accuracy: 0.7500\n",
      "Batch number: 065, Training: Loss: 0.4390, Accuracy: 0.7500\n",
      "Batch number: 066, Training: Loss: 0.4296, Accuracy: 0.7812\n",
      "Batch number: 067, Training: Loss: 0.4403, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.4196, Accuracy: 0.8125\n",
      "Batch number: 069, Training: Loss: 0.4654, Accuracy: 0.7188\n",
      "Batch number: 070, Training: Loss: 0.5388, Accuracy: 0.5938\n",
      "Batch number: 071, Training: Loss: 0.4555, Accuracy: 0.7500\n",
      "Batch number: 072, Training: Loss: 0.3473, Accuracy: 0.8750\n",
      "Batch number: 073, Training: Loss: 0.4350, Accuracy: 0.8125\n",
      "Batch number: 074, Training: Loss: 0.5100, Accuracy: 0.5312\n",
      "Batch number: 075, Training: Loss: 0.4211, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 0.4314, Accuracy: 0.7188\n",
      "Batch number: 077, Training: Loss: 0.4756, Accuracy: 0.6875\n",
      "Batch number: 078, Training: Loss: 0.5407, Accuracy: 0.5938\n",
      "Batch number: 079, Training: Loss: 0.4925, Accuracy: 0.6875\n",
      "Batch number: 080, Training: Loss: 0.4999, Accuracy: 0.5625\n",
      "Batch number: 081, Training: Loss: 0.4246, Accuracy: 0.8438\n",
      "Batch number: 082, Training: Loss: 0.5259, Accuracy: 0.5312\n",
      "Batch number: 083, Training: Loss: 0.3600, Accuracy: 0.7812\n",
      "Batch number: 084, Training: Loss: 0.4139, Accuracy: 0.7188\n",
      "Batch number: 085, Training: Loss: 0.4194, Accuracy: 0.8125\n",
      "Batch number: 086, Training: Loss: 0.4940, Accuracy: 0.6250\n",
      "Batch number: 087, Training: Loss: 0.4402, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.4585, Accuracy: 0.6250\n",
      "Batch number: 089, Training: Loss: 0.4837, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.4306, Accuracy: 0.7188\n",
      "Batch number: 091, Training: Loss: 0.4401, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 0.4305, Accuracy: 0.7188\n",
      "Batch number: 093, Training: Loss: 0.3886, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 0.4231, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 0.4473, Accuracy: 0.6562\n",
      "Batch number: 096, Training: Loss: 0.3866, Accuracy: 0.7500\n",
      "Batch number: 097, Training: Loss: 0.5013, Accuracy: 0.7188\n",
      "Batch number: 098, Training: Loss: 0.4135, Accuracy: 0.7812\n",
      "Batch number: 099, Training: Loss: 0.4567, Accuracy: 0.6875\n",
      "Batch number: 100, Training: Loss: 0.4305, Accuracy: 0.7188\n",
      "Batch number: 101, Training: Loss: 0.4392, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.4572, Accuracy: 0.7500\n",
      "Batch number: 103, Training: Loss: 0.4042, Accuracy: 0.7500\n",
      "Batch number: 104, Training: Loss: 0.4571, Accuracy: 0.7500\n",
      "Batch number: 105, Training: Loss: 0.4571, Accuracy: 0.8125\n",
      "Batch number: 106, Training: Loss: 0.4926, Accuracy: 0.5625\n",
      "Batch number: 107, Training: Loss: 0.4390, Accuracy: 0.8125\n",
      "Batch number: 108, Training: Loss: 0.5016, Accuracy: 0.5938\n",
      "Batch number: 109, Training: Loss: 0.4736, Accuracy: 0.8125\n",
      "Batch number: 110, Training: Loss: 0.4482, Accuracy: 0.7188\n",
      "Batch number: 111, Training: Loss: 0.4747, Accuracy: 0.6875\n",
      "Batch number: 112, Training: Loss: 0.3949, Accuracy: 0.7812\n",
      "Batch number: 113, Training: Loss: 0.5015, Accuracy: 0.6562\n",
      "Batch number: 114, Training: Loss: 0.5016, Accuracy: 0.6562\n",
      "Batch number: 115, Training: Loss: 0.5016, Accuracy: 0.6562\n",
      "Batch number: 116, Training: Loss: 43.4951, Accuracy: 0.6875\n",
      "Batch number: 117, Training: Loss: 0.5487, Accuracy: 0.5000\n",
      "Batch number: 118, Training: Loss: 0.5295, Accuracy: 0.5625\n",
      "Batch number: 119, Training: Loss: 0.4399, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5361, Accuracy: 0.6562\n",
      "Batch number: 121, Training: Loss: 0.5887, Accuracy: 0.4688\n",
      "Batch number: 122, Training: Loss: 0.5768, Accuracy: 0.5938\n",
      "Batch number: 123, Training: Loss: 0.5687, Accuracy: 0.5312\n",
      "Batch number: 124, Training: Loss: 0.5977, Accuracy: 0.6250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 125, Training: Loss: 0.5766, Accuracy: 0.5625\n",
      "Batch number: 126, Training: Loss: 0.5972, Accuracy: 0.4688\n",
      "Batch number: 127, Training: Loss: 0.5765, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.6047, Accuracy: 0.5938\n",
      "Batch number: 129, Training: Loss: 0.5788, Accuracy: 0.5312\n",
      "Batch number: 130, Training: Loss: 0.6241, Accuracy: 0.3750\n",
      "Batch number: 131, Training: Loss: 0.6485, Accuracy: 0.4375\n",
      "Batch number: 132, Training: Loss: 0.6043, Accuracy: 0.5938\n",
      "Batch number: 133, Training: Loss: 0.6237, Accuracy: 0.4375\n",
      "Batch number: 134, Training: Loss: 0.6077, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5925, Accuracy: 0.5000\n",
      "Batch number: 136, Training: Loss: 0.6356, Accuracy: 0.4688\n",
      "Batch number: 137, Training: Loss: 0.6329, Accuracy: 0.4688\n",
      "Batch number: 138, Training: Loss: 0.5865, Accuracy: 0.5312\n",
      "Batch number: 139, Training: Loss: 0.5861, Accuracy: 0.5000\n",
      "Batch number: 140, Training: Loss: 0.6497, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.6047, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.7574, Accuracy: 0.7188\n",
      "Batch number: 143, Training: Loss: 0.6032, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.6584, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5545, Accuracy: 0.6250\n",
      "Batch number: 146, Training: Loss: 0.5929, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5848, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.6336, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.6425, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.6292, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5984, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.6298, Accuracy: 0.6250\n",
      "Batch number: 153, Training: Loss: 0.6188, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5620, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5841, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.8247, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3504, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.4327, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.4327, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.2885, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.8247, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.4327, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.2885, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.6185, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.4123, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3504, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.4327, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3504, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.2885, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.2885, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.6185, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.4123, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.4946, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 088, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.4123, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.6185, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.6185, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3504, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.2885, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.4947, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.5566, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.4946, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.3504, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7627, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.7008, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.5769, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.6389, Accuracy: 0.7500\n",
      "Epoch : 017, Training: Loss : 0.7735, Accuracy: 67.1137%\n",
      "Validation : Loss : 0.6007, Accuracy: 61.5108%, Time: 50.5303s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 19/25\n",
      "Batch number: 000, Training: Loss: 0.5642, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5817, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5976, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.6415, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.5188, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.5795, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.6496, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.6168, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.6614, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.5848, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5739, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.6524, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.6345, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.6005, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.6796, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.5798, Accuracy: 0.6875\n",
      "Batch number: 016, Training: Loss: 2.7106, Accuracy: 0.6562\n",
      "Batch number: 017, Training: Loss: 0.6028, Accuracy: 0.5625\n",
      "Batch number: 018, Training: Loss: 0.5488, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.5990, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.6524, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.5871, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.6255, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.6087, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5752, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.6495, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.6314, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5704, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.6359, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5829, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.6121, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.6194, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.6522, Accuracy: 0.4688\n",
      "Batch number: 033, Training: Loss: 0.6205, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.6313, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.6137, Accuracy: 0.5312\n",
      "Batch number: 036, Training: Loss: 0.6624, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.5536, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.6288, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.5945, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.5778, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.6246, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 0.6633, Accuracy: 0.4688\n",
      "Batch number: 043, Training: Loss: 0.6416, Accuracy: 0.5938\n",
      "Batch number: 044, Training: Loss: 0.6602, Accuracy: 0.5312\n",
      "Batch number: 045, Training: Loss: 0.5877, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 0.6244, Accuracy: 0.5312\n",
      "Batch number: 047, Training: Loss: 0.6239, Accuracy: 0.5312\n",
      "Batch number: 048, Training: Loss: 0.5884, Accuracy: 0.5938\n",
      "Batch number: 049, Training: Loss: 0.5798, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 0.6138, Accuracy: 0.5625\n",
      "Batch number: 051, Training: Loss: 0.6157, Accuracy: 0.6250\n",
      "Batch number: 052, Training: Loss: 0.6569, Accuracy: 0.5312\n",
      "Batch number: 053, Training: Loss: 0.6490, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 0.5834, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.5715, Accuracy: 0.5000\n",
      "Batch number: 056, Training: Loss: 0.6211, Accuracy: 0.6875\n",
      "Batch number: 057, Training: Loss: 0.5914, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 0.5612, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.6364, Accuracy: 0.5312\n",
      "Batch number: 060, Training: Loss: 0.6097, Accuracy: 0.7500\n",
      "Batch number: 061, Training: Loss: 0.5797, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.6468, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.6365, Accuracy: 0.5312\n",
      "Batch number: 064, Training: Loss: 0.6182, Accuracy: 0.5312\n",
      "Batch number: 065, Training: Loss: 0.6418, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 0.6116, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.6215, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.5632, Accuracy: 0.5312\n",
      "Batch number: 069, Training: Loss: 0.6418, Accuracy: 0.5938\n",
      "Batch number: 070, Training: Loss: 0.5913, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.5933, Accuracy: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 072, Training: Loss: 0.5747, Accuracy: 0.5625\n",
      "Batch number: 073, Training: Loss: 0.6619, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 0.6143, Accuracy: 0.8125\n",
      "Batch number: 075, Training: Loss: 0.5611, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 0.5914, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.5981, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.6280, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 0.6050, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.5626, Accuracy: 0.7812\n",
      "Batch number: 081, Training: Loss: 0.6342, Accuracy: 0.4688\n",
      "Batch number: 082, Training: Loss: 0.5975, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 0.5826, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.5821, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.6218, Accuracy: 0.5312\n",
      "Batch number: 086, Training: Loss: 0.5811, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.5807, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.5100, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.6236, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.5709, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.6701, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.6051, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.6028, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.6273, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.5680, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.5265, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.6617, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.5905, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.6048, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.5877, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5778, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.5663, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.6390, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.6175, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.6471, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.5489, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.6622, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.5515, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.6549, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.5629, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5627, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.5975, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.6151, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5976, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.6152, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.6721, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.5734, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.5901, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.5098, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5795, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.5277, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5448, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5624, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.6156, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.6326, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.5709, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.5890, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.6420, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5536, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5178, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.5361, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5899, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5096, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5632, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5186, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5273, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5271, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5269, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5360, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.4907, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5180, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5842, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.4717, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5643, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5511, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5465, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5871, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.5471, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.4955, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5936, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5057, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5546, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5424, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5311, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5217, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7864, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3039, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1966, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1520, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.1520, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7864, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.3039, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.4559, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3932, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3039, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6525, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 046, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3932, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3039, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6079, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3039, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1520, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3932, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.3932, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.4559, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3932, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3932, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.4559, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3039, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3039, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3932, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.6079, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4559, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3039, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5898, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.3039, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3932, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4559, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.6971, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1966, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7417, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5452, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6525, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6079, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5005, Accuracy: 0.7500\n",
      "Epoch : 018, Training: Loss : 0.6058, Accuracy: 62.1497%\n",
      "Validation : Loss : 0.5257, Accuracy: 61.5108%, Time: 50.5088s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 20/25\n",
      "Batch number: 000, Training: Loss: 0.5117, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5500, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5225, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5659, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.4596, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4931, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.5608, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5704, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5943, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.5472, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5426, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.4921, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.6030, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.5759, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.6063, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.5370, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5885, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.5127, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.4905, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.5309, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.5371, Accuracy: 0.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 021, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.5437, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.4620, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5228, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.5366, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.5823, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.5235, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5364, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.5337, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.5022, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.5275, Accuracy: 0.4688\n",
      "Batch number: 033, Training: Loss: 0.5187, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.5200, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5235, Accuracy: 0.5312\n",
      "Batch number: 036, Training: Loss: 0.5683, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.4766, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.5214, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.5409, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.4332, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.4980, Accuracy: 0.6875\n",
      "Batch number: 042, Training: Loss: 0.4964, Accuracy: 0.8125\n",
      "Batch number: 043, Training: Loss: 0.5193, Accuracy: 0.6562\n",
      "Batch number: 044, Training: Loss: 0.5615, Accuracy: 0.6562\n",
      "Batch number: 045, Training: Loss: 0.5458, Accuracy: 0.5000\n",
      "Batch number: 046, Training: Loss: 0.5159, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.4929, Accuracy: 0.7500\n",
      "Batch number: 048, Training: Loss: 0.4957, Accuracy: 0.6875\n",
      "Batch number: 049, Training: Loss: 0.5430, Accuracy: 0.5938\n",
      "Batch number: 050, Training: Loss: 0.5386, Accuracy: 0.6562\n",
      "Batch number: 051, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 052, Training: Loss: 0.5584, Accuracy: 0.6562\n",
      "Batch number: 053, Training: Loss: 0.5379, Accuracy: 0.6562\n",
      "Batch number: 054, Training: Loss: 0.4573, Accuracy: 0.6562\n",
      "Batch number: 055, Training: Loss: 0.4608, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 0.5283, Accuracy: 0.5625\n",
      "Batch number: 057, Training: Loss: 0.4778, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 0.5213, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.5331, Accuracy: 0.6875\n",
      "Batch number: 060, Training: Loss: 0.5142, Accuracy: 0.5312\n",
      "Batch number: 061, Training: Loss: 0.4977, Accuracy: 0.6562\n",
      "Batch number: 062, Training: Loss: 0.5144, Accuracy: 0.5312\n",
      "Batch number: 063, Training: Loss: 0.5330, Accuracy: 0.6875\n",
      "Batch number: 064, Training: Loss: 0.5568, Accuracy: 0.6562\n",
      "Batch number: 065, Training: Loss: 0.5174, Accuracy: 0.6562\n",
      "Batch number: 066, Training: Loss: 0.5608, Accuracy: 0.6250\n",
      "Batch number: 067, Training: Loss: 0.5058, Accuracy: 0.5938\n",
      "Batch number: 068, Training: Loss: 0.4857, Accuracy: 0.7500\n",
      "Batch number: 069, Training: Loss: 0.4937, Accuracy: 0.6875\n",
      "Batch number: 070, Training: Loss: 0.5730, Accuracy: 0.5312\n",
      "Batch number: 071, Training: Loss: 0.5608, Accuracy: 0.6250\n",
      "Batch number: 072, Training: Loss: 0.5608, Accuracy: 0.6250\n",
      "Batch number: 073, Training: Loss: 0.5766, Accuracy: 0.6562\n",
      "Batch number: 074, Training: Loss: 0.4742, Accuracy: 0.5312\n",
      "Batch number: 075, Training: Loss: 0.5213, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 0.5487, Accuracy: 0.5625\n",
      "Batch number: 077, Training: Loss: 0.5447, Accuracy: 0.5938\n",
      "Batch number: 078, Training: Loss: 0.5949, Accuracy: 0.5000\n",
      "Batch number: 079, Training: Loss: 0.5410, Accuracy: 0.6250\n",
      "Batch number: 080, Training: Loss: 0.5363, Accuracy: 0.4688\n",
      "Batch number: 081, Training: Loss: 0.5297, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 0.5567, Accuracy: 0.4375\n",
      "Batch number: 083, Training: Loss: 0.5251, Accuracy: 0.5625\n",
      "Batch number: 084, Training: Loss: 0.5016, Accuracy: 0.5938\n",
      "Batch number: 085, Training: Loss: 0.5376, Accuracy: 0.6875\n",
      "Batch number: 086, Training: Loss: 0.5230, Accuracy: 0.5625\n",
      "Batch number: 087, Training: Loss: 0.4778, Accuracy: 0.6250\n",
      "Batch number: 088, Training: Loss: 0.5014, Accuracy: 0.5312\n",
      "Batch number: 089, Training: Loss: 0.5863, Accuracy: 0.5000\n",
      "Batch number: 090, Training: Loss: 0.5637, Accuracy: 0.5312\n",
      "Batch number: 091, Training: Loss: 0.6068, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.4766, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.5438, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.5223, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.5202, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.4756, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.5655, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.4791, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.5188, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.4345, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.4755, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.5224, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.5188, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.5012, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.5677, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.4729, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.5659, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.4964, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.5241, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.7681, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.5846, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.5415, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5849, Accuracy: 0.5312\n",
      "Batch number: 115, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.6065, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.5425, Accuracy: 0.4375\n",
      "Batch number: 118, Training: Loss: 0.6070, Accuracy: 0.4062\n",
      "Batch number: 119, Training: Loss: 0.5629, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.6065, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.6034, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.6261, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.6020, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.6099, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.6654, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.6579, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.6250, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.6105, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.6378, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.6194, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.6746, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5942, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5699, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.6099, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.6117, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5610, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.6354, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.6349, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.6274, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.6160, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.6260, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.6176, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.6506, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.6322, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5376, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5610, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5395, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.5777, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5405, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5679, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5470, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5028, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5943, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.4282, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5418, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.8599, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.7051, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.4901, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 005, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.7051, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.2150, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1376, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.8599, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.4300, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.7051, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.4300, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.5502, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.7051, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.7051, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.2150, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.7051, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1376, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.4300, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.2150, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.4300, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.4300, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.4300, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.7051, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.4127, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.4300, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.7051, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.4127, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.4322, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4127, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3525, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 118, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.6450, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.2751, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3525, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.7051, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.4300, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4127, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6277, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.2150, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7825, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5675, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.5502, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.4901, Accuracy: 0.7500\n",
      "Epoch : 019, Training: Loss : 0.5454, Accuracy: 61.7294%\n",
      "Validation : Loss : 0.5151, Accuracy: 61.5108%, Time: 50.4956s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 21/25\n",
      "Batch number: 000, Training: Loss: 0.4923, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5268, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5281, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5195, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.4304, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4569, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.4737, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.4896, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5140, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.5026, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.4959, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.5387, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5395, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.4614, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.5779, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.5173, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5130, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.5107, Accuracy: 0.5625\n",
      "Batch number: 018, Training: Loss: 0.4699, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 1.5868, Accuracy: 0.6562\n",
      "Batch number: 020, Training: Loss: 0.5571, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.5178, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 1.7477, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.4397, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5217, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.5176, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.4987, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5194, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.5204, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5416, Accuracy: 0.5000\n",
      "Batch number: 030, Training: Loss: 0.4992, Accuracy: 0.5000\n",
      "Batch number: 031, Training: Loss: 0.5193, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 0.5835, Accuracy: 0.6875\n",
      "Batch number: 033, Training: Loss: 0.5204, Accuracy: 0.5938\n",
      "Batch number: 034, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5617, Accuracy: 0.6562\n",
      "Batch number: 036, Training: Loss: 0.5808, Accuracy: 0.7188\n",
      "Batch number: 037, Training: Loss: 0.6323, Accuracy: 0.4375\n",
      "Batch number: 038, Training: Loss: 0.5386, Accuracy: 0.6875\n",
      "Batch number: 039, Training: Loss: 0.5452, Accuracy: 0.5312\n",
      "Batch number: 040, Training: Loss: 0.5454, Accuracy: 0.5312\n",
      "Batch number: 041, Training: Loss: 0.5634, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 0.5799, Accuracy: 0.6875\n",
      "Batch number: 043, Training: Loss: 0.5859, Accuracy: 0.5625\n",
      "Batch number: 044, Training: Loss: 0.6277, Accuracy: 0.5625\n",
      "Batch number: 045, Training: Loss: 0.5925, Accuracy: 0.4375\n",
      "Batch number: 046, Training: Loss: 0.5378, Accuracy: 0.6875\n",
      "Batch number: 047, Training: Loss: 0.5379, Accuracy: 0.6875\n",
      "Batch number: 048, Training: Loss: 0.5185, Accuracy: 0.6562\n",
      "Batch number: 049, Training: Loss: 0.5426, Accuracy: 0.5938\n",
      "Batch number: 050, Training: Loss: 0.6293, Accuracy: 0.5312\n",
      "Batch number: 051, Training: Loss: 0.5877, Accuracy: 0.5312\n",
      "Batch number: 052, Training: Loss: 0.6052, Accuracy: 0.5938\n",
      "Batch number: 053, Training: Loss: 8.4434, Accuracy: 0.5312\n",
      "Batch number: 054, Training: Loss: 0.5681, Accuracy: 0.5000\n",
      "Batch number: 055, Training: Loss: 0.4695, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 0.5901, Accuracy: 0.4688\n",
      "Batch number: 057, Training: Loss: 0.5885, Accuracy: 0.5000\n",
      "Batch number: 058, Training: Loss: 0.5200, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.5833, Accuracy: 0.6250\n",
      "Batch number: 060, Training: Loss: 0.5462, Accuracy: 0.4688\n",
      "Batch number: 061, Training: Loss: 0.5641, Accuracy: 0.5625\n",
      "Batch number: 062, Training: Loss: 0.5665, Accuracy: 0.4375\n",
      "Batch number: 063, Training: Loss: 0.5625, Accuracy: 0.6562\n",
      "Batch number: 064, Training: Loss: 0.6063, Accuracy: 0.5938\n",
      "Batch number: 065, Training: Loss: 0.5415, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 0.5418, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.5194, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.4780, Accuracy: 0.5312\n",
      "Batch number: 069, Training: Loss: 0.5417, Accuracy: 0.5938\n",
      "Batch number: 070, Training: Loss: 0.6053, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.5423, Accuracy: 0.5625\n",
      "Batch number: 072, Training: Loss: 0.5423, Accuracy: 0.5625\n",
      "Batch number: 073, Training: Loss: 0.5647, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 0.5387, Accuracy: 0.8125\n",
      "Batch number: 075, Training: Loss: 0.4985, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 0.5621, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.5839, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.5617, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 0.5845, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.4937, Accuracy: 0.7812\n",
      "Batch number: 081, Training: Loss: 0.5683, Accuracy: 0.4688\n",
      "Batch number: 082, Training: Loss: 0.4924, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 0.5171, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.4959, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.5467, Accuracy: 0.5312\n",
      "Batch number: 086, Training: Loss: 0.5368, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.5159, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.5109, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.5587, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.5178, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.5532, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.5341, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.4906, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.5113, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.4743, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.5704, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.5843, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.5784, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.5178, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.4946, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.5669, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.5669, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.5548, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.5890, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.5921, Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 108, Training: Loss: 0.5333, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.5584, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.5204, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5614, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.5615, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.5822, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5203, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.5616, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.5448, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.5316, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.4937, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.4956, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5039, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.5117, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5430, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5157, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.5252, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 1.2629, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.7944, Accuracy: 0.6875\n",
      "Batch number: 127, Training: Loss: 0.5433, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.5685, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5386, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.4679, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.5181, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5890, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.4337, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5251, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5797, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.4793, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.6026, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.4998, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5181, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.4997, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5593, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5528, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.5614, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5976, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.4823, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5966, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5437, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.5498, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5641, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5470, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5409, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5177, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5297, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5467, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5700, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7409, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.4856, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5557, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5557, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7409, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.4856, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5557, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3705, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5557, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3705, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.4856, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6475, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.3471, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 076, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1619, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5557, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3705, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3705, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3705, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.4856, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5557, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5557, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3705, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.6475, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4856, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5557, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.3238, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3705, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4856, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1852, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7176, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5324, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6709, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6475, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5090, Accuracy: 0.7500\n",
      "Epoch : 020, Training: Loss : 0.6083, Accuracy: 61.6093%\n",
      "Validation : Loss : 0.5239, Accuracy: 61.5108%, Time: 50.4704s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 22/25\n",
      "Batch number: 000, Training: Loss: 0.4946, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5351, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5207, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5439, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.4250, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4945, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.5208, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.4869, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.6107, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.6325, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5284, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.5475, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5576, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.5176, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.6408, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.5579, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5470, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.5497, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.5530, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.5939, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.5544, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.5638, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.4819, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5432, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.5569, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.5822, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5515, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 2.4449, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5145, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.5732, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.5447, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.5920, Accuracy: 0.4688\n",
      "Batch number: 033, Training: Loss: 0.5391, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.5617, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5048, Accuracy: 0.5312\n",
      "Batch number: 036, Training: Loss: 0.6136, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.5604, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.5242, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.4965, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.4756, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.5633, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 0.6091, Accuracy: 0.4688\n",
      "Batch number: 043, Training: Loss: 0.5846, Accuracy: 0.5938\n",
      "Batch number: 044, Training: Loss: 0.5643, Accuracy: 0.5312\n",
      "Batch number: 045, Training: Loss: 0.4329, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 0.5421, Accuracy: 0.5312\n",
      "Batch number: 047, Training: Loss: 0.5200, Accuracy: 0.5312\n",
      "Batch number: 048, Training: Loss: 0.5197, Accuracy: 0.6562\n",
      "Batch number: 049, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 0.5846, Accuracy: 0.5938\n",
      "Batch number: 051, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 052, Training: Loss: 0.5392, Accuracy: 0.6875\n",
      "Batch number: 053, Training: Loss: 0.5844, Accuracy: 0.5938\n",
      "Batch number: 054, Training: Loss: 0.5004, Accuracy: 0.5938\n",
      "Batch number: 055, Training: Loss: 0.4470, Accuracy: 0.8438\n",
      "Batch number: 056, Training: Loss: 0.5460, Accuracy: 0.5312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 057, Training: Loss: 0.5446, Accuracy: 0.5625\n",
      "Batch number: 058, Training: Loss: 0.4751, Accuracy: 0.6875\n",
      "Batch number: 059, Training: Loss: 0.5369, Accuracy: 0.6875\n",
      "Batch number: 060, Training: Loss: 0.5517, Accuracy: 0.4688\n",
      "Batch number: 061, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 062, Training: Loss: 0.6429, Accuracy: 0.3438\n",
      "Batch number: 063, Training: Loss: 0.5142, Accuracy: 0.7188\n",
      "Batch number: 064, Training: Loss: 0.5823, Accuracy: 0.6250\n",
      "Batch number: 065, Training: Loss: 0.5636, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 0.4938, Accuracy: 0.7188\n",
      "Batch number: 067, Training: Loss: 0.4787, Accuracy: 0.6250\n",
      "Batch number: 068, Training: Loss: 0.4694, Accuracy: 0.7812\n",
      "Batch number: 069, Training: Loss: 0.5409, Accuracy: 0.6250\n",
      "Batch number: 070, Training: Loss: 0.6129, Accuracy: 0.4688\n",
      "Batch number: 071, Training: Loss: 0.5616, Accuracy: 0.6250\n",
      "Batch number: 072, Training: Loss: 0.5616, Accuracy: 0.6250\n",
      "Batch number: 073, Training: Loss: 0.5351, Accuracy: 0.7188\n",
      "Batch number: 074, Training: Loss: 0.4643, Accuracy: 0.5312\n",
      "Batch number: 075, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 076, Training: Loss: 0.5901, Accuracy: 0.5000\n",
      "Batch number: 077, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 078, Training: Loss: 0.5896, Accuracy: 0.5000\n",
      "Batch number: 079, Training: Loss: 0.5635, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.5052, Accuracy: 0.5000\n",
      "Batch number: 081, Training: Loss: 0.5136, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 0.4591, Accuracy: 0.5625\n",
      "Batch number: 083, Training: Loss: 0.5663, Accuracy: 0.5000\n",
      "Batch number: 084, Training: Loss: 0.5436, Accuracy: 0.5312\n",
      "Batch number: 085, Training: Loss: 0.5183, Accuracy: 0.7188\n",
      "Batch number: 086, Training: Loss: 0.5206, Accuracy: 0.5625\n",
      "Batch number: 087, Training: Loss: 0.4985, Accuracy: 0.5938\n",
      "Batch number: 088, Training: Loss: 0.5197, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.5411, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.5407, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.5234, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.5189, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.5037, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.5027, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.5164, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.4544, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.5448, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.5633, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.5399, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.5399, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5597, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.5646, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.5187, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.5648, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.5480, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.5345, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.5464, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.4741, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.5469, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.4989, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 1.3123, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.4988, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.4891, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5413, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 8.2809, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.5633, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 3.3557, Accuracy: 0.3125\n",
      "Batch number: 118, Training: Loss: 0.5420, Accuracy: 0.5000\n",
      "Batch number: 119, Training: Loss: 0.4983, Accuracy: 0.5938\n",
      "Batch number: 120, Training: Loss: 0.4766, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.4981, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5411, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.5856, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.5401, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.5204, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.5858, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5406, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5163, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.5191, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5442, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5388, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5856, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5397, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.4784, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5828, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5827, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.5202, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5390, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5533, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.5203, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5361, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5231, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5151, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5236, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.5096, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5238, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5873, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.6218, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5380, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5700, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5276, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5296, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7417, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3234, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1854, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5562, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5562, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1617, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5562, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.1617, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7417, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.4852, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.4852, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5089, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 034, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5562, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3708, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3234, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5562, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3708, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3234, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6469, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3234, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1617, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5562, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3708, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5562, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.3708, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.4852, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3708, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3708, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.6469, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5562, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5562, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.4852, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3234, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3708, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.4852, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4852, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3234, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.3234, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.6943, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3708, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4852, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1854, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7180, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5325, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6706, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6469, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5089, Accuracy: 0.7500\n",
      "Epoch : 021, Training: Loss : 0.6200, Accuracy: 61.2690%\n",
      "Validation : Loss : 0.5309, Accuracy: 61.5108%, Time: 50.5408s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 23/25\n",
      "Batch number: 000, Training: Loss: 0.5350, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.4945, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 1.6447, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5238, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.4651, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.5346, Accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 006, Training: Loss: 0.5811, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5277, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5313, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.5308, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5273, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.5426, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5588, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.5328, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 5.6595, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 1.8279, Accuracy: 0.5312\n",
      "Batch number: 016, Training: Loss: 0.5466, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.5078, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.5752, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.5760, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.5301, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.5808, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.6052, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.6052, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5425, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.6013, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.5620, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5606, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.5439, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5367, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.5962, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.5860, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.6126, Accuracy: 0.4688\n",
      "Batch number: 033, Training: Loss: 0.5803, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.7269, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5719, Accuracy: 0.5000\n",
      "Batch number: 036, Training: Loss: 0.5954, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.6006, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.5679, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.5781, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.5161, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.5430, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 0.5928, Accuracy: 0.4688\n",
      "Batch number: 043, Training: Loss: 0.5430, Accuracy: 0.5938\n",
      "Batch number: 044, Training: Loss: 0.5882, Accuracy: 0.5312\n",
      "Batch number: 045, Training: Loss: 0.5976, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 0.5465, Accuracy: 0.5312\n",
      "Batch number: 047, Training: Loss: 0.5463, Accuracy: 0.5312\n",
      "Batch number: 048, Training: Loss: 0.5843, Accuracy: 0.5938\n",
      "Batch number: 049, Training: Loss: 0.5828, Accuracy: 0.6250\n",
      "Batch number: 050, Training: Loss: 0.5231, Accuracy: 0.5625\n",
      "Batch number: 051, Training: Loss: 0.5830, Accuracy: 0.6250\n",
      "Batch number: 052, Training: Loss: 3.1020, Accuracy: 0.5312\n",
      "Batch number: 053, Training: Loss: 0.6067, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 0.5390, Accuracy: 0.6875\n",
      "Batch number: 055, Training: Loss: 0.5027, Accuracy: 0.5000\n",
      "Batch number: 056, Training: Loss: 0.5820, Accuracy: 0.6875\n",
      "Batch number: 057, Training: Loss: 0.5617, Accuracy: 0.6562\n",
      "Batch number: 058, Training: Loss: 0.5625, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.5221, Accuracy: 0.5312\n",
      "Batch number: 060, Training: Loss: 0.5812, Accuracy: 0.7500\n",
      "Batch number: 061, Training: Loss: 0.5412, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.5593, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.5440, Accuracy: 0.5312\n",
      "Batch number: 064, Training: Loss: 0.6288, Accuracy: 0.5312\n",
      "Batch number: 065, Training: Loss: 0.5844, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 0.5856, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.4755, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.5658, Accuracy: 0.5312\n",
      "Batch number: 069, Training: Loss: 0.5633, Accuracy: 0.5938\n",
      "Batch number: 070, Training: Loss: 0.5820, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.5436, Accuracy: 0.5625\n",
      "Batch number: 072, Training: Loss: 0.6067, Accuracy: 0.5625\n",
      "Batch number: 073, Training: Loss: 0.6302, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 0.5971, Accuracy: 0.8125\n",
      "Batch number: 075, Training: Loss: 0.5200, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 0.6234, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.5827, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.5600, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 0.5429, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.5097, Accuracy: 0.7812\n",
      "Batch number: 081, Training: Loss: 0.5732, Accuracy: 0.4688\n",
      "Batch number: 082, Training: Loss: 0.5694, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 0.5558, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.5350, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.5512, Accuracy: 0.5312\n",
      "Batch number: 086, Training: Loss: 0.5541, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.5138, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.5454, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.5174, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.5368, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.5601, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.5173, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.5998, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.5371, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.5075, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.4737, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.5755, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.5269, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.5173, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.4590, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5712, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.5502, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.5173, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.5498, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.5817, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 0.5016, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.5763, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.5101, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.5625, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.5209, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5007, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.4802, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.5205, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5409, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.5203, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.5657, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.5939, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.5355, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.5165, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.4826, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.4711, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5616, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5162, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.5905, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.5822, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.5345, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.5843, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.5684, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5591, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.4470, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.5284, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.4745, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5666, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.4772, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.4798, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5613, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5001, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5792, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.5409, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.4975, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5513, Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 143, Training: Loss: 0.5205, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5762, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5032, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5149, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5237, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.5904, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5440, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5672, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.4803, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5785, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5900, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5270, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5899, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7390, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6717, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6717, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1847, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1623, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7390, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6717, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6717, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6717, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6717, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6493, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1623, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.1847, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6717, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3247, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 104, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.6942, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.4870, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6717, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4870, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3247, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5542, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.1623, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3471, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3695, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4870, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6717, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1847, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7166, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5318, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.6493, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5094, Accuracy: 0.7500\n",
      "Epoch : 022, Training: Loss : 0.6134, Accuracy: 61.9496%\n",
      "Validation : Loss : 0.4981, Accuracy: 61.5108%, Time: 50.6317s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 24/25\n",
      "Batch number: 000, Training: Loss: 0.5150, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.5555, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5004, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5236, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.5262, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4945, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.5812, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5676, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5511, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.4908, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.5473, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.4883, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.5590, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.6216, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.4775, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5270, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.4888, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.5331, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.4722, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.5523, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 0.5595, Accuracy: 0.6562\n",
      "Batch number: 022, Training: Loss: 0.5221, Accuracy: 0.5938\n",
      "Batch number: 023, Training: Loss: 0.4801, Accuracy: 0.5938\n",
      "Batch number: 024, Training: Loss: 0.5425, Accuracy: 0.5938\n",
      "Batch number: 025, Training: Loss: 0.5175, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.5622, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.5402, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.5429, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.5176, Accuracy: 0.7188\n",
      "Batch number: 030, Training: Loss: 0.4949, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 0.5641, Accuracy: 0.5625\n",
      "Batch number: 032, Training: Loss: 0.5879, Accuracy: 0.4688\n",
      "Batch number: 033, Training: Loss: 0.4979, Accuracy: 0.6562\n",
      "Batch number: 034, Training: Loss: 0.5199, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5860, Accuracy: 0.5312\n",
      "Batch number: 036, Training: Loss: 0.5664, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 0.4980, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 0.5208, Accuracy: 0.5312\n",
      "Batch number: 039, Training: Loss: 0.5845, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.4332, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.5632, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 0.5625, Accuracy: 0.7188\n",
      "Batch number: 043, Training: Loss: 0.5195, Accuracy: 0.6562\n",
      "Batch number: 044, Training: Loss: 0.5622, Accuracy: 0.6562\n",
      "Batch number: 045, Training: Loss: 0.5440, Accuracy: 0.5000\n",
      "Batch number: 046, Training: Loss: 0.5837, Accuracy: 0.6250\n",
      "Batch number: 047, Training: Loss: 0.5614, Accuracy: 0.6562\n",
      "Batch number: 048, Training: Loss: 0.5189, Accuracy: 0.6562\n",
      "Batch number: 049, Training: Loss: 0.5423, Accuracy: 0.5938\n",
      "Batch number: 050, Training: Loss: 0.5174, Accuracy: 0.6875\n",
      "Batch number: 051, Training: Loss: 0.4753, Accuracy: 0.6875\n",
      "Batch number: 052, Training: Loss: 0.5378, Accuracy: 0.6875\n",
      "Batch number: 053, Training: Loss: 0.5843, Accuracy: 0.5938\n",
      "Batch number: 054, Training: Loss: 0.5241, Accuracy: 0.5625\n",
      "Batch number: 055, Training: Loss: 0.4894, Accuracy: 0.7812\n",
      "Batch number: 056, Training: Loss: 0.5475, Accuracy: 0.5312\n",
      "Batch number: 057, Training: Loss: 0.4998, Accuracy: 0.6250\n",
      "Batch number: 058, Training: Loss: 0.4747, Accuracy: 0.6875\n",
      "Batch number: 059, Training: Loss: 0.6047, Accuracy: 0.5938\n",
      "Batch number: 060, Training: Loss: 0.5534, Accuracy: 0.4688\n",
      "Batch number: 061, Training: Loss: 0.5204, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.5529, Accuracy: 0.4688\n",
      "Batch number: 063, Training: Loss: 0.5364, Accuracy: 0.6875\n",
      "Batch number: 064, Training: Loss: 0.5138, Accuracy: 0.7188\n",
      "Batch number: 065, Training: Loss: 0.5864, Accuracy: 0.5625\n",
      "Batch number: 066, Training: Loss: 0.5615, Accuracy: 0.6250\n",
      "Batch number: 067, Training: Loss: 0.5244, Accuracy: 0.5625\n",
      "Batch number: 068, Training: Loss: 0.5370, Accuracy: 0.6875\n",
      "Batch number: 069, Training: Loss: 0.5183, Accuracy: 0.6562\n",
      "Batch number: 070, Training: Loss: 0.5675, Accuracy: 0.5312\n",
      "Batch number: 071, Training: Loss: 0.5617, Accuracy: 0.6250\n",
      "Batch number: 072, Training: Loss: 0.5338, Accuracy: 0.6875\n",
      "Batch number: 073, Training: Loss: 0.5354, Accuracy: 0.7188\n",
      "Batch number: 074, Training: Loss: 0.4863, Accuracy: 0.5000\n",
      "Batch number: 075, Training: Loss: 0.4976, Accuracy: 0.6562\n",
      "Batch number: 076, Training: Loss: 0.4543, Accuracy: 0.6875\n",
      "Batch number: 077, Training: Loss: 29.9054, Accuracy: 0.6562\n",
      "Batch number: 078, Training: Loss: 1.7748, Accuracy: 0.5625\n",
      "Batch number: 079, Training: Loss: 0.5635, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.4837, Accuracy: 0.5312\n",
      "Batch number: 081, Training: Loss: 0.5570, Accuracy: 0.7188\n",
      "Batch number: 082, Training: Loss: 0.5280, Accuracy: 0.4688\n",
      "Batch number: 083, Training: Loss: 0.5454, Accuracy: 0.5312\n",
      "Batch number: 084, Training: Loss: 0.5226, Accuracy: 0.5625\n",
      "Batch number: 085, Training: Loss: 0.5834, Accuracy: 0.6250\n",
      "Batch number: 086, Training: Loss: 0.4775, Accuracy: 0.6250\n",
      "Batch number: 087, Training: Loss: 0.4773, Accuracy: 0.6250\n",
      "Batch number: 088, Training: Loss: 0.5440, Accuracy: 0.4688\n",
      "Batch number: 089, Training: Loss: 0.5856, Accuracy: 0.5000\n",
      "Batch number: 090, Training: Loss: 0.5848, Accuracy: 0.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 091, Training: Loss: 0.5642, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.5621, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.5453, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.5863, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.6016, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.5598, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.6081, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.5843, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.6224, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.5183, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5844, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.5457, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.5995, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 0.5668, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.5065, Accuracy: 0.5625\n",
      "Batch number: 106, Training: Loss: 2.3337, Accuracy: 0.7812\n",
      "Batch number: 107, Training: Loss: 0.6142, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.5708, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.7367, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.5211, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5808, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.6006, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.5421, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.6203, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.5806, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.5688, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.5802, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.5483, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.4934, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5506, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.4643, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5608, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5506, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.5818, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.5799, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.5446, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.5473, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.5967, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5552, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5515, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.6071, Accuracy: 0.6875\n",
      "Batch number: 132, Training: Loss: 0.7952, Accuracy: 0.5000\n",
      "Batch number: 133, Training: Loss: 0.5120, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5736, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5546, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5053, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5796, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.5611, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5174, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.5611, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5545, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5345, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.4969, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5660, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5129, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5658, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5315, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.5638, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5313, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.5562, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5240, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5174, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5610, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.4746, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5407, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.7873, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.6972, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6521, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6972, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.1968, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 023, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.1518, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.7873, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.1518, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.6972, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6521, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6521, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6521, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.6071, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 064, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.6972, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.6972, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.1968, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1518, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6521, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.6972, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.1518, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.6972, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6521, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.4553, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.3035, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.5905, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.1518, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3486, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.5928, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.3937, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4553, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6521, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.1968, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7423, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.5454, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.4553, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.5004, Accuracy: 0.7500\n",
      "Epoch : 023, Training: Loss : 0.7521, Accuracy: 61.9496%\n",
      "Validation : Loss : 0.4843, Accuracy: 61.5108%, Time: 50.5553s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n",
      "Epoch: 25/25\n",
      "Batch number: 000, Training: Loss: 0.4168, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 0.4737, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.5253, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.5273, Accuracy: 0.5938\n",
      "Batch number: 004, Training: Loss: 0.4207, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.4738, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 0.4243, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.5107, Accuracy: 0.5625\n",
      "Batch number: 008, Training: Loss: 0.5139, Accuracy: 0.5312\n",
      "Batch number: 009, Training: Loss: 0.5100, Accuracy: 0.7188\n",
      "Batch number: 010, Training: Loss: 0.4671, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.4876, Accuracy: 0.5625\n",
      "Batch number: 012, Training: Loss: 0.4962, Accuracy: 0.4688\n",
      "Batch number: 013, Training: Loss: 0.4569, Accuracy: 0.6562\n",
      "Batch number: 014, Training: Loss: 0.5761, Accuracy: 0.4375\n",
      "Batch number: 015, Training: Loss: 0.4563, Accuracy: 0.6562\n",
      "Batch number: 016, Training: Loss: 0.5234, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 0.4396, Accuracy: 0.5312\n",
      "Batch number: 018, Training: Loss: 0.4963, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.4972, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.5203, Accuracy: 0.4688\n",
      "Batch number: 021, Training: Loss: 11.7990, Accuracy: 0.6250\n",
      "Batch number: 022, Training: Loss: 0.4973, Accuracy: 0.6875\n",
      "Batch number: 023, Training: Loss: 0.4083, Accuracy: 0.8125\n",
      "Batch number: 024, Training: Loss: 0.4964, Accuracy: 0.6875\n",
      "Batch number: 025, Training: Loss: 0.5256, Accuracy: 0.5625\n",
      "Batch number: 026, Training: Loss: 0.5202, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.4847, Accuracy: 0.6562\n",
      "Batch number: 028, Training: Loss: 0.5385, Accuracy: 0.6562\n",
      "Batch number: 029, Training: Loss: 1.1927, Accuracy: 0.4688\n",
      "Batch number: 030, Training: Loss: 0.5110, Accuracy: 0.5000\n",
      "Batch number: 031, Training: Loss: 0.5153, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 0.5764, Accuracy: 0.6875\n",
      "Batch number: 033, Training: Loss: 0.5232, Accuracy: 0.5938\n",
      "Batch number: 034, Training: Loss: 0.5205, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.5586, Accuracy: 0.6562\n",
      "Batch number: 036, Training: Loss: 0.5735, Accuracy: 0.7188\n",
      "Batch number: 037, Training: Loss: 0.5696, Accuracy: 0.5312\n",
      "Batch number: 038, Training: Loss: 0.5352, Accuracy: 0.6875\n",
      "Batch number: 039, Training: Loss: 0.5728, Accuracy: 0.5000\n",
      "Batch number: 040, Training: Loss: 0.5265, Accuracy: 0.5625\n",
      "Batch number: 041, Training: Loss: 0.5178, Accuracy: 0.6562\n",
      "Batch number: 042, Training: Loss: 0.5759, Accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch number: 043, Training: Loss: 0.6333, Accuracy: 0.5000\n",
      "Batch number: 044, Training: Loss: 0.5584, Accuracy: 0.6562\n",
      "Batch number: 045, Training: Loss: 0.5753, Accuracy: 0.4688\n",
      "Batch number: 046, Training: Loss: 0.5124, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.5586, Accuracy: 0.6562\n",
      "Batch number: 048, Training: Loss: 0.5870, Accuracy: 0.5625\n",
      "Batch number: 049, Training: Loss: 0.5436, Accuracy: 0.5938\n",
      "Batch number: 050, Training: Loss: 0.5383, Accuracy: 0.6562\n",
      "Batch number: 051, Training: Loss: 0.5895, Accuracy: 0.5312\n",
      "Batch number: 052, Training: Loss: 0.5588, Accuracy: 0.6562\n",
      "Batch number: 053, Training: Loss: 0.6302, Accuracy: 0.5312\n",
      "Batch number: 054, Training: Loss: 0.6172, Accuracy: 0.4375\n",
      "Batch number: 055, Training: Loss: 0.5338, Accuracy: 0.7188\n",
      "Batch number: 056, Training: Loss: 0.5934, Accuracy: 0.4688\n",
      "Batch number: 057, Training: Loss: 0.5907, Accuracy: 0.5000\n",
      "Batch number: 058, Training: Loss: 0.5429, Accuracy: 0.5938\n",
      "Batch number: 059, Training: Loss: 0.6051, Accuracy: 0.5938\n",
      "Batch number: 060, Training: Loss: 0.5939, Accuracy: 0.4062\n",
      "Batch number: 061, Training: Loss: 0.5424, Accuracy: 0.5938\n",
      "Batch number: 062, Training: Loss: 0.5464, Accuracy: 0.4688\n",
      "Batch number: 063, Training: Loss: 0.6279, Accuracy: 0.5625\n",
      "Batch number: 064, Training: Loss: 0.5842, Accuracy: 0.6250\n",
      "Batch number: 065, Training: Loss: 0.5414, Accuracy: 0.6250\n",
      "Batch number: 066, Training: Loss: 0.5632, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.5411, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.5854, Accuracy: 0.5312\n",
      "Batch number: 069, Training: Loss: 0.4990, Accuracy: 0.5938\n",
      "Batch number: 070, Training: Loss: 0.5620, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 0.6066, Accuracy: 0.5625\n",
      "Batch number: 072, Training: Loss: 0.5429, Accuracy: 0.5625\n",
      "Batch number: 073, Training: Loss: 0.5872, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 0.5781, Accuracy: 0.8125\n",
      "Batch number: 075, Training: Loss: 0.5411, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 0.6030, Accuracy: 0.6562\n",
      "Batch number: 077, Training: Loss: 0.6039, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.5811, Accuracy: 0.6562\n",
      "Batch number: 079, Training: Loss: 0.5428, Accuracy: 0.5938\n",
      "Batch number: 080, Training: Loss: 0.5512, Accuracy: 0.7812\n",
      "Batch number: 081, Training: Loss: 0.5937, Accuracy: 0.4688\n",
      "Batch number: 082, Training: Loss: 0.5894, Accuracy: 0.7812\n",
      "Batch number: 083, Training: Loss: 0.5960, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 0.5748, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.5716, Accuracy: 0.5312\n",
      "Batch number: 086, Training: Loss: 0.6126, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.5918, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 0.5232, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 0.5751, Accuracy: 0.6562\n",
      "Batch number: 090, Training: Loss: 0.5746, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.5838, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 0.5926, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.6047, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 0.5803, Accuracy: 0.5312\n",
      "Batch number: 095, Training: Loss: 0.5602, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.5294, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 0.5999, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.5311, Accuracy: 0.5938\n",
      "Batch number: 099, Training: Loss: 0.5359, Accuracy: 0.6562\n",
      "Batch number: 100, Training: Loss: 0.5912, Accuracy: 0.6562\n",
      "Batch number: 101, Training: Loss: 0.5659, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.6119, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.5321, Accuracy: 0.6875\n",
      "Batch number: 104, Training: Loss: 0.6487, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.6331, Accuracy: 0.4688\n",
      "Batch number: 106, Training: Loss: 2.3357, Accuracy: 0.7500\n",
      "Batch number: 107, Training: Loss: 0.6077, Accuracy: 0.5000\n",
      "Batch number: 108, Training: Loss: 0.5771, Accuracy: 0.7188\n",
      "Batch number: 109, Training: Loss: 0.5850, Accuracy: 0.4375\n",
      "Batch number: 110, Training: Loss: 0.5612, Accuracy: 0.6250\n",
      "Batch number: 111, Training: Loss: 0.5796, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.6536, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.5796, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.5611, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.5982, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.5927, Accuracy: 0.5625\n",
      "Batch number: 117, Training: Loss: 0.5102, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.5416, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.5665, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.5560, Accuracy: 0.5625\n",
      "Batch number: 121, Training: Loss: 0.5342, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 0.5428, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 0.5841, Accuracy: 0.6875\n",
      "Batch number: 124, Training: Loss: 0.6263, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 0.5613, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.5581, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 0.5687, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 0.5834, Accuracy: 0.5312\n",
      "Batch number: 129, Training: Loss: 0.5906, Accuracy: 0.6562\n",
      "Batch number: 130, Training: Loss: 0.5250, Accuracy: 0.7812\n",
      "Batch number: 131, Training: Loss: 0.5905, Accuracy: 0.6562\n",
      "Batch number: 132, Training: Loss: 0.6203, Accuracy: 0.5312\n",
      "Batch number: 133, Training: Loss: 0.5465, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.5949, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 0.5720, Accuracy: 0.6562\n",
      "Batch number: 136, Training: Loss: 0.5616, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5616, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 0.6158, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.5720, Accuracy: 0.6562\n",
      "Batch number: 140, Training: Loss: 0.5256, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 0.5719, Accuracy: 0.6562\n",
      "Batch number: 142, Training: Loss: 0.5666, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 0.5797, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 0.5638, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 0.5700, Accuracy: 0.5938\n",
      "Batch number: 146, Training: Loss: 0.5812, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.5703, Accuracy: 0.5938\n",
      "Batch number: 148, Training: Loss: 0.5695, Accuracy: 0.5312\n",
      "Batch number: 149, Training: Loss: 0.5704, Accuracy: 0.5938\n",
      "Batch number: 150, Training: Loss: 0.6142, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 0.5441, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.5359, Accuracy: 0.6562\n",
      "Batch number: 153, Training: Loss: 0.5857, Accuracy: 0.5312\n",
      "Batch number: 154, Training: Loss: 0.5408, Accuracy: 0.7812\n",
      "Batch number: 155, Training: Loss: 0.5668, Accuracy: 0.5312\n",
      "Validation Batch number: 000, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 001, Validation: Loss: 0.8220, Accuracy: 0.0000\n",
      "Validation Batch number: 002, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 003, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 004, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 006, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 007, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 009, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 010, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 011, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 012, Validation: Loss: 0.4343, Accuracy: 1.0000\n",
      "Validation Batch number: 013, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 014, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 016, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 017, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 018, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 019, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 020, Validation: Loss: 0.6165, Accuracy: 0.2500\n",
      "Validation Batch number: 021, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 022, Validation: Loss: 0.7612, Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Batch number: 023, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 024, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 025, Validation: Loss: 0.4343, Accuracy: 1.0000\n",
      "Validation Batch number: 026, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 027, Validation: Loss: 0.2895, Accuracy: 1.0000\n",
      "Validation Batch number: 028, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 029, Validation: Loss: 0.8220, Accuracy: 0.0000\n",
      "Validation Batch number: 030, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 031, Validation: Loss: 0.4343, Accuracy: 1.0000\n",
      "Validation Batch number: 032, Validation: Loss: 0.5791, Accuracy: 1.0000\n",
      "Validation Batch number: 033, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 034, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 035, Validation: Loss: 0.6165, Accuracy: 0.2500\n",
      "Validation Batch number: 036, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 037, Validation: Loss: 0.4110, Accuracy: 0.5000\n",
      "Validation Batch number: 038, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 039, Validation: Loss: 0.2895, Accuracy: 1.0000\n",
      "Validation Batch number: 040, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 041, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 042, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 043, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 044, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 045, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 046, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 047, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 048, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 049, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 050, Validation: Loss: 0.6165, Accuracy: 0.2500\n",
      "Validation Batch number: 051, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 052, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 053, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 054, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 055, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 056, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 057, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 058, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 059, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 060, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 061, Validation: Loss: 0.4343, Accuracy: 1.0000\n",
      "Validation Batch number: 062, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 063, Validation: Loss: 0.5791, Accuracy: 1.0000\n",
      "Validation Batch number: 064, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 065, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 066, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 067, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 068, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 069, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 070, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 071, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 072, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 073, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 074, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 075, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 076, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 077, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 078, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 079, Validation: Loss: 0.2895, Accuracy: 1.0000\n",
      "Validation Batch number: 080, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 081, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 082, Validation: Loss: 0.1448, Accuracy: 1.0000\n",
      "Validation Batch number: 083, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 084, Validation: Loss: 0.6165, Accuracy: 0.2500\n",
      "Validation Batch number: 085, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 086, Validation: Loss: 0.4110, Accuracy: 0.5000\n",
      "Validation Batch number: 087, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 088, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 089, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 090, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 091, Validation: Loss: 0.4343, Accuracy: 1.0000\n",
      "Validation Batch number: 092, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 093, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 094, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 095, Validation: Loss: 0.4110, Accuracy: 0.5000\n",
      "Validation Batch number: 096, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 097, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 098, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 099, Validation: Loss: 0.5791, Accuracy: 1.0000\n",
      "Validation Batch number: 100, Validation: Loss: 0.6165, Accuracy: 0.2500\n",
      "Validation Batch number: 101, Validation: Loss: 0.6165, Accuracy: 0.2500\n",
      "Validation Batch number: 102, Validation: Loss: 0.4343, Accuracy: 1.0000\n",
      "Validation Batch number: 103, Validation: Loss: 0.4343, Accuracy: 1.0000\n",
      "Validation Batch number: 104, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 105, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 106, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 107, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 108, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 109, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 110, Validation: Loss: 0.5791, Accuracy: 1.0000\n",
      "Validation Batch number: 111, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 112, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 113, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 114, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 115, Validation: Loss: 0.5791, Accuracy: 1.0000\n",
      "Validation Batch number: 116, Validation: Loss: 0.4343, Accuracy: 1.0000\n",
      "Validation Batch number: 117, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 118, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 119, Validation: Loss: 0.6165, Accuracy: 0.2500\n",
      "Validation Batch number: 120, Validation: Loss: 0.2895, Accuracy: 1.0000\n",
      "Validation Batch number: 121, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 122, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 123, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 124, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 125, Validation: Loss: 0.4110, Accuracy: 0.5000\n",
      "Validation Batch number: 126, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 127, Validation: Loss: 0.4343, Accuracy: 1.0000\n",
      "Validation Batch number: 128, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 129, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 130, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 131, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Validation Batch number: 132, Validation: Loss: 0.5557, Accuracy: 0.5000\n",
      "Validation Batch number: 133, Validation: Loss: 0.3503, Accuracy: 0.7500\n",
      "Validation Batch number: 134, Validation: Loss: 0.7612, Accuracy: 0.2500\n",
      "Validation Batch number: 135, Validation: Loss: 0.7005, Accuracy: 0.5000\n",
      "Validation Batch number: 136, Validation: Loss: 0.6398, Accuracy: 0.7500\n",
      "Validation Batch number: 137, Validation: Loss: 0.5791, Accuracy: 1.0000\n",
      "Validation Batch number: 138, Validation: Loss: 0.4950, Accuracy: 0.7500\n",
      "Epoch : 024, Training: Loss : 0.6426, Accuracy: 61.8895%\n",
      "Validation : Loss : 0.5736, Accuracy: 61.5108%, Time: 50.5161s\n",
      "Best accuracy achieved so far : 0.8813 on epoch 0\n"
     ]
    }
   ],
   "source": [
    "history = trainValid(vgg19, lossFunc, optimizer, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "culacpOcgcUj"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotCost(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lmqA9osy49ac"
   },
   "source": [
    "Select the best model (i.e. the weights file saved on the max epoch) to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEDTZ_VmkEK6"
   },
   "outputs": [],
   "source": [
    "saved_model = torch.load('src_model_0.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39gAloATgHQl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4996, 556]\n",
      "5552\n",
      "Test Batch number: 000, Test: Loss: 0.5800, Accuracy: 1.0000\n",
      "Test Batch number: 001, Test: Loss: 0.0541, Accuracy: 1.0000\n",
      "Test Batch number: 002, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 003, Test: Loss: 0.0028, Accuracy: 1.0000\n",
      "Test Batch number: 004, Test: Loss: 0.0191, Accuracy: 1.0000\n",
      "Test Batch number: 005, Test: Loss: 0.3819, Accuracy: 1.0000\n",
      "Test Batch number: 006, Test: Loss: 0.2229, Accuracy: 1.0000\n",
      "Test Batch number: 007, Test: Loss: 0.0020, Accuracy: 1.0000\n",
      "Test Batch number: 008, Test: Loss: 0.0275, Accuracy: 1.0000\n",
      "Test Batch number: 009, Test: Loss: 0.3582, Accuracy: 1.0000\n",
      "Test Batch number: 010, Test: Loss: 0.0263, Accuracy: 1.0000\n",
      "Test Batch number: 011, Test: Loss: 0.1369, Accuracy: 1.0000\n",
      "Test Batch number: 012, Test: Loss: 0.0192, Accuracy: 1.0000\n",
      "Test Batch number: 013, Test: Loss: 0.0073, Accuracy: 1.0000\n",
      "Test Batch number: 014, Test: Loss: 0.3422, Accuracy: 1.0000\n",
      "Test Batch number: 015, Test: Loss: 0.4534, Accuracy: 1.0000\n",
      "Test Batch number: 016, Test: Loss: 0.2193, Accuracy: 1.0000\n",
      "Test Batch number: 017, Test: Loss: 0.0995, Accuracy: 1.0000\n",
      "Test Batch number: 018, Test: Loss: 0.2752, Accuracy: 1.0000\n",
      "Test Batch number: 019, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 020, Test: Loss: 0.0444, Accuracy: 1.0000\n",
      "Test Batch number: 021, Test: Loss: 0.1478, Accuracy: 1.0000\n",
      "Test Batch number: 022, Test: Loss: 0.2787, Accuracy: 1.0000\n",
      "Test Batch number: 023, Test: Loss: 0.1000, Accuracy: 1.0000\n",
      "Test Batch number: 024, Test: Loss: 0.0740, Accuracy: 1.0000\n",
      "Test Batch number: 025, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 026, Test: Loss: 0.1805, Accuracy: 1.0000\n",
      "Test Batch number: 027, Test: Loss: 0.5566, Accuracy: 1.0000\n",
      "Test Batch number: 028, Test: Loss: 0.5829, Accuracy: 1.0000\n",
      "Test Batch number: 029, Test: Loss: 0.2052, Accuracy: 1.0000\n",
      "Test Batch number: 030, Test: Loss: 0.0063, Accuracy: 1.0000\n",
      "Test Batch number: 031, Test: Loss: 0.0062, Accuracy: 1.0000\n",
      "Test Batch number: 032, Test: Loss: 0.3317, Accuracy: 1.0000\n",
      "Test Batch number: 033, Test: Loss: 0.0503, Accuracy: 1.0000\n",
      "Test Batch number: 034, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 035, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 036, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 037, Test: Loss: 0.2633, Accuracy: 1.0000\n",
      "Test Batch number: 038, Test: Loss: 0.1827, Accuracy: 1.0000\n",
      "Test Batch number: 039, Test: Loss: 0.0313, Accuracy: 1.0000\n",
      "Test Batch number: 040, Test: Loss: 0.0549, Accuracy: 1.0000\n",
      "Test Batch number: 041, Test: Loss: 0.0022, Accuracy: 1.0000\n",
      "Test Batch number: 042, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 043, Test: Loss: 0.2331, Accuracy: 1.0000\n",
      "Test Batch number: 044, Test: Loss: 0.0074, Accuracy: 1.0000\n",
      "Test Batch number: 045, Test: Loss: 0.0223, Accuracy: 1.0000\n",
      "Test Batch number: 046, Test: Loss: 2.2484, Accuracy: 0.0000\n",
      "Test Batch number: 047, Test: Loss: 0.0116, Accuracy: 1.0000\n",
      "Test Batch number: 048, Test: Loss: 0.2175, Accuracy: 1.0000\n",
      "Test Batch number: 049, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 050, Test: Loss: 0.5871, Accuracy: 1.0000\n",
      "Test Batch number: 051, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 052, Test: Loss: 2.4673, Accuracy: 0.0000\n",
      "Test Batch number: 053, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 054, Test: Loss: 0.1624, Accuracy: 1.0000\n",
      "Test Batch number: 055, Test: Loss: 0.4740, Accuracy: 1.0000\n",
      "Test Batch number: 056, Test: Loss: 0.2628, Accuracy: 1.0000\n",
      "Test Batch number: 057, Test: Loss: 0.2550, Accuracy: 1.0000\n",
      "Test Batch number: 058, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 059, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 060, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 061, Test: Loss: 0.0019, Accuracy: 1.0000\n",
      "Test Batch number: 062, Test: Loss: 0.0621, Accuracy: 1.0000\n",
      "Test Batch number: 063, Test: Loss: 0.0151, Accuracy: 1.0000\n",
      "Test Batch number: 064, Test: Loss: 0.0020, Accuracy: 1.0000\n",
      "Test Batch number: 065, Test: Loss: 0.6182, Accuracy: 1.0000\n",
      "Test Batch number: 066, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 067, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 068, Test: Loss: 0.1964, Accuracy: 1.0000\n",
      "Test Batch number: 069, Test: Loss: 0.0327, Accuracy: 1.0000\n",
      "Test Batch number: 070, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 071, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 072, Test: Loss: 1.0334, Accuracy: 0.0000\n",
      "Test Batch number: 073, Test: Loss: 0.4856, Accuracy: 1.0000\n",
      "Test Batch number: 074, Test: Loss: 0.2628, Accuracy: 1.0000\n",
      "Test Batch number: 075, Test: Loss: 0.2403, Accuracy: 1.0000\n",
      "Test Batch number: 076, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 077, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 078, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 079, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 080, Test: Loss: 0.1700, Accuracy: 1.0000\n",
      "Test Batch number: 081, Test: Loss: 0.5187, Accuracy: 1.0000\n",
      "Test Batch number: 082, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 083, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 084, Test: Loss: 0.5725, Accuracy: 1.0000\n",
      "Test Batch number: 085, Test: Loss: 0.2472, Accuracy: 1.0000\n",
      "Test Batch number: 086, Test: Loss: 1.2201, Accuracy: 0.0000\n",
      "Test Batch number: 087, Test: Loss: 1.1592, Accuracy: 0.0000\n",
      "Test Batch number: 088, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 089, Test: Loss: 0.5900, Accuracy: 1.0000\n",
      "Test Batch number: 090, Test: Loss: 1.0492, Accuracy: 0.0000\n",
      "Test Batch number: 091, Test: Loss: 0.0289, Accuracy: 1.0000\n",
      "Test Batch number: 092, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 093, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 094, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 095, Test: Loss: 2.4921, Accuracy: 0.0000\n",
      "Test Batch number: 096, Test: Loss: 0.1760, Accuracy: 1.0000\n",
      "Test Batch number: 097, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 098, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 099, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 100, Test: Loss: 0.2045, Accuracy: 1.0000\n",
      "Test Batch number: 101, Test: Loss: 0.0297, Accuracy: 1.0000\n",
      "Test Batch number: 102, Test: Loss: 0.1094, Accuracy: 1.0000\n",
      "Test Batch number: 103, Test: Loss: 0.0440, Accuracy: 1.0000\n",
      "Test Batch number: 104, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 105, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 106, Test: Loss: 0.1141, Accuracy: 1.0000\n",
      "Test Batch number: 107, Test: Loss: 0.5879, Accuracy: 1.0000\n",
      "Test Batch number: 108, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 109, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 110, Test: Loss: 0.2760, Accuracy: 1.0000\n",
      "Test Batch number: 111, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 112, Test: Loss: 0.6737, Accuracy: 1.0000\n",
      "Test Batch number: 113, Test: Loss: 0.4518, Accuracy: 1.0000\n",
      "Test Batch number: 114, Test: Loss: 0.0858, Accuracy: 1.0000\n",
      "Test Batch number: 115, Test: Loss: 0.1143, Accuracy: 1.0000\n",
      "Test Batch number: 116, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 117, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 118, Test: Loss: 0.0737, Accuracy: 1.0000\n",
      "Test Batch number: 119, Test: Loss: 0.9713, Accuracy: 0.0000\n",
      "Test Batch number: 120, Test: Loss: 0.0295, Accuracy: 1.0000\n",
      "Test Batch number: 121, Test: Loss: 0.1336, Accuracy: 1.0000\n",
      "Test Batch number: 122, Test: Loss: 0.0160, Accuracy: 1.0000\n",
      "Test Batch number: 123, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 124, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 125, Test: Loss: 0.3591, Accuracy: 1.0000\n",
      "Test Batch number: 126, Test: Loss: 0.8496, Accuracy: 0.0000\n",
      "Test Batch number: 127, Test: Loss: 0.2508, Accuracy: 1.0000\n",
      "Test Batch number: 128, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 129, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 130, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 131, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 132, Test: Loss: 0.0453, Accuracy: 1.0000\n",
      "Test Batch number: 133, Test: Loss: 0.0132, Accuracy: 1.0000\n",
      "Test Batch number: 134, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 135, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 136, Test: Loss: 0.0017, Accuracy: 1.0000\n",
      "Test Batch number: 137, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 138, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 139, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 140, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 141, Test: Loss: 0.0431, Accuracy: 1.0000\n",
      "Test Batch number: 142, Test: Loss: 0.4158, Accuracy: 1.0000\n",
      "Test Batch number: 143, Test: Loss: 1.0891, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 144, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 145, Test: Loss: 0.2261, Accuracy: 1.0000\n",
      "Test Batch number: 146, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 147, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 148, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 149, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 150, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 151, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 152, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 153, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 154, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 155, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 156, Test: Loss: 0.9480, Accuracy: 0.0000\n",
      "Test Batch number: 157, Test: Loss: 0.7595, Accuracy: 0.0000\n",
      "Test Batch number: 158, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 159, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 160, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 161, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 162, Test: Loss: 0.7825, Accuracy: 0.0000\n",
      "Test Batch number: 163, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 164, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 165, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 166, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 167, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 168, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 169, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 170, Test: Loss: 0.0009, Accuracy: 1.0000\n",
      "Test Batch number: 171, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 172, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 173, Test: Loss: 0.5830, Accuracy: 1.0000\n",
      "Test Batch number: 174, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 175, Test: Loss: 0.3301, Accuracy: 1.0000\n",
      "Test Batch number: 176, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 177, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 178, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 179, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 180, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 181, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 182, Test: Loss: 0.8912, Accuracy: 0.0000\n",
      "Test Batch number: 183, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 184, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 185, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 186, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 187, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 188, Test: Loss: 0.0010, Accuracy: 1.0000\n",
      "Test Batch number: 189, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 190, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 191, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 192, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 193, Test: Loss: 0.0008, Accuracy: 1.0000\n",
      "Test Batch number: 194, Test: Loss: 0.0824, Accuracy: 1.0000\n",
      "Test Batch number: 195, Test: Loss: 1.0345, Accuracy: 0.0000\n",
      "Test Batch number: 196, Test: Loss: 0.2530, Accuracy: 1.0000\n",
      "Test Batch number: 197, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 198, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 199, Test: Loss: 0.0144, Accuracy: 1.0000\n",
      "Test Batch number: 200, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 201, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 202, Test: Loss: 0.0009, Accuracy: 1.0000\n",
      "Test Batch number: 203, Test: Loss: 0.0551, Accuracy: 1.0000\n",
      "Test Batch number: 204, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 205, Test: Loss: 0.8761, Accuracy: 0.0000\n",
      "Test Batch number: 206, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 207, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 208, Test: Loss: 2.2667, Accuracy: 0.0000\n",
      "Test Batch number: 209, Test: Loss: 0.1793, Accuracy: 1.0000\n",
      "Test Batch number: 210, Test: Loss: 0.0989, Accuracy: 1.0000\n",
      "Test Batch number: 211, Test: Loss: 0.4009, Accuracy: 1.0000\n",
      "Test Batch number: 212, Test: Loss: 0.2499, Accuracy: 1.0000\n",
      "Test Batch number: 213, Test: Loss: 1.2611, Accuracy: 0.0000\n",
      "Test Batch number: 214, Test: Loss: 0.0037, Accuracy: 1.0000\n",
      "Test Batch number: 215, Test: Loss: 0.0128, Accuracy: 1.0000\n",
      "Test Batch number: 216, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 217, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 218, Test: Loss: 0.0074, Accuracy: 1.0000\n",
      "Test Batch number: 219, Test: Loss: 0.0024, Accuracy: 1.0000\n",
      "Test Batch number: 220, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 221, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 222, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 223, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 224, Test: Loss: 0.1821, Accuracy: 1.0000\n",
      "Test Batch number: 225, Test: Loss: 0.0676, Accuracy: 1.0000\n",
      "Test Batch number: 226, Test: Loss: 0.2495, Accuracy: 1.0000\n",
      "Test Batch number: 227, Test: Loss: 0.0202, Accuracy: 1.0000\n",
      "Test Batch number: 228, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 229, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 230, Test: Loss: 0.0011, Accuracy: 1.0000\n",
      "Test Batch number: 231, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 232, Test: Loss: 0.4914, Accuracy: 1.0000\n",
      "Test Batch number: 233, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 234, Test: Loss: 0.6970, Accuracy: 0.0000\n",
      "Test Batch number: 235, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 236, Test: Loss: 0.0029, Accuracy: 1.0000\n",
      "Test Batch number: 237, Test: Loss: 0.2712, Accuracy: 1.0000\n",
      "Test Batch number: 238, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 239, Test: Loss: 0.7127, Accuracy: 0.0000\n",
      "Test Batch number: 240, Test: Loss: 0.0051, Accuracy: 1.0000\n",
      "Test Batch number: 241, Test: Loss: 0.1239, Accuracy: 1.0000\n",
      "Test Batch number: 242, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 243, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 244, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 245, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 246, Test: Loss: 0.3637, Accuracy: 1.0000\n",
      "Test Batch number: 247, Test: Loss: 0.0008, Accuracy: 1.0000\n",
      "Test Batch number: 248, Test: Loss: 0.0526, Accuracy: 1.0000\n",
      "Test Batch number: 249, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 250, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 251, Test: Loss: 0.0191, Accuracy: 1.0000\n",
      "Test Batch number: 252, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 253, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 254, Test: Loss: 0.0034, Accuracy: 1.0000\n",
      "Test Batch number: 255, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 256, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 257, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 258, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 259, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 260, Test: Loss: 0.4274, Accuracy: 1.0000\n",
      "Test Batch number: 261, Test: Loss: 0.3104, Accuracy: 1.0000\n",
      "Test Batch number: 262, Test: Loss: 0.0167, Accuracy: 1.0000\n",
      "Test Batch number: 263, Test: Loss: 0.6097, Accuracy: 1.0000\n",
      "Test Batch number: 264, Test: Loss: 0.2302, Accuracy: 1.0000\n",
      "Test Batch number: 265, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 266, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 267, Test: Loss: 0.9258, Accuracy: 0.0000\n",
      "Test Batch number: 268, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 269, Test: Loss: 0.0169, Accuracy: 1.0000\n",
      "Test Batch number: 270, Test: Loss: 0.7517, Accuracy: 0.0000\n",
      "Test Batch number: 271, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 272, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 273, Test: Loss: 0.0353, Accuracy: 1.0000\n",
      "Test Batch number: 274, Test: Loss: 0.0021, Accuracy: 1.0000\n",
      "Test Batch number: 275, Test: Loss: 0.4325, Accuracy: 1.0000\n",
      "Test Batch number: 276, Test: Loss: 0.0429, Accuracy: 1.0000\n",
      "Test Batch number: 277, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 278, Test: Loss: 0.2409, Accuracy: 1.0000\n",
      "Test Batch number: 279, Test: Loss: 0.3928, Accuracy: 1.0000\n",
      "Test Batch number: 280, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 281, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 282, Test: Loss: 0.5368, Accuracy: 1.0000\n",
      "Test Batch number: 283, Test: Loss: 0.1708, Accuracy: 1.0000\n",
      "Test Batch number: 284, Test: Loss: 0.0333, Accuracy: 1.0000\n",
      "Test Batch number: 285, Test: Loss: 0.4373, Accuracy: 1.0000\n",
      "Test Batch number: 286, Test: Loss: 0.5039, Accuracy: 1.0000\n",
      "Test Batch number: 287, Test: Loss: 0.0876, Accuracy: 1.0000\n",
      "Test Batch number: 288, Test: Loss: 0.2078, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 289, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 290, Test: Loss: 1.1299, Accuracy: 0.0000\n",
      "Test Batch number: 291, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 292, Test: Loss: 0.5220, Accuracy: 1.0000\n",
      "Test Batch number: 293, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 294, Test: Loss: 0.4324, Accuracy: 1.0000\n",
      "Test Batch number: 295, Test: Loss: 0.1058, Accuracy: 1.0000\n",
      "Test Batch number: 296, Test: Loss: 0.9377, Accuracy: 0.0000\n",
      "Test Batch number: 297, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 298, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 299, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 300, Test: Loss: 0.3846, Accuracy: 1.0000\n",
      "Test Batch number: 301, Test: Loss: 0.8034, Accuracy: 0.0000\n",
      "Test Batch number: 302, Test: Loss: 0.2341, Accuracy: 1.0000\n",
      "Test Batch number: 303, Test: Loss: 0.0201, Accuracy: 1.0000\n",
      "Test Batch number: 304, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 305, Test: Loss: 0.0291, Accuracy: 1.0000\n",
      "Test Batch number: 306, Test: Loss: 1.5108, Accuracy: 0.0000\n",
      "Test Batch number: 307, Test: Loss: 0.8735, Accuracy: 0.0000\n",
      "Test Batch number: 308, Test: Loss: 0.8620, Accuracy: 0.0000\n",
      "Test Batch number: 309, Test: Loss: 1.2623, Accuracy: 0.0000\n",
      "Test Batch number: 310, Test: Loss: 0.0039, Accuracy: 1.0000\n",
      "Test Batch number: 311, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 312, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 313, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 314, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 315, Test: Loss: 0.3998, Accuracy: 1.0000\n",
      "Test Batch number: 316, Test: Loss: 0.4581, Accuracy: 1.0000\n",
      "Test Batch number: 317, Test: Loss: 0.0795, Accuracy: 1.0000\n",
      "Test Batch number: 318, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 319, Test: Loss: 0.0057, Accuracy: 1.0000\n",
      "Test Batch number: 320, Test: Loss: 0.2074, Accuracy: 1.0000\n",
      "Test Batch number: 321, Test: Loss: 1.1765, Accuracy: 0.0000\n",
      "Test Batch number: 322, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 323, Test: Loss: 0.0010, Accuracy: 1.0000\n",
      "Test Batch number: 324, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 325, Test: Loss: 0.4065, Accuracy: 1.0000\n",
      "Test Batch number: 326, Test: Loss: 1.2044, Accuracy: 0.0000\n",
      "Test Batch number: 327, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 328, Test: Loss: 2.3521, Accuracy: 0.0000\n",
      "Test Batch number: 329, Test: Loss: 0.2832, Accuracy: 1.0000\n",
      "Test Batch number: 330, Test: Loss: 0.1147, Accuracy: 1.0000\n",
      "Test Batch number: 331, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 332, Test: Loss: 0.0161, Accuracy: 1.0000\n",
      "Test Batch number: 333, Test: Loss: 0.0608, Accuracy: 1.0000\n",
      "Test Batch number: 334, Test: Loss: 0.0541, Accuracy: 1.0000\n",
      "Test Batch number: 335, Test: Loss: 0.4249, Accuracy: 1.0000\n",
      "Test Batch number: 336, Test: Loss: 0.3123, Accuracy: 1.0000\n",
      "Test Batch number: 337, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 338, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 339, Test: Loss: 0.0023, Accuracy: 1.0000\n",
      "Test Batch number: 340, Test: Loss: 0.8443, Accuracy: 0.0000\n",
      "Test Batch number: 341, Test: Loss: 0.0009, Accuracy: 1.0000\n",
      "Test Batch number: 342, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 343, Test: Loss: 0.9279, Accuracy: 0.0000\n",
      "Test Batch number: 344, Test: Loss: 0.2696, Accuracy: 1.0000\n",
      "Test Batch number: 345, Test: Loss: 0.9162, Accuracy: 0.0000\n",
      "Test Batch number: 346, Test: Loss: 0.4331, Accuracy: 1.0000\n",
      "Test Batch number: 347, Test: Loss: 0.1289, Accuracy: 1.0000\n",
      "Test Batch number: 348, Test: Loss: 0.2423, Accuracy: 1.0000\n",
      "Test Batch number: 349, Test: Loss: 0.6221, Accuracy: 1.0000\n",
      "Test Batch number: 350, Test: Loss: 0.2739, Accuracy: 1.0000\n",
      "Test Batch number: 351, Test: Loss: 0.0406, Accuracy: 1.0000\n",
      "Test Batch number: 352, Test: Loss: 0.0025, Accuracy: 1.0000\n",
      "Test Batch number: 353, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 354, Test: Loss: 0.1548, Accuracy: 1.0000\n",
      "Test Batch number: 355, Test: Loss: 0.0197, Accuracy: 1.0000\n",
      "Test Batch number: 356, Test: Loss: 0.9973, Accuracy: 0.0000\n",
      "Test Batch number: 357, Test: Loss: 0.1426, Accuracy: 1.0000\n",
      "Test Batch number: 358, Test: Loss: 0.1435, Accuracy: 1.0000\n",
      "Test Batch number: 359, Test: Loss: 0.5867, Accuracy: 1.0000\n",
      "Test Batch number: 360, Test: Loss: 0.6771, Accuracy: 1.0000\n",
      "Test Batch number: 361, Test: Loss: 0.1042, Accuracy: 1.0000\n",
      "Test Batch number: 362, Test: Loss: 0.7380, Accuracy: 0.0000\n",
      "Test Batch number: 363, Test: Loss: 0.0036, Accuracy: 1.0000\n",
      "Test Batch number: 364, Test: Loss: 1.0228, Accuracy: 0.0000\n",
      "Test Batch number: 365, Test: Loss: 0.1830, Accuracy: 1.0000\n",
      "Test Batch number: 366, Test: Loss: 0.1022, Accuracy: 1.0000\n",
      "Test Batch number: 367, Test: Loss: 0.7058, Accuracy: 0.0000\n",
      "Test Batch number: 368, Test: Loss: 0.0134, Accuracy: 1.0000\n",
      "Test Batch number: 369, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 370, Test: Loss: 0.9106, Accuracy: 0.0000\n",
      "Test Batch number: 371, Test: Loss: 0.1229, Accuracy: 1.0000\n",
      "Test Batch number: 372, Test: Loss: 0.3316, Accuracy: 1.0000\n",
      "Test Batch number: 373, Test: Loss: 0.5741, Accuracy: 1.0000\n",
      "Test Batch number: 374, Test: Loss: 0.3883, Accuracy: 1.0000\n",
      "Test Batch number: 375, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 376, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 377, Test: Loss: 0.1983, Accuracy: 1.0000\n",
      "Test Batch number: 378, Test: Loss: 1.0151, Accuracy: 0.0000\n",
      "Test Batch number: 379, Test: Loss: 0.5539, Accuracy: 1.0000\n",
      "Test Batch number: 380, Test: Loss: 2.3964, Accuracy: 0.0000\n",
      "Test Batch number: 381, Test: Loss: 0.6530, Accuracy: 1.0000\n",
      "Test Batch number: 382, Test: Loss: 0.5977, Accuracy: 1.0000\n",
      "Test Batch number: 383, Test: Loss: 0.0025, Accuracy: 1.0000\n",
      "Test Batch number: 384, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 385, Test: Loss: 0.3352, Accuracy: 1.0000\n",
      "Test Batch number: 386, Test: Loss: 0.2796, Accuracy: 1.0000\n",
      "Test Batch number: 387, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 388, Test: Loss: 0.0569, Accuracy: 1.0000\n",
      "Test Batch number: 389, Test: Loss: 1.2178, Accuracy: 0.0000\n",
      "Test Batch number: 390, Test: Loss: 1.2132, Accuracy: 0.0000\n",
      "Test Batch number: 391, Test: Loss: 0.4128, Accuracy: 1.0000\n",
      "Test Batch number: 392, Test: Loss: 0.1655, Accuracy: 1.0000\n",
      "Test Batch number: 393, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 394, Test: Loss: 0.3630, Accuracy: 1.0000\n",
      "Test Batch number: 395, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 396, Test: Loss: 0.0003, Accuracy: 1.0000\n",
      "Test Batch number: 397, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 398, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 399, Test: Loss: 0.0047, Accuracy: 1.0000\n",
      "Test Batch number: 400, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 401, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 402, Test: Loss: 0.1179, Accuracy: 1.0000\n",
      "Test Batch number: 403, Test: Loss: 0.0235, Accuracy: 1.0000\n",
      "Test Batch number: 404, Test: Loss: 0.4186, Accuracy: 1.0000\n",
      "Test Batch number: 405, Test: Loss: 0.7488, Accuracy: 0.0000\n",
      "Test Batch number: 406, Test: Loss: 0.3069, Accuracy: 1.0000\n",
      "Test Batch number: 407, Test: Loss: 0.2115, Accuracy: 1.0000\n",
      "Test Batch number: 408, Test: Loss: 0.7411, Accuracy: 0.0000\n",
      "Test Batch number: 409, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 410, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 411, Test: Loss: 0.0986, Accuracy: 1.0000\n",
      "Test Batch number: 412, Test: Loss: 0.2628, Accuracy: 1.0000\n",
      "Test Batch number: 413, Test: Loss: 0.1632, Accuracy: 1.0000\n",
      "Test Batch number: 414, Test: Loss: 0.9617, Accuracy: 0.0000\n",
      "Test Batch number: 415, Test: Loss: 0.0008, Accuracy: 1.0000\n",
      "Test Batch number: 416, Test: Loss: 0.5640, Accuracy: 1.0000\n",
      "Test Batch number: 417, Test: Loss: 0.1512, Accuracy: 1.0000\n",
      "Test Batch number: 418, Test: Loss: 0.6360, Accuracy: 1.0000\n",
      "Test Batch number: 419, Test: Loss: 0.0282, Accuracy: 1.0000\n",
      "Test Batch number: 420, Test: Loss: 1.1655, Accuracy: 0.0000\n",
      "Test Batch number: 421, Test: Loss: 0.5263, Accuracy: 1.0000\n",
      "Test Batch number: 422, Test: Loss: 0.6383, Accuracy: 1.0000\n",
      "Test Batch number: 423, Test: Loss: 0.0016, Accuracy: 1.0000\n",
      "Test Batch number: 424, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 425, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 426, Test: Loss: 0.0450, Accuracy: 1.0000\n",
      "Test Batch number: 427, Test: Loss: 0.2477, Accuracy: 1.0000\n",
      "Test Batch number: 428, Test: Loss: 1.5483, Accuracy: 0.0000\n",
      "Test Batch number: 429, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 430, Test: Loss: 0.4640, Accuracy: 1.0000\n",
      "Test Batch number: 431, Test: Loss: 0.1370, Accuracy: 1.0000\n",
      "Test Batch number: 432, Test: Loss: 0.0003, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 433, Test: Loss: 0.0043, Accuracy: 1.0000\n",
      "Test Batch number: 434, Test: Loss: 0.3010, Accuracy: 1.0000\n",
      "Test Batch number: 435, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 436, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 437, Test: Loss: 0.2751, Accuracy: 1.0000\n",
      "Test Batch number: 438, Test: Loss: 0.0950, Accuracy: 1.0000\n",
      "Test Batch number: 439, Test: Loss: 0.1212, Accuracy: 1.0000\n",
      "Test Batch number: 440, Test: Loss: 0.0003, Accuracy: 1.0000\n",
      "Test Batch number: 441, Test: Loss: 0.0883, Accuracy: 1.0000\n",
      "Test Batch number: 442, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 443, Test: Loss: 1.8922, Accuracy: 0.0000\n",
      "Test Batch number: 444, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 445, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 446, Test: Loss: 4.3724, Accuracy: 0.0000\n",
      "Test Batch number: 447, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 448, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 449, Test: Loss: 0.2628, Accuracy: 1.0000\n",
      "Test Batch number: 450, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 451, Test: Loss: 0.0683, Accuracy: 1.0000\n",
      "Test Batch number: 452, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 453, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 454, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 455, Test: Loss: 0.3096, Accuracy: 1.0000\n",
      "Test Batch number: 456, Test: Loss: 0.0092, Accuracy: 1.0000\n",
      "Test Batch number: 457, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 458, Test: Loss: 0.6270, Accuracy: 1.0000\n",
      "Test Batch number: 459, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 460, Test: Loss: 7.6072, Accuracy: 0.0000\n",
      "Test Batch number: 461, Test: Loss: 0.0164, Accuracy: 1.0000\n",
      "Test Batch number: 462, Test: Loss: 0.0026, Accuracy: 1.0000\n",
      "Test Batch number: 463, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 464, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 465, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 466, Test: Loss: 0.6079, Accuracy: 1.0000\n",
      "Test Batch number: 467, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 468, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 469, Test: Loss: 0.4077, Accuracy: 1.0000\n",
      "Test Batch number: 470, Test: Loss: 0.6645, Accuracy: 1.0000\n",
      "Test Batch number: 471, Test: Loss: 0.0012, Accuracy: 1.0000\n",
      "Test Batch number: 472, Test: Loss: 0.0007, Accuracy: 1.0000\n",
      "Test Batch number: 473, Test: Loss: 0.0019, Accuracy: 1.0000\n",
      "Test Batch number: 474, Test: Loss: 0.0050, Accuracy: 1.0000\n",
      "Test Batch number: 475, Test: Loss: 0.6337, Accuracy: 1.0000\n",
      "Test Batch number: 476, Test: Loss: 0.9755, Accuracy: 0.0000\n",
      "Test Batch number: 477, Test: Loss: 0.9382, Accuracy: 0.0000\n",
      "Test Batch number: 478, Test: Loss: 0.1706, Accuracy: 1.0000\n",
      "Test Batch number: 479, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 480, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 481, Test: Loss: 0.0031, Accuracy: 1.0000\n",
      "Test Batch number: 482, Test: Loss: 0.5067, Accuracy: 1.0000\n",
      "Test Batch number: 483, Test: Loss: 0.6270, Accuracy: 1.0000\n",
      "Test Batch number: 484, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 485, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 486, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 487, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 488, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 489, Test: Loss: 0.5648, Accuracy: 1.0000\n",
      "Test Batch number: 490, Test: Loss: 0.1670, Accuracy: 1.0000\n",
      "Test Batch number: 491, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 492, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 493, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 494, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 495, Test: Loss: 0.0003, Accuracy: 1.0000\n",
      "Test Batch number: 496, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 497, Test: Loss: 0.4175, Accuracy: 1.0000\n",
      "Test Batch number: 498, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 499, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 500, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 501, Test: Loss: 0.6368, Accuracy: 1.0000\n",
      "Test Batch number: 502, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 503, Test: Loss: 0.2542, Accuracy: 1.0000\n",
      "Test Batch number: 504, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 505, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 506, Test: Loss: 0.6979, Accuracy: 0.0000\n",
      "Test Batch number: 507, Test: Loss: 0.0444, Accuracy: 1.0000\n",
      "Test Batch number: 508, Test: Loss: 0.1122, Accuracy: 1.0000\n",
      "Test Batch number: 509, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 510, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 511, Test: Loss: 0.2628, Accuracy: 1.0000\n",
      "Test Batch number: 512, Test: Loss: 0.0316, Accuracy: 1.0000\n",
      "Test Batch number: 513, Test: Loss: 0.0035, Accuracy: 1.0000\n",
      "Test Batch number: 514, Test: Loss: 0.0048, Accuracy: 1.0000\n",
      "Test Batch number: 515, Test: Loss: 0.2170, Accuracy: 1.0000\n",
      "Test Batch number: 516, Test: Loss: 0.7392, Accuracy: 0.0000\n",
      "Test Batch number: 517, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 518, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 519, Test: Loss: 0.9756, Accuracy: 0.0000\n",
      "Test Batch number: 520, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 521, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 522, Test: Loss: 0.2632, Accuracy: 1.0000\n",
      "Test Batch number: 523, Test: Loss: 0.3323, Accuracy: 1.0000\n",
      "Test Batch number: 524, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 525, Test: Loss: 0.4019, Accuracy: 1.0000\n",
      "Test Batch number: 526, Test: Loss: 0.3917, Accuracy: 1.0000\n",
      "Test Batch number: 527, Test: Loss: 1.8556, Accuracy: 0.0000\n",
      "Test Batch number: 528, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 529, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 530, Test: Loss: 0.6637, Accuracy: 1.0000\n",
      "Test Batch number: 531, Test: Loss: 0.2331, Accuracy: 1.0000\n",
      "Test Batch number: 532, Test: Loss: 0.7991, Accuracy: 0.0000\n",
      "Test Batch number: 533, Test: Loss: 0.7952, Accuracy: 0.0000\n",
      "Test Batch number: 534, Test: Loss: 0.7852, Accuracy: 0.0000\n",
      "Test Batch number: 535, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 536, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 537, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 538, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 539, Test: Loss: 0.0003, Accuracy: 1.0000\n",
      "Test Batch number: 540, Test: Loss: 0.5013, Accuracy: 1.0000\n",
      "Test Batch number: 541, Test: Loss: 0.2265, Accuracy: 1.0000\n",
      "Test Batch number: 542, Test: Loss: 0.2994, Accuracy: 1.0000\n",
      "Test Batch number: 543, Test: Loss: 0.1464, Accuracy: 1.0000\n",
      "Test Batch number: 544, Test: Loss: 1.0121, Accuracy: 0.0000\n",
      "Test Batch number: 545, Test: Loss: 0.4228, Accuracy: 1.0000\n",
      "Test Batch number: 546, Test: Loss: 0.0010, Accuracy: 1.0000\n",
      "Test Batch number: 547, Test: Loss: 0.4103, Accuracy: 1.0000\n",
      "Test Batch number: 548, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 549, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 550, Test: Loss: 0.4297, Accuracy: 1.0000\n",
      "Test Batch number: 551, Test: Loss: 0.3641, Accuracy: 1.0000\n",
      "Test Batch number: 552, Test: Loss: 0.2567, Accuracy: 1.0000\n",
      "Test Batch number: 553, Test: Loss: 0.6757, Accuracy: 1.0000\n",
      "Test Batch number: 554, Test: Loss: 0.7549, Accuracy: 0.0000\n",
      "Test Batch number: 555, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 556, Test: Loss: 0.5206, Accuracy: 1.0000\n",
      "Test Batch number: 557, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 558, Test: Loss: 0.2795, Accuracy: 1.0000\n",
      "Test Batch number: 559, Test: Loss: 1.0820, Accuracy: 0.0000\n",
      "Test Batch number: 560, Test: Loss: 0.1696, Accuracy: 1.0000\n",
      "Test Batch number: 561, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 562, Test: Loss: 0.2208, Accuracy: 1.0000\n",
      "Test Batch number: 563, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 564, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 565, Test: Loss: 0.4878, Accuracy: 1.0000\n",
      "Test Batch number: 566, Test: Loss: 0.0749, Accuracy: 1.0000\n",
      "Test Batch number: 567, Test: Loss: 0.3952, Accuracy: 1.0000\n",
      "Test Batch number: 568, Test: Loss: 1.4067, Accuracy: 0.0000\n",
      "Test Batch number: 569, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 570, Test: Loss: 0.5815, Accuracy: 1.0000\n",
      "Test Batch number: 571, Test: Loss: 0.0607, Accuracy: 1.0000\n",
      "Test Batch number: 572, Test: Loss: 1.4648, Accuracy: 0.0000\n",
      "Test Batch number: 573, Test: Loss: 0.1729, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 574, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 575, Test: Loss: 0.1530, Accuracy: 1.0000\n",
      "Test Batch number: 576, Test: Loss: 1.8996, Accuracy: 0.0000\n",
      "Test Batch number: 577, Test: Loss: 0.5670, Accuracy: 1.0000\n",
      "Test Batch number: 578, Test: Loss: 0.8156, Accuracy: 0.0000\n",
      "Test Batch number: 579, Test: Loss: 0.0157, Accuracy: 1.0000\n",
      "Test Batch number: 580, Test: Loss: 0.0006, Accuracy: 1.0000\n",
      "Test Batch number: 581, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 582, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 583, Test: Loss: 0.0061, Accuracy: 1.0000\n",
      "Test Batch number: 584, Test: Loss: 0.5108, Accuracy: 1.0000\n",
      "Test Batch number: 585, Test: Loss: 0.0025, Accuracy: 1.0000\n",
      "Test Batch number: 586, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 587, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 588, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 589, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 590, Test: Loss: 0.8664, Accuracy: 0.0000\n",
      "Test Batch number: 591, Test: Loss: 0.4319, Accuracy: 1.0000\n",
      "Test Batch number: 592, Test: Loss: 0.5666, Accuracy: 1.0000\n",
      "Test Batch number: 593, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 594, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 595, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 596, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 597, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 598, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 599, Test: Loss: 0.2592, Accuracy: 1.0000\n",
      "Test Batch number: 600, Test: Loss: 0.3011, Accuracy: 1.0000\n",
      "Test Batch number: 601, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 602, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 603, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 604, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 605, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 606, Test: Loss: 0.2770, Accuracy: 1.0000\n",
      "Test Batch number: 607, Test: Loss: 0.4215, Accuracy: 1.0000\n",
      "Test Batch number: 608, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 609, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 610, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 611, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 612, Test: Loss: 0.4778, Accuracy: 1.0000\n",
      "Test Batch number: 613, Test: Loss: 0.7976, Accuracy: 0.0000\n",
      "Test Batch number: 614, Test: Loss: 0.0067, Accuracy: 1.0000\n",
      "Test Batch number: 615, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 616, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 617, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 618, Test: Loss: 1.2771, Accuracy: 0.0000\n",
      "Test Batch number: 619, Test: Loss: 0.1904, Accuracy: 1.0000\n",
      "Test Batch number: 620, Test: Loss: 0.0201, Accuracy: 1.0000\n",
      "Test Batch number: 621, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 622, Test: Loss: 0.2965, Accuracy: 1.0000\n",
      "Test Batch number: 623, Test: Loss: 0.0198, Accuracy: 1.0000\n",
      "Test Batch number: 624, Test: Loss: 0.3960, Accuracy: 1.0000\n",
      "Test Batch number: 625, Test: Loss: 0.1388, Accuracy: 1.0000\n",
      "Test Batch number: 626, Test: Loss: 0.6490, Accuracy: 1.0000\n",
      "Test Batch number: 627, Test: Loss: 0.3613, Accuracy: 1.0000\n",
      "Test Batch number: 628, Test: Loss: 0.0578, Accuracy: 1.0000\n",
      "Test Batch number: 629, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 630, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 631, Test: Loss: 0.1143, Accuracy: 1.0000\n",
      "Test Batch number: 632, Test: Loss: 0.4115, Accuracy: 1.0000\n",
      "Test Batch number: 633, Test: Loss: 0.1782, Accuracy: 1.0000\n",
      "Test Batch number: 634, Test: Loss: 0.4361, Accuracy: 1.0000\n",
      "Test Batch number: 635, Test: Loss: 0.9948, Accuracy: 0.0000\n",
      "Test Batch number: 636, Test: Loss: 0.1144, Accuracy: 1.0000\n",
      "Test Batch number: 637, Test: Loss: 0.4215, Accuracy: 1.0000\n",
      "Test Batch number: 638, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 639, Test: Loss: 0.1818, Accuracy: 1.0000\n",
      "Test Batch number: 640, Test: Loss: 0.1954, Accuracy: 1.0000\n",
      "Test Batch number: 641, Test: Loss: 0.1683, Accuracy: 1.0000\n",
      "Test Batch number: 642, Test: Loss: 0.5087, Accuracy: 1.0000\n",
      "Test Batch number: 643, Test: Loss: 0.0015, Accuracy: 1.0000\n",
      "Test Batch number: 644, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 645, Test: Loss: 0.6707, Accuracy: 1.0000\n",
      "Test Batch number: 646, Test: Loss: 0.4465, Accuracy: 1.0000\n",
      "Test Batch number: 647, Test: Loss: 0.0263, Accuracy: 1.0000\n",
      "Test Batch number: 648, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 649, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 650, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 651, Test: Loss: 0.2227, Accuracy: 1.0000\n",
      "Test Batch number: 652, Test: Loss: 0.3244, Accuracy: 1.0000\n",
      "Test Batch number: 653, Test: Loss: 0.4380, Accuracy: 1.0000\n",
      "Test Batch number: 654, Test: Loss: 0.1238, Accuracy: 1.0000\n",
      "Test Batch number: 655, Test: Loss: 2.2076, Accuracy: 0.0000\n",
      "Test Batch number: 656, Test: Loss: 0.0030, Accuracy: 1.0000\n",
      "Test Batch number: 657, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 658, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 659, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 660, Test: Loss: 0.3650, Accuracy: 1.0000\n",
      "Test Batch number: 661, Test: Loss: 0.6673, Accuracy: 1.0000\n",
      "Test Batch number: 662, Test: Loss: 0.3324, Accuracy: 1.0000\n",
      "Test Batch number: 663, Test: Loss: 0.5055, Accuracy: 1.0000\n",
      "Test Batch number: 664, Test: Loss: 0.3269, Accuracy: 1.0000\n",
      "Test Batch number: 665, Test: Loss: 0.4547, Accuracy: 1.0000\n",
      "Test Batch number: 666, Test: Loss: 0.0810, Accuracy: 1.0000\n",
      "Test Batch number: 667, Test: Loss: 0.0489, Accuracy: 1.0000\n",
      "Test Batch number: 668, Test: Loss: 0.3386, Accuracy: 1.0000\n",
      "Test Batch number: 669, Test: Loss: 0.7891, Accuracy: 0.0000\n",
      "Test Batch number: 670, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 671, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 672, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 673, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 674, Test: Loss: 0.0025, Accuracy: 1.0000\n",
      "Test Batch number: 675, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 676, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 677, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 678, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 679, Test: Loss: 0.0972, Accuracy: 1.0000\n",
      "Test Batch number: 680, Test: Loss: 0.6427, Accuracy: 1.0000\n",
      "Test Batch number: 681, Test: Loss: 0.3905, Accuracy: 1.0000\n",
      "Test Batch number: 682, Test: Loss: 0.0016, Accuracy: 1.0000\n",
      "Test Batch number: 683, Test: Loss: 0.0014, Accuracy: 1.0000\n",
      "Test Batch number: 684, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 685, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 686, Test: Loss: 0.5262, Accuracy: 1.0000\n",
      "Test Batch number: 687, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 688, Test: Loss: 0.2947, Accuracy: 1.0000\n",
      "Test Batch number: 689, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 690, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 691, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 692, Test: Loss: 0.0571, Accuracy: 1.0000\n",
      "Test Batch number: 693, Test: Loss: 0.4385, Accuracy: 1.0000\n",
      "Test Batch number: 694, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 695, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 696, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 697, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 698, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 699, Test: Loss: 0.0037, Accuracy: 1.0000\n",
      "Test Batch number: 700, Test: Loss: 0.0661, Accuracy: 1.0000\n",
      "Test Batch number: 701, Test: Loss: 1.2394, Accuracy: 0.0000\n",
      "Test Batch number: 702, Test: Loss: 0.5827, Accuracy: 1.0000\n",
      "Test Batch number: 703, Test: Loss: 0.3704, Accuracy: 1.0000\n",
      "Test Batch number: 704, Test: Loss: 0.1212, Accuracy: 1.0000\n",
      "Test Batch number: 705, Test: Loss: 0.0016, Accuracy: 1.0000\n",
      "Test Batch number: 706, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 707, Test: Loss: 1.3539, Accuracy: 0.0000\n",
      "Test Batch number: 708, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 709, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 710, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 711, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 712, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 713, Test: Loss: 1.4648, Accuracy: 0.0000\n",
      "Test Batch number: 714, Test: Loss: 0.5734, Accuracy: 1.0000\n",
      "Test Batch number: 715, Test: Loss: 0.0012, Accuracy: 1.0000\n",
      "Test Batch number: 716, Test: Loss: 0.3299, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 717, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 718, Test: Loss: 0.7629, Accuracy: 0.0000\n",
      "Test Batch number: 719, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 720, Test: Loss: 0.1753, Accuracy: 1.0000\n",
      "Test Batch number: 721, Test: Loss: 0.3443, Accuracy: 1.0000\n",
      "Test Batch number: 722, Test: Loss: 0.3331, Accuracy: 1.0000\n",
      "Test Batch number: 723, Test: Loss: 0.0008, Accuracy: 1.0000\n",
      "Test Batch number: 724, Test: Loss: 0.1366, Accuracy: 1.0000\n",
      "Test Batch number: 725, Test: Loss: 0.6721, Accuracy: 1.0000\n",
      "Test Batch number: 726, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 727, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 728, Test: Loss: 0.0116, Accuracy: 1.0000\n",
      "Test Batch number: 729, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 730, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 731, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 732, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 733, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 734, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 735, Test: Loss: 0.3885, Accuracy: 1.0000\n",
      "Test Batch number: 736, Test: Loss: 0.2628, Accuracy: 1.0000\n",
      "Test Batch number: 737, Test: Loss: 1.1823, Accuracy: 0.0000\n",
      "Test Batch number: 738, Test: Loss: 0.7282, Accuracy: 0.0000\n",
      "Test Batch number: 739, Test: Loss: 1.4568, Accuracy: 0.0000\n",
      "Test Batch number: 740, Test: Loss: 0.0174, Accuracy: 1.0000\n",
      "Test Batch number: 741, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 742, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 743, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 744, Test: Loss: 1.3350, Accuracy: 0.0000\n",
      "Test Batch number: 745, Test: Loss: 0.9286, Accuracy: 0.0000\n",
      "Test Batch number: 746, Test: Loss: 0.0336, Accuracy: 1.0000\n",
      "Test Batch number: 747, Test: Loss: 1.6948, Accuracy: 0.0000\n",
      "Test Batch number: 748, Test: Loss: 0.1229, Accuracy: 1.0000\n",
      "Test Batch number: 749, Test: Loss: 0.1474, Accuracy: 1.0000\n",
      "Test Batch number: 750, Test: Loss: 0.0695, Accuracy: 1.0000\n",
      "Test Batch number: 751, Test: Loss: 1.1717, Accuracy: 0.0000\n",
      "Test Batch number: 752, Test: Loss: 0.7161, Accuracy: 0.0000\n",
      "Test Batch number: 753, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 754, Test: Loss: 0.4432, Accuracy: 1.0000\n",
      "Test Batch number: 755, Test: Loss: 0.0006, Accuracy: 1.0000\n",
      "Test Batch number: 756, Test: Loss: 0.0241, Accuracy: 1.0000\n",
      "Test Batch number: 757, Test: Loss: 0.3381, Accuracy: 1.0000\n",
      "Test Batch number: 758, Test: Loss: 0.0787, Accuracy: 1.0000\n",
      "Test Batch number: 759, Test: Loss: 0.2288, Accuracy: 1.0000\n",
      "Test Batch number: 760, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 761, Test: Loss: 0.0003, Accuracy: 1.0000\n",
      "Test Batch number: 762, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 763, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 764, Test: Loss: 0.0015, Accuracy: 1.0000\n",
      "Test Batch number: 765, Test: Loss: 0.6676, Accuracy: 1.0000\n",
      "Test Batch number: 766, Test: Loss: 0.3104, Accuracy: 1.0000\n",
      "Test Batch number: 767, Test: Loss: 0.0917, Accuracy: 1.0000\n",
      "Test Batch number: 768, Test: Loss: 0.2938, Accuracy: 1.0000\n",
      "Test Batch number: 769, Test: Loss: 0.7104, Accuracy: 0.0000\n",
      "Test Batch number: 770, Test: Loss: 1.3323, Accuracy: 0.0000\n",
      "Test Batch number: 771, Test: Loss: 0.0022, Accuracy: 1.0000\n",
      "Test Batch number: 772, Test: Loss: 0.4082, Accuracy: 1.0000\n",
      "Test Batch number: 773, Test: Loss: 0.0944, Accuracy: 1.0000\n",
      "Test Batch number: 774, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 775, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 776, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 777, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 778, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 779, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 780, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 781, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 782, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 783, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 784, Test: Loss: 0.3116, Accuracy: 1.0000\n",
      "Test Batch number: 785, Test: Loss: 0.3110, Accuracy: 1.0000\n",
      "Test Batch number: 786, Test: Loss: 0.0577, Accuracy: 1.0000\n",
      "Test Batch number: 787, Test: Loss: 0.0152, Accuracy: 1.0000\n",
      "Test Batch number: 788, Test: Loss: 0.2475, Accuracy: 1.0000\n",
      "Test Batch number: 789, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 790, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 791, Test: Loss: 0.6461, Accuracy: 1.0000\n",
      "Test Batch number: 792, Test: Loss: 0.7106, Accuracy: 0.0000\n",
      "Test Batch number: 793, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 794, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 795, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 796, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 797, Test: Loss: 0.4093, Accuracy: 1.0000\n",
      "Test Batch number: 798, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 799, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 800, Test: Loss: 0.0514, Accuracy: 1.0000\n",
      "Test Batch number: 801, Test: Loss: 0.0396, Accuracy: 1.0000\n",
      "Test Batch number: 802, Test: Loss: 0.3533, Accuracy: 1.0000\n",
      "Test Batch number: 803, Test: Loss: 0.6814, Accuracy: 1.0000\n",
      "Test Batch number: 804, Test: Loss: 0.0985, Accuracy: 1.0000\n",
      "Test Batch number: 805, Test: Loss: 1.2124, Accuracy: 0.0000\n",
      "Test Batch number: 806, Test: Loss: 0.0462, Accuracy: 1.0000\n",
      "Test Batch number: 807, Test: Loss: 0.5785, Accuracy: 1.0000\n",
      "Test Batch number: 808, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 809, Test: Loss: 0.0036, Accuracy: 1.0000\n",
      "Test Batch number: 810, Test: Loss: 0.0006, Accuracy: 1.0000\n",
      "Test Batch number: 811, Test: Loss: 0.0018, Accuracy: 1.0000\n",
      "Test Batch number: 812, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 813, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 814, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 815, Test: Loss: 0.7011, Accuracy: 0.0000\n",
      "Test Batch number: 816, Test: Loss: 0.0107, Accuracy: 1.0000\n",
      "Test Batch number: 817, Test: Loss: 0.2861, Accuracy: 1.0000\n",
      "Test Batch number: 818, Test: Loss: 0.5383, Accuracy: 1.0000\n",
      "Test Batch number: 819, Test: Loss: 0.6297, Accuracy: 1.0000\n",
      "Test Batch number: 820, Test: Loss: 0.2893, Accuracy: 1.0000\n",
      "Test Batch number: 821, Test: Loss: 0.2506, Accuracy: 1.0000\n",
      "Test Batch number: 822, Test: Loss: 0.0351, Accuracy: 1.0000\n",
      "Test Batch number: 823, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 824, Test: Loss: 0.2326, Accuracy: 1.0000\n",
      "Test Batch number: 825, Test: Loss: 0.1175, Accuracy: 1.0000\n",
      "Test Batch number: 826, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 827, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 828, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 829, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 830, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 831, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 832, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 833, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 834, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 835, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 836, Test: Loss: 0.7770, Accuracy: 0.0000\n",
      "Test Batch number: 837, Test: Loss: 0.5716, Accuracy: 1.0000\n",
      "Test Batch number: 838, Test: Loss: 0.5672, Accuracy: 1.0000\n",
      "Test Batch number: 839, Test: Loss: 0.7598, Accuracy: 0.0000\n",
      "Test Batch number: 840, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 841, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 842, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 843, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 844, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 845, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 846, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 847, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 848, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 849, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 850, Test: Loss: 0.4858, Accuracy: 1.0000\n",
      "Test Batch number: 851, Test: Loss: 0.0426, Accuracy: 1.0000\n",
      "Test Batch number: 852, Test: Loss: 0.4342, Accuracy: 1.0000\n",
      "Test Batch number: 853, Test: Loss: 0.6031, Accuracy: 1.0000\n",
      "Test Batch number: 854, Test: Loss: 0.7957, Accuracy: 0.0000\n",
      "Test Batch number: 855, Test: Loss: 0.4243, Accuracy: 1.0000\n",
      "Test Batch number: 856, Test: Loss: 0.4948, Accuracy: 1.0000\n",
      "Test Batch number: 857, Test: Loss: 0.0223, Accuracy: 1.0000\n",
      "Test Batch number: 858, Test: Loss: 0.0005, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 859, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 860, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 861, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 862, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 863, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 864, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 865, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 866, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 867, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 868, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 869, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 870, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 871, Test: Loss: 0.7070, Accuracy: 0.0000\n",
      "Test Batch number: 872, Test: Loss: 0.0181, Accuracy: 1.0000\n",
      "Test Batch number: 873, Test: Loss: 0.4399, Accuracy: 1.0000\n",
      "Test Batch number: 874, Test: Loss: 0.0020, Accuracy: 1.0000\n",
      "Test Batch number: 875, Test: Loss: 0.2199, Accuracy: 1.0000\n",
      "Test Batch number: 876, Test: Loss: 0.7262, Accuracy: 0.0000\n",
      "Test Batch number: 877, Test: Loss: 0.0020, Accuracy: 1.0000\n",
      "Test Batch number: 878, Test: Loss: 0.0040, Accuracy: 1.0000\n",
      "Test Batch number: 879, Test: Loss: 0.4406, Accuracy: 1.0000\n",
      "Test Batch number: 880, Test: Loss: 0.2483, Accuracy: 1.0000\n",
      "Test Batch number: 881, Test: Loss: 0.3708, Accuracy: 1.0000\n",
      "Test Batch number: 882, Test: Loss: 0.2571, Accuracy: 1.0000\n",
      "Test Batch number: 883, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 884, Test: Loss: 0.6349, Accuracy: 1.0000\n",
      "Test Batch number: 885, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 886, Test: Loss: 0.7890, Accuracy: 0.0000\n",
      "Test Batch number: 887, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 888, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 889, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 890, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 891, Test: Loss: 0.3120, Accuracy: 1.0000\n",
      "Test Batch number: 892, Test: Loss: 1.5165, Accuracy: 0.0000\n",
      "Test Batch number: 893, Test: Loss: 0.0003, Accuracy: 1.0000\n",
      "Test Batch number: 894, Test: Loss: 0.0618, Accuracy: 1.0000\n",
      "Test Batch number: 895, Test: Loss: 1.4668, Accuracy: 0.0000\n",
      "Test Batch number: 896, Test: Loss: 0.6103, Accuracy: 1.0000\n",
      "Test Batch number: 897, Test: Loss: 0.1754, Accuracy: 1.0000\n",
      "Test Batch number: 898, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 899, Test: Loss: 0.0485, Accuracy: 1.0000\n",
      "Test Batch number: 900, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 901, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 902, Test: Loss: 0.0047, Accuracy: 1.0000\n",
      "Test Batch number: 903, Test: Loss: 0.6644, Accuracy: 1.0000\n",
      "Test Batch number: 904, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 905, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 906, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 907, Test: Loss: 0.4814, Accuracy: 1.0000\n",
      "Test Batch number: 908, Test: Loss: 0.0035, Accuracy: 1.0000\n",
      "Test Batch number: 909, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 910, Test: Loss: 1.0753, Accuracy: 0.0000\n",
      "Test Batch number: 911, Test: Loss: 0.1437, Accuracy: 1.0000\n",
      "Test Batch number: 912, Test: Loss: 0.2001, Accuracy: 1.0000\n",
      "Test Batch number: 913, Test: Loss: 0.7644, Accuracy: 0.0000\n",
      "Test Batch number: 914, Test: Loss: 0.0179, Accuracy: 1.0000\n",
      "Test Batch number: 915, Test: Loss: 0.2667, Accuracy: 1.0000\n",
      "Test Batch number: 916, Test: Loss: 0.4137, Accuracy: 1.0000\n",
      "Test Batch number: 917, Test: Loss: 0.3263, Accuracy: 1.0000\n",
      "Test Batch number: 918, Test: Loss: 1.4163, Accuracy: 0.0000\n",
      "Test Batch number: 919, Test: Loss: 0.3430, Accuracy: 1.0000\n",
      "Test Batch number: 920, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 921, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 922, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 923, Test: Loss: 0.3094, Accuracy: 1.0000\n",
      "Test Batch number: 924, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 925, Test: Loss: 0.6770, Accuracy: 1.0000\n",
      "Test Batch number: 926, Test: Loss: 0.0067, Accuracy: 1.0000\n",
      "Test Batch number: 927, Test: Loss: 0.1301, Accuracy: 1.0000\n",
      "Test Batch number: 928, Test: Loss: 0.0009, Accuracy: 1.0000\n",
      "Test Batch number: 929, Test: Loss: 0.2027, Accuracy: 1.0000\n",
      "Test Batch number: 930, Test: Loss: 1.2658, Accuracy: 0.0000\n",
      "Test Batch number: 931, Test: Loss: 0.5481, Accuracy: 1.0000\n",
      "Test Batch number: 932, Test: Loss: 0.4818, Accuracy: 1.0000\n",
      "Test Batch number: 933, Test: Loss: 0.7386, Accuracy: 0.0000\n",
      "Test Batch number: 934, Test: Loss: 0.3285, Accuracy: 1.0000\n",
      "Test Batch number: 935, Test: Loss: 0.4566, Accuracy: 1.0000\n",
      "Test Batch number: 936, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 937, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 938, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 939, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 940, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 941, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 942, Test: Loss: 0.0216, Accuracy: 1.0000\n",
      "Test Batch number: 943, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 944, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 945, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 946, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 947, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 948, Test: Loss: 0.4992, Accuracy: 1.0000\n",
      "Test Batch number: 949, Test: Loss: 0.3068, Accuracy: 1.0000\n",
      "Test Batch number: 950, Test: Loss: 0.1093, Accuracy: 1.0000\n",
      "Test Batch number: 951, Test: Loss: 0.7898, Accuracy: 0.0000\n",
      "Test Batch number: 952, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 953, Test: Loss: 0.0120, Accuracy: 1.0000\n",
      "Test Batch number: 954, Test: Loss: 0.3266, Accuracy: 1.0000\n",
      "Test Batch number: 955, Test: Loss: 0.4917, Accuracy: 1.0000\n",
      "Test Batch number: 956, Test: Loss: 0.0209, Accuracy: 1.0000\n",
      "Test Batch number: 957, Test: Loss: 1.4630, Accuracy: 0.0000\n",
      "Test Batch number: 958, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 959, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 960, Test: Loss: 0.1855, Accuracy: 1.0000\n",
      "Test Batch number: 961, Test: Loss: 0.0094, Accuracy: 1.0000\n",
      "Test Batch number: 962, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 963, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 964, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 965, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 966, Test: Loss: 0.4786, Accuracy: 1.0000\n",
      "Test Batch number: 967, Test: Loss: 0.0235, Accuracy: 1.0000\n",
      "Test Batch number: 968, Test: Loss: 0.3724, Accuracy: 1.0000\n",
      "Test Batch number: 969, Test: Loss: 0.1701, Accuracy: 1.0000\n",
      "Test Batch number: 970, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 971, Test: Loss: 0.0020, Accuracy: 1.0000\n",
      "Test Batch number: 972, Test: Loss: 0.0223, Accuracy: 1.0000\n",
      "Test Batch number: 973, Test: Loss: 0.6714, Accuracy: 1.0000\n",
      "Test Batch number: 974, Test: Loss: 1.0860, Accuracy: 0.0000\n",
      "Test Batch number: 975, Test: Loss: 0.3637, Accuracy: 1.0000\n",
      "Test Batch number: 976, Test: Loss: 0.0563, Accuracy: 1.0000\n",
      "Test Batch number: 977, Test: Loss: 1.1207, Accuracy: 0.0000\n",
      "Test Batch number: 978, Test: Loss: 0.2359, Accuracy: 1.0000\n",
      "Test Batch number: 979, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 980, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 981, Test: Loss: 0.6355, Accuracy: 1.0000\n",
      "Test Batch number: 982, Test: Loss: 0.1657, Accuracy: 1.0000\n",
      "Test Batch number: 983, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 984, Test: Loss: 0.4453, Accuracy: 1.0000\n",
      "Test Batch number: 985, Test: Loss: 0.1857, Accuracy: 1.0000\n",
      "Test Batch number: 986, Test: Loss: 0.5514, Accuracy: 1.0000\n",
      "Test Batch number: 987, Test: Loss: 0.2324, Accuracy: 1.0000\n",
      "Test Batch number: 988, Test: Loss: 0.1664, Accuracy: 1.0000\n",
      "Test Batch number: 989, Test: Loss: 0.1106, Accuracy: 1.0000\n",
      "Test Batch number: 990, Test: Loss: 0.0994, Accuracy: 1.0000\n",
      "Test Batch number: 991, Test: Loss: 0.9863, Accuracy: 0.0000\n",
      "Test Batch number: 992, Test: Loss: 0.2951, Accuracy: 1.0000\n",
      "Test Batch number: 993, Test: Loss: 0.2858, Accuracy: 1.0000\n",
      "Test Batch number: 994, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 995, Test: Loss: 0.1225, Accuracy: 1.0000\n",
      "Test Batch number: 996, Test: Loss: 0.4610, Accuracy: 1.0000\n",
      "Test Batch number: 997, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 998, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 999, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1000, Test: Loss: 0.3261, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 1001, Test: Loss: 0.4085, Accuracy: 1.0000\n",
      "Test Batch number: 1002, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1003, Test: Loss: 0.0204, Accuracy: 1.0000\n",
      "Test Batch number: 1004, Test: Loss: 0.0460, Accuracy: 1.0000\n",
      "Test Batch number: 1005, Test: Loss: 0.2844, Accuracy: 1.0000\n",
      "Test Batch number: 1006, Test: Loss: 0.2468, Accuracy: 1.0000\n",
      "Test Batch number: 1007, Test: Loss: 0.1259, Accuracy: 1.0000\n",
      "Test Batch number: 1008, Test: Loss: 0.1884, Accuracy: 1.0000\n",
      "Test Batch number: 1009, Test: Loss: 0.2416, Accuracy: 1.0000\n",
      "Test Batch number: 1010, Test: Loss: 0.4629, Accuracy: 1.0000\n",
      "Test Batch number: 1011, Test: Loss: 0.0305, Accuracy: 1.0000\n",
      "Test Batch number: 1012, Test: Loss: 0.1541, Accuracy: 1.0000\n",
      "Test Batch number: 1013, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 1014, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1015, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1016, Test: Loss: 0.4261, Accuracy: 1.0000\n",
      "Test Batch number: 1017, Test: Loss: 0.7276, Accuracy: 0.0000\n",
      "Test Batch number: 1018, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1019, Test: Loss: 0.0370, Accuracy: 1.0000\n",
      "Test Batch number: 1020, Test: Loss: 0.4049, Accuracy: 1.0000\n",
      "Test Batch number: 1021, Test: Loss: 0.2798, Accuracy: 1.0000\n",
      "Test Batch number: 1022, Test: Loss: 0.1955, Accuracy: 1.0000\n",
      "Test Batch number: 1023, Test: Loss: 0.2486, Accuracy: 1.0000\n",
      "Test Batch number: 1024, Test: Loss: 0.3202, Accuracy: 1.0000\n",
      "Test Batch number: 1025, Test: Loss: 0.3924, Accuracy: 1.0000\n",
      "Test Batch number: 1026, Test: Loss: 0.0671, Accuracy: 1.0000\n",
      "Test Batch number: 1027, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1028, Test: Loss: 0.0140, Accuracy: 1.0000\n",
      "Test Batch number: 1029, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1030, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1031, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1032, Test: Loss: 2.6623, Accuracy: 0.0000\n",
      "Test Batch number: 1033, Test: Loss: 0.5891, Accuracy: 1.0000\n",
      "Test Batch number: 1034, Test: Loss: 0.5694, Accuracy: 1.0000\n",
      "Test Batch number: 1035, Test: Loss: 0.4272, Accuracy: 1.0000\n",
      "Test Batch number: 1036, Test: Loss: 0.2803, Accuracy: 1.0000\n",
      "Test Batch number: 1037, Test: Loss: 0.0692, Accuracy: 1.0000\n",
      "Test Batch number: 1038, Test: Loss: 0.2996, Accuracy: 1.0000\n",
      "Test Batch number: 1039, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1040, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1041, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1042, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 1043, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1044, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1045, Test: Loss: 0.0019, Accuracy: 1.0000\n",
      "Test Batch number: 1046, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1047, Test: Loss: 0.0389, Accuracy: 1.0000\n",
      "Test Batch number: 1048, Test: Loss: 0.3733, Accuracy: 1.0000\n",
      "Test Batch number: 1049, Test: Loss: 0.1346, Accuracy: 1.0000\n",
      "Test Batch number: 1050, Test: Loss: 0.3528, Accuracy: 1.0000\n",
      "Test Batch number: 1051, Test: Loss: 0.0081, Accuracy: 1.0000\n",
      "Test Batch number: 1052, Test: Loss: 0.1871, Accuracy: 1.0000\n",
      "Test Batch number: 1053, Test: Loss: 0.6512, Accuracy: 1.0000\n",
      "Test Batch number: 1054, Test: Loss: 0.1007, Accuracy: 1.0000\n",
      "Test Batch number: 1055, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1056, Test: Loss: 0.1242, Accuracy: 1.0000\n",
      "Test Batch number: 1057, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1058, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1059, Test: Loss: 0.9888, Accuracy: 0.0000\n",
      "Test Batch number: 1060, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1061, Test: Loss: 0.0523, Accuracy: 1.0000\n",
      "Test Batch number: 1062, Test: Loss: 0.4693, Accuracy: 1.0000\n",
      "Test Batch number: 1063, Test: Loss: 0.0820, Accuracy: 1.0000\n",
      "Test Batch number: 1064, Test: Loss: 1.4420, Accuracy: 0.0000\n",
      "Test Batch number: 1065, Test: Loss: 0.3375, Accuracy: 1.0000\n",
      "Test Batch number: 1066, Test: Loss: 0.0415, Accuracy: 1.0000\n",
      "Test Batch number: 1067, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1068, Test: Loss: 0.0003, Accuracy: 1.0000\n",
      "Test Batch number: 1069, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1070, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1071, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1072, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1073, Test: Loss: 0.2190, Accuracy: 1.0000\n",
      "Test Batch number: 1074, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1075, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1076, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1077, Test: Loss: 0.0047, Accuracy: 1.0000\n",
      "Test Batch number: 1078, Test: Loss: 0.4065, Accuracy: 1.0000\n",
      "Test Batch number: 1079, Test: Loss: 0.0777, Accuracy: 1.0000\n",
      "Test Batch number: 1080, Test: Loss: 0.5289, Accuracy: 1.0000\n",
      "Test Batch number: 1081, Test: Loss: 0.4857, Accuracy: 1.0000\n",
      "Test Batch number: 1082, Test: Loss: 0.3168, Accuracy: 1.0000\n",
      "Test Batch number: 1083, Test: Loss: 0.2495, Accuracy: 1.0000\n",
      "Test Batch number: 1084, Test: Loss: 0.2149, Accuracy: 1.0000\n",
      "Test Batch number: 1085, Test: Loss: 0.3245, Accuracy: 1.0000\n",
      "Test Batch number: 1086, Test: Loss: 0.2147, Accuracy: 1.0000\n",
      "Test Batch number: 1087, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 1088, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1089, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1090, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1091, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1092, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1093, Test: Loss: 0.5878, Accuracy: 1.0000\n",
      "Test Batch number: 1094, Test: Loss: 0.0443, Accuracy: 1.0000\n",
      "Test Batch number: 1095, Test: Loss: 0.0003, Accuracy: 1.0000\n",
      "Test Batch number: 1096, Test: Loss: 0.2320, Accuracy: 1.0000\n",
      "Test Batch number: 1097, Test: Loss: 0.4878, Accuracy: 1.0000\n",
      "Test Batch number: 1098, Test: Loss: 0.2795, Accuracy: 1.0000\n",
      "Test Batch number: 1099, Test: Loss: 0.1710, Accuracy: 1.0000\n",
      "Test Batch number: 1100, Test: Loss: 1.4648, Accuracy: 0.0000\n",
      "Test Batch number: 1101, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 1102, Test: Loss: 1.2234, Accuracy: 0.0000\n",
      "Test Batch number: 1103, Test: Loss: 0.3129, Accuracy: 1.0000\n",
      "Test Batch number: 1104, Test: Loss: 0.2463, Accuracy: 1.0000\n",
      "Test Batch number: 1105, Test: Loss: 0.4000, Accuracy: 1.0000\n",
      "Test Batch number: 1106, Test: Loss: 0.1849, Accuracy: 1.0000\n",
      "Test Batch number: 1107, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1108, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1109, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1110, Test: Loss: 0.7907, Accuracy: 0.0000\n",
      "Test Batch number: 1111, Test: Loss: 0.1503, Accuracy: 1.0000\n",
      "Test Batch number: 1112, Test: Loss: 0.3220, Accuracy: 1.0000\n",
      "Test Batch number: 1113, Test: Loss: 0.5643, Accuracy: 1.0000\n",
      "Test Batch number: 1114, Test: Loss: 0.1979, Accuracy: 1.0000\n",
      "Test Batch number: 1115, Test: Loss: 0.3820, Accuracy: 1.0000\n",
      "Test Batch number: 1116, Test: Loss: 0.9066, Accuracy: 0.0000\n",
      "Test Batch number: 1117, Test: Loss: 0.1355, Accuracy: 1.0000\n",
      "Test Batch number: 1118, Test: Loss: 0.1936, Accuracy: 1.0000\n",
      "Test Batch number: 1119, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1120, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1121, Test: Loss: 0.2551, Accuracy: 1.0000\n",
      "Test Batch number: 1122, Test: Loss: 0.1127, Accuracy: 1.0000\n",
      "Test Batch number: 1123, Test: Loss: 0.6744, Accuracy: 1.0000\n",
      "Test Batch number: 1124, Test: Loss: 0.4387, Accuracy: 1.0000\n",
      "Test Batch number: 1125, Test: Loss: 0.4707, Accuracy: 1.0000\n",
      "Test Batch number: 1126, Test: Loss: 0.8548, Accuracy: 0.0000\n",
      "Test Batch number: 1127, Test: Loss: 0.5249, Accuracy: 1.0000\n",
      "Test Batch number: 1128, Test: Loss: 0.3989, Accuracy: 1.0000\n",
      "Test Batch number: 1129, Test: Loss: 1.1981, Accuracy: 0.0000\n",
      "Test Batch number: 1130, Test: Loss: 0.0194, Accuracy: 1.0000\n",
      "Test Batch number: 1131, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1132, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 1133, Test: Loss: 0.3137, Accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 1134, Test: Loss: 1.4104, Accuracy: 0.0000\n",
      "Test Batch number: 1135, Test: Loss: 0.4102, Accuracy: 1.0000\n",
      "Test Batch number: 1136, Test: Loss: 0.4598, Accuracy: 1.0000\n",
      "Test Batch number: 1137, Test: Loss: 0.1612, Accuracy: 1.0000\n",
      "Test Batch number: 1138, Test: Loss: 0.2626, Accuracy: 1.0000\n",
      "Test Batch number: 1139, Test: Loss: 0.4692, Accuracy: 1.0000\n",
      "Test Batch number: 1140, Test: Loss: 0.2015, Accuracy: 1.0000\n",
      "Test Batch number: 1141, Test: Loss: 0.3971, Accuracy: 1.0000\n",
      "Test Batch number: 1142, Test: Loss: 0.1964, Accuracy: 1.0000\n",
      "Test Batch number: 1143, Test: Loss: 0.5423, Accuracy: 1.0000\n",
      "Test Batch number: 1144, Test: Loss: 0.3255, Accuracy: 1.0000\n",
      "Test Batch number: 1145, Test: Loss: 0.0956, Accuracy: 1.0000\n",
      "Test Batch number: 1146, Test: Loss: 1.2918, Accuracy: 0.0000\n",
      "Test Batch number: 1147, Test: Loss: 0.0006, Accuracy: 1.0000\n",
      "Test Batch number: 1148, Test: Loss: 0.5286, Accuracy: 1.0000\n",
      "Test Batch number: 1149, Test: Loss: 0.5051, Accuracy: 1.0000\n",
      "Test Batch number: 1150, Test: Loss: 0.2462, Accuracy: 1.0000\n",
      "Test Batch number: 1151, Test: Loss: 0.3002, Accuracy: 1.0000\n",
      "Test Batch number: 1152, Test: Loss: 0.2195, Accuracy: 1.0000\n",
      "Test Batch number: 1153, Test: Loss: 0.4204, Accuracy: 1.0000\n",
      "Test Batch number: 1154, Test: Loss: 0.4624, Accuracy: 1.0000\n",
      "Test Batch number: 1155, Test: Loss: 0.2978, Accuracy: 1.0000\n",
      "Test Batch number: 1156, Test: Loss: 0.3935, Accuracy: 1.0000\n",
      "Test Batch number: 1157, Test: Loss: 0.7171, Accuracy: 0.0000\n",
      "Test Batch number: 1158, Test: Loss: 0.1579, Accuracy: 1.0000\n",
      "Test Batch number: 1159, Test: Loss: 0.3117, Accuracy: 1.0000\n",
      "Test Batch number: 1160, Test: Loss: 0.1755, Accuracy: 1.0000\n",
      "Test Batch number: 1161, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 1162, Test: Loss: 0.9098, Accuracy: 0.0000\n",
      "Test Batch number: 1163, Test: Loss: 0.5543, Accuracy: 1.0000\n",
      "Test Batch number: 1164, Test: Loss: 0.1819, Accuracy: 1.0000\n",
      "Test Batch number: 1165, Test: Loss: 0.2609, Accuracy: 1.0000\n",
      "Test Batch number: 1166, Test: Loss: 0.4469, Accuracy: 1.0000\n",
      "Test Batch number: 1167, Test: Loss: 0.3069, Accuracy: 1.0000\n",
      "Test Batch number: 1168, Test: Loss: 0.0463, Accuracy: 1.0000\n",
      "Test Batch number: 1169, Test: Loss: 0.5365, Accuracy: 1.0000\n",
      "Test Batch number: 1170, Test: Loss: 0.2831, Accuracy: 1.0000\n",
      "Test Batch number: 1171, Test: Loss: 0.4016, Accuracy: 1.0000\n",
      "Test Batch number: 1172, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 1173, Test: Loss: 0.8159, Accuracy: 0.0000\n",
      "Test Batch number: 1174, Test: Loss: 0.0311, Accuracy: 1.0000\n",
      "Test Batch number: 1175, Test: Loss: 0.3899, Accuracy: 1.0000\n",
      "Test Batch number: 1176, Test: Loss: 0.0506, Accuracy: 1.0000\n",
      "Test Batch number: 1177, Test: Loss: 0.3505, Accuracy: 1.0000\n",
      "Test Batch number: 1178, Test: Loss: 0.3333, Accuracy: 1.0000\n",
      "Test Batch number: 1179, Test: Loss: 0.3860, Accuracy: 1.0000\n",
      "Test Batch number: 1180, Test: Loss: 1.2277, Accuracy: 0.0000\n",
      "Test Batch number: 1181, Test: Loss: 0.0559, Accuracy: 1.0000\n",
      "Test Batch number: 1182, Test: Loss: 0.0022, Accuracy: 1.0000\n",
      "Test Batch number: 1183, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1184, Test: Loss: 0.3447, Accuracy: 1.0000\n",
      "Test Batch number: 1185, Test: Loss: 0.4463, Accuracy: 1.0000\n",
      "Test Batch number: 1186, Test: Loss: 0.3323, Accuracy: 1.0000\n",
      "Test Batch number: 1187, Test: Loss: 0.4232, Accuracy: 1.0000\n",
      "Test Batch number: 1188, Test: Loss: 0.4585, Accuracy: 1.0000\n",
      "Test Batch number: 1189, Test: Loss: 0.8348, Accuracy: 0.0000\n",
      "Test Batch number: 1190, Test: Loss: 0.0021, Accuracy: 1.0000\n",
      "Test Batch number: 1191, Test: Loss: 0.0002, Accuracy: 1.0000\n",
      "Test Batch number: 1192, Test: Loss: 0.0207, Accuracy: 1.0000\n",
      "Test Batch number: 1193, Test: Loss: 0.4061, Accuracy: 1.0000\n",
      "Test Batch number: 1194, Test: Loss: 0.1896, Accuracy: 1.0000\n",
      "Test Batch number: 1195, Test: Loss: 0.3848, Accuracy: 1.0000\n",
      "Test Batch number: 1196, Test: Loss: 0.7223, Accuracy: 0.0000\n",
      "Test Batch number: 1197, Test: Loss: 0.0281, Accuracy: 1.0000\n",
      "Test Batch number: 1198, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1199, Test: Loss: 0.0077, Accuracy: 1.0000\n",
      "Test Batch number: 1200, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 1201, Test: Loss: 1.0810, Accuracy: 0.0000\n",
      "Test Batch number: 1202, Test: Loss: 0.4336, Accuracy: 1.0000\n",
      "Test Batch number: 1203, Test: Loss: 0.2302, Accuracy: 1.0000\n",
      "Test Batch number: 1204, Test: Loss: 1.9814, Accuracy: 0.0000\n",
      "Test Batch number: 1205, Test: Loss: 0.5949, Accuracy: 1.0000\n",
      "Test Batch number: 1206, Test: Loss: 0.3796, Accuracy: 1.0000\n",
      "Test Batch number: 1207, Test: Loss: 0.0012, Accuracy: 1.0000\n",
      "Test Batch number: 1208, Test: Loss: 0.0027, Accuracy: 1.0000\n",
      "Test Batch number: 1209, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 1210, Test: Loss: 0.0074, Accuracy: 1.0000\n",
      "Test Batch number: 1211, Test: Loss: 0.3180, Accuracy: 1.0000\n",
      "Test Batch number: 1212, Test: Loss: 0.1927, Accuracy: 1.0000\n",
      "Test Batch number: 1213, Test: Loss: 0.4237, Accuracy: 1.0000\n",
      "Test Batch number: 1214, Test: Loss: 0.1103, Accuracy: 1.0000\n",
      "Test Batch number: 1215, Test: Loss: 0.1009, Accuracy: 1.0000\n",
      "Test Batch number: 1216, Test: Loss: 0.2948, Accuracy: 1.0000\n",
      "Test Batch number: 1217, Test: Loss: 0.2660, Accuracy: 1.0000\n",
      "Test Batch number: 1218, Test: Loss: 0.1043, Accuracy: 1.0000\n",
      "Test Batch number: 1219, Test: Loss: 0.4549, Accuracy: 1.0000\n",
      "Test Batch number: 1220, Test: Loss: 0.5940, Accuracy: 1.0000\n",
      "Test Batch number: 1221, Test: Loss: 0.5683, Accuracy: 1.0000\n",
      "Test Batch number: 1222, Test: Loss: 0.6786, Accuracy: 1.0000\n",
      "Test Batch number: 1223, Test: Loss: 0.1982, Accuracy: 1.0000\n",
      "Test Batch number: 1224, Test: Loss: 0.0005, Accuracy: 1.0000\n",
      "Test Batch number: 1225, Test: Loss: 0.5153, Accuracy: 1.0000\n",
      "Test Batch number: 1226, Test: Loss: 0.2517, Accuracy: 1.0000\n",
      "Test Batch number: 1227, Test: Loss: 0.6056, Accuracy: 1.0000\n",
      "Test Batch number: 1228, Test: Loss: 0.7599, Accuracy: 0.0000\n",
      "Test Batch number: 1229, Test: Loss: 0.2464, Accuracy: 1.0000\n",
      "Test Batch number: 1230, Test: Loss: 0.8230, Accuracy: 0.0000\n",
      "Test Batch number: 1231, Test: Loss: 0.5902, Accuracy: 1.0000\n",
      "Test Batch number: 1232, Test: Loss: 0.0031, Accuracy: 1.0000\n",
      "Test Batch number: 1233, Test: Loss: 0.4776, Accuracy: 1.0000\n",
      "Test Batch number: 1234, Test: Loss: 0.6316, Accuracy: 1.0000\n",
      "Test Batch number: 1235, Test: Loss: 0.3833, Accuracy: 1.0000\n",
      "Test Batch number: 1236, Test: Loss: 0.1798, Accuracy: 1.0000\n",
      "Test Batch number: 1237, Test: Loss: 0.0705, Accuracy: 1.0000\n",
      "Test Batch number: 1238, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1239, Test: Loss: 0.7079, Accuracy: 0.0000\n",
      "Test Batch number: 1240, Test: Loss: 0.5987, Accuracy: 1.0000\n",
      "Test Batch number: 1241, Test: Loss: 0.3129, Accuracy: 1.0000\n",
      "Test Batch number: 1242, Test: Loss: 0.1073, Accuracy: 1.0000\n",
      "Test Batch number: 1243, Test: Loss: 0.4682, Accuracy: 1.0000\n",
      "Test Batch number: 1244, Test: Loss: 0.1280, Accuracy: 1.0000\n",
      "Test Batch number: 1245, Test: Loss: 0.4258, Accuracy: 1.0000\n",
      "Test Batch number: 1246, Test: Loss: 0.1053, Accuracy: 1.0000\n",
      "Test Batch number: 1247, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1248, Test: Loss: 0.2640, Accuracy: 1.0000\n",
      "Test Batch number: 1249, Test: Loss: 0.2877, Accuracy: 1.0000\n",
      "Test Batch number: 1250, Test: Loss: 0.0336, Accuracy: 1.0000\n",
      "Test Batch number: 1251, Test: Loss: 1.5557, Accuracy: 0.0000\n",
      "Test Batch number: 1252, Test: Loss: 0.3906, Accuracy: 1.0000\n",
      "Test Batch number: 1253, Test: Loss: 0.0575, Accuracy: 1.0000\n",
      "Test Batch number: 1254, Test: Loss: 0.6139, Accuracy: 1.0000\n",
      "Test Batch number: 1255, Test: Loss: 0.8046, Accuracy: 0.0000\n",
      "Test Batch number: 1256, Test: Loss: 0.2289, Accuracy: 1.0000\n",
      "Test Batch number: 1257, Test: Loss: 0.3787, Accuracy: 1.0000\n",
      "Test Batch number: 1258, Test: Loss: 0.0026, Accuracy: 1.0000\n",
      "Test Batch number: 1259, Test: Loss: 1.1717, Accuracy: 0.0000\n",
      "Test Batch number: 1260, Test: Loss: 1.3970, Accuracy: 0.0000\n",
      "Test Batch number: 1261, Test: Loss: 0.1047, Accuracy: 1.0000\n",
      "Test Batch number: 1262, Test: Loss: 0.4590, Accuracy: 1.0000\n",
      "Test Batch number: 1263, Test: Loss: 0.2529, Accuracy: 1.0000\n",
      "Test Batch number: 1264, Test: Loss: 0.3051, Accuracy: 1.0000\n",
      "Test Batch number: 1265, Test: Loss: 0.6582, Accuracy: 1.0000\n",
      "Test Batch number: 1266, Test: Loss: 0.5294, Accuracy: 1.0000\n",
      "Test Batch number: 1267, Test: Loss: 0.7610, Accuracy: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Batch number: 1268, Test: Loss: 0.0004, Accuracy: 1.0000\n",
      "Test Batch number: 1269, Test: Loss: 0.4235, Accuracy: 1.0000\n",
      "Test Batch number: 1270, Test: Loss: 0.7016, Accuracy: 0.0000\n",
      "Test Batch number: 1271, Test: Loss: 0.3910, Accuracy: 1.0000\n",
      "Test Batch number: 1272, Test: Loss: 0.2411, Accuracy: 1.0000\n",
      "Test Batch number: 1273, Test: Loss: 1.3898, Accuracy: 0.0000\n",
      "Test Batch number: 1274, Test: Loss: 0.9386, Accuracy: 0.0000\n",
      "Test Batch number: 1275, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1276, Test: Loss: 0.3979, Accuracy: 1.0000\n",
      "Test Batch number: 1277, Test: Loss: 0.1740, Accuracy: 1.0000\n",
      "Test Batch number: 1278, Test: Loss: 0.4668, Accuracy: 1.0000\n",
      "Test Batch number: 1279, Test: Loss: 0.0050, Accuracy: 1.0000\n",
      "Test Batch number: 1280, Test: Loss: 0.4027, Accuracy: 1.0000\n",
      "Test Batch number: 1281, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1282, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1283, Test: Loss: 0.3585, Accuracy: 1.0000\n",
      "Test Batch number: 1284, Test: Loss: 0.3431, Accuracy: 1.0000\n",
      "Test Batch number: 1285, Test: Loss: 0.5850, Accuracy: 1.0000\n",
      "Test Batch number: 1286, Test: Loss: 0.6061, Accuracy: 1.0000\n",
      "Test Batch number: 1287, Test: Loss: 0.3411, Accuracy: 1.0000\n",
      "Test Batch number: 1288, Test: Loss: 0.7697, Accuracy: 0.0000\n",
      "Test Batch number: 1289, Test: Loss: 0.9218, Accuracy: 0.0000\n",
      "Test Batch number: 1290, Test: Loss: 1.1731, Accuracy: 0.0000\n",
      "Test Batch number: 1291, Test: Loss: 0.6800, Accuracy: 1.0000\n",
      "Test Batch number: 1292, Test: Loss: 0.0283, Accuracy: 1.0000\n",
      "Test Batch number: 1293, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1294, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1295, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1296, Test: Loss: 0.3327, Accuracy: 1.0000\n",
      "Test Batch number: 1297, Test: Loss: 0.8535, Accuracy: 0.0000\n",
      "Test Batch number: 1298, Test: Loss: 0.7213, Accuracy: 0.0000\n",
      "Test Batch number: 1299, Test: Loss: 0.4517, Accuracy: 1.0000\n",
      "Test Batch number: 1300, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1301, Test: Loss: 1.0609, Accuracy: 0.0000\n",
      "Test Batch number: 1302, Test: Loss: 0.3076, Accuracy: 1.0000\n",
      "Test Batch number: 1303, Test: Loss: 0.7105, Accuracy: 0.0000\n",
      "Test Batch number: 1304, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1305, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1306, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1307, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1308, Test: Loss: 0.3373, Accuracy: 1.0000\n",
      "Test Batch number: 1309, Test: Loss: 1.4328, Accuracy: 0.0000\n",
      "Test Batch number: 1310, Test: Loss: 0.6002, Accuracy: 1.0000\n",
      "Test Batch number: 1311, Test: Loss: 0.0046, Accuracy: 1.0000\n",
      "Test Batch number: 1312, Test: Loss: 0.4129, Accuracy: 1.0000\n",
      "Test Batch number: 1313, Test: Loss: 0.5531, Accuracy: 1.0000\n",
      "Test Batch number: 1314, Test: Loss: 3.3378, Accuracy: 0.0000\n",
      "Test Batch number: 1315, Test: Loss: 8.0278, Accuracy: 0.0000\n",
      "Test Batch number: 1316, Test: Loss: 0.0959, Accuracy: 1.0000\n",
      "Test Batch number: 1317, Test: Loss: 1.8155, Accuracy: 0.0000\n",
      "Test Batch number: 1318, Test: Loss: 0.0402, Accuracy: 1.0000\n",
      "Test Batch number: 1319, Test: Loss: 0.4330, Accuracy: 1.0000\n",
      "Test Batch number: 1320, Test: Loss: 0.3036, Accuracy: 1.0000\n",
      "Test Batch number: 1321, Test: Loss: 0.0451, Accuracy: 1.0000\n",
      "Test Batch number: 1322, Test: Loss: 0.0888, Accuracy: 1.0000\n",
      "Test Batch number: 1323, Test: Loss: 0.0099, Accuracy: 1.0000\n",
      "Test Batch number: 1324, Test: Loss: 0.0595, Accuracy: 1.0000\n",
      "Test Batch number: 1325, Test: Loss: 0.0212, Accuracy: 1.0000\n",
      "Test Batch number: 1326, Test: Loss: 0.0612, Accuracy: 1.0000\n",
      "Test Batch number: 1327, Test: Loss: 0.3848, Accuracy: 1.0000\n",
      "Test Batch number: 1328, Test: Loss: 0.0925, Accuracy: 1.0000\n",
      "Test Batch number: 1329, Test: Loss: 1.2459, Accuracy: 0.0000\n",
      "Test Batch number: 1330, Test: Loss: 0.5290, Accuracy: 1.0000\n",
      "Test Batch number: 1331, Test: Loss: 0.1211, Accuracy: 1.0000\n",
      "Test Batch number: 1332, Test: Loss: 0.3253, Accuracy: 1.0000\n",
      "Test Batch number: 1333, Test: Loss: 0.0338, Accuracy: 1.0000\n",
      "Test Batch number: 1334, Test: Loss: 0.0464, Accuracy: 1.0000\n",
      "Test Batch number: 1335, Test: Loss: 0.6330, Accuracy: 1.0000\n",
      "Test Batch number: 1336, Test: Loss: 0.0436, Accuracy: 1.0000\n",
      "Test Batch number: 1337, Test: Loss: 0.8494, Accuracy: 0.0000\n",
      "Test Batch number: 1338, Test: Loss: 0.0691, Accuracy: 1.0000\n",
      "Test Batch number: 1339, Test: Loss: 0.5264, Accuracy: 1.0000\n",
      "Test Batch number: 1340, Test: Loss: 0.0807, Accuracy: 1.0000\n",
      "Test Batch number: 1341, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1342, Test: Loss: 0.8518, Accuracy: 0.0000\n",
      "Test Batch number: 1343, Test: Loss: 0.0212, Accuracy: 1.0000\n",
      "Test Batch number: 1344, Test: Loss: 0.6453, Accuracy: 1.0000\n",
      "Test Batch number: 1345, Test: Loss: 0.6889, Accuracy: 1.0000\n",
      "Test Batch number: 1346, Test: Loss: 0.0093, Accuracy: 1.0000\n",
      "Test Batch number: 1347, Test: Loss: 0.0001, Accuracy: 1.0000\n",
      "Test Batch number: 1348, Test: Loss: 0.2049, Accuracy: 1.0000\n",
      "Test Batch number: 1349, Test: Loss: 0.0114, Accuracy: 1.0000\n",
      "Test Batch number: 1350, Test: Loss: 1.5463, Accuracy: 0.0000\n",
      "Test Batch number: 1351, Test: Loss: 0.0034, Accuracy: 1.0000\n",
      "Test Batch number: 1352, Test: Loss: 0.0161, Accuracy: 1.0000\n",
      "Test Batch number: 1353, Test: Loss: 0.2566, Accuracy: 1.0000\n",
      "Test Batch number: 1354, Test: Loss: 0.6413, Accuracy: 1.0000\n",
      "Test Batch number: 1355, Test: Loss: 0.2467, Accuracy: 1.0000\n",
      "Test Batch number: 1356, Test: Loss: 0.1392, Accuracy: 1.0000\n",
      "Test Batch number: 1357, Test: Loss: 0.0440, Accuracy: 1.0000\n",
      "Test Batch number: 1358, Test: Loss: 0.0122, Accuracy: 1.0000\n",
      "Test Batch number: 1359, Test: Loss: 0.0647, Accuracy: 1.0000\n",
      "Test Batch number: 1360, Test: Loss: 0.3400, Accuracy: 1.0000\n",
      "Test Batch number: 1361, Test: Loss: 0.0290, Accuracy: 1.0000\n",
      "Test Batch number: 1362, Test: Loss: 0.0704, Accuracy: 1.0000\n",
      "Test Batch number: 1363, Test: Loss: 0.5265, Accuracy: 1.0000\n",
      "Test Batch number: 1364, Test: Loss: 0.5795, Accuracy: 1.0000\n",
      "Test Batch number: 1365, Test: Loss: 0.0784, Accuracy: 1.0000\n",
      "Test Batch number: 1366, Test: Loss: 0.7237, Accuracy: 0.0000\n",
      "Test Batch number: 1367, Test: Loss: 0.1999, Accuracy: 1.0000\n",
      "Test Batch number: 1368, Test: Loss: 0.1132, Accuracy: 1.0000\n",
      "Test Batch number: 1369, Test: Loss: 0.5812, Accuracy: 1.0000\n",
      "Test Batch number: 1370, Test: Loss: 0.0274, Accuracy: 1.0000\n",
      "Test Batch number: 1371, Test: Loss: 0.4915, Accuracy: 1.0000\n",
      "Test Batch number: 1372, Test: Loss: 0.2845, Accuracy: 1.0000\n",
      "Test Batch number: 1373, Test: Loss: 0.2930, Accuracy: 1.0000\n",
      "Test Batch number: 1374, Test: Loss: 0.2580, Accuracy: 1.0000\n",
      "Test Batch number: 1375, Test: Loss: 0.2905, Accuracy: 1.0000\n",
      "Test Batch number: 1376, Test: Loss: 1.5479, Accuracy: 0.0000\n",
      "Test Batch number: 1377, Test: Loss: 0.1458, Accuracy: 1.0000\n",
      "Test Batch number: 1378, Test: Loss: 0.2253, Accuracy: 1.0000\n",
      "Test Batch number: 1379, Test: Loss: 0.0013, Accuracy: 1.0000\n",
      "Test Batch number: 1380, Test: Loss: 0.0008, Accuracy: 1.0000\n",
      "Test Batch number: 1381, Test: Loss: 0.0000, Accuracy: 1.0000\n",
      "Test Batch number: 1382, Test: Loss: 0.0762, Accuracy: 1.0000\n",
      "Test Batch number: 1383, Test: Loss: 0.3256, Accuracy: 1.0000\n",
      "Test Batch number: 1384, Test: Loss: 0.0013, Accuracy: 1.0000\n",
      "Test Batch number: 1385, Test: Loss: 0.0018, Accuracy: 1.0000\n",
      "Test Batch number: 1386, Test: Loss: 0.2712, Accuracy: 1.0000\n",
      "Test Batch number: 1387, Test: Loss: 0.2513, Accuracy: 1.0000\n",
      "Test accuracy : 0.8868876080691642\n"
     ]
    }
   ],
   "source": [
    "computeTestSetAccuracy(saved_model, lossFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_example = transforms.Compose([transforms.Resize(size=256),\n",
    "                            transforms.CenterCrop(size=224),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                 [0.229, 0.224, 0.225])])\n",
    "\n",
    "dataset = datasets.ImageFolder('examples', transform=transform_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " # Disable grad\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Retrieve item\n",
    "    index = 0\n",
    "    item = dataset[index]\n",
    "    image = item[0]\n",
    "    #true_target = item[1]\n",
    "    imshow(image)\n",
    "    image=image.to(device)\n",
    "    # Loading the saved model\n",
    "\n",
    "    \n",
    "    # Generate prediction\n",
    "    prediction = saved_model(image.unsqueeze(0))\n",
    "    \n",
    "    # Predicted class value using argmax\n",
    "    predicted_class = np.argmax(prediction.cpu())\n",
    "    \n",
    "    # Reshape image\n",
    "\n",
    "    image=image.cpu()\n",
    "    image=image.swapaxes(0,1)\n",
    "    image=image.swapaxes(1,2)\n",
    "    \n",
    "    # Show result\n",
    "    plt.imshow(image.cpu(), cmap='nipy_spectral')\n",
    "    plt.title(f'Prediction: {predicted_class} - Actual target: 1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "RS-Image-Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
